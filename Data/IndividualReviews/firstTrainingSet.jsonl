{"id":246,"text":"The step decay (constant then cut) strategy is widely used in training deep neural networks. There have been quite a few works dedicated to it, both theoretical and empirical. This paper does a great job in synthesizing the related works, identifying their contributions and limits, and pointing out where this work stands.\n\nThis strategy is of course not novel, nor are the rates they obtained in any setting, but providing the theoretical guarantees for the step decay schedule as proved in this paper for various settings are new. In addition, the sampling scheme of being inversely proportional to $\\eta_t$ is novel and very intuitive in giving weights to last iterates.\n\nSome of my concerns are:\n1. The assumption on the upper-boundedness of $f$ is worrisome, especially when it appears in the bound with the term $f_{max} - f^*$ showing that if it is unbounded then the bound is trivial. If the domain is bounded then this assumption would be more natural but for unbounded domain, this assumption seems too restrictive. Note that other results typically include a term $f(x_1) - f^*$ which is fine as $f(x_1)$ can be manually selected and easily be bounded. Looking at the proofs, however, the term $f_{max} - f^*$ is not just a simple upper-bound of $f(x_1) - f^*$ and the assumption on $f_{max} < \\infty$ is indispensable.\n2. In the paper on exponential step sizes [Li et al., 2020], they also studied the cosine step sizes and showed that it also performs really well and surpasses the exponential step sizes and the step decay in some settings. Also, in their paper, the step decay schedule has the flexibility of using different inner-loop sizes thus should be different from your definition in this paper. Hence, I would like to see a comparison with it.\n3. I see that you answered Yes to Checklist 3.a, but could not find your codes on implementing the algorithm and comparing with other optimizers, apart from download link to datasets you used.","cats":["0"],"entities":[[176,323,"positive"]],"Comments":[]}
{"id":247,"text":"Overall I like the theoretical results the authors obtained. But the write-up and presentation can be further polished and improved. For example, there are still quite a few typos or grammatical errors in the submission. The technical proof is quite dense, which is evident from the length of the supplementary materials. A guide for readers on how to read the supplementary materials is necessary for this paper to have more impact if they hope machine learning researchers could use their tools to perform honest statistical inference.\n\nIn terms of technical content, the authors propose a condition called $\\mathbb{C}^{3}$-approximability and use a surrogate $\\mathbb{C}^{3}$-function sequence as an intermediate step to show bootstrap consistency. This is an important assumption but the authors delay the discussion of this assumption to Appendix C. I understand due to NeurIPS's page limit there has to be a trade-off on what to be included in the main text. But I am worried that the supplementary materials will eventually be buried under most readers' desk drawers. I also hope that the authors can provide one example (if they could) that violates Assumption 1 yet the bootstrap consistency still holds.\n\nThe authors advocate that the strength of their paper provides tools to show bootstrap consistency for general nonlinear functionals beyond what people have done case by case. But it would be useful if the authors could recover several established results as special cases of their master theorem.\n\nIn addition, the authors admit that their technical results are inspired by Chatterjee's work. Then what technical challenge do they need to solve? Are the proofs, even though very long, standard or requiring some innovations? These should be made clearer.\n\nSome extra comments:\n\n(1) Lines 47-49: \"Notably, this intuition has already been exploited to show that the bootstrap method is consistent in particular applications in the econometric literature, such as the construction of uniform confidence bands.\" This sentence should be accompanied by relevant citations.\n\n(2) Lines 88-89: \"We note that the limiting distribution of those statistics are in general not Gaussian [22].\" (i) \"distribution\" should be \"distributions\"; (ii) I understand that the authors try to tell readers Gaussian limiting distributions for these statistics are established under specific conditions, e.g. $U$-statistics with dominating first-order terms in Hoeffiding's decomposition. But this and later sentences seem to be quite disconnected from previous sentences without adding more contexts. For example, Xiaohui Chen's work [15] is about Gaussian limiting distribution for high-dimensional $U$-statistics.\n\n(3) I would say 70% of NeurIPS submission does not contain such a long supplementary material filled with technical lemmas and proofs. One presentation style that I found very useful for technical papers in statistics and theoretical machine learning is to draw a diagram indicating which technical lemma was used to prove Theorem X and how different technical results are connected.\n\n(4) Lines 146-147: \"Given that these results typically require stronger conditions on the statistic and many times Gaussian limits.\" What does \"many times Gaussian limits\" mean exactly?\n\n(5) Line 231: \"the test statistics is computed on\" should be \"the test statistics are computed on\"\n\n(6) A recent paper (first appeared in 2020) by Vladimir Koltchinskii (https:\/\/arxiv.org\/pdf\/2011.03789.pdf) considered a relatively similar setup, but he focused on Gaussian limit and root-n parametric theory for estimating nonlinear statistics. I suggest the authors also cite this paper and discuss the connection and difference between their work and Koltchinskii's. \n\n(7) No discussion is written. The authors are recommended to write at least some sentences on future works or caveats... This is also strongly encouraged by NeurIPS.\n\nGiven above, I temporarily give the authors a score of 5 but if they could convince me with their rebuttal I am willing to change my evaluation. Also, this paper seems to fit better in a journal in statistics, JMLR or a TCS-type conference. The page limit of NeurIPS may have a negative impact on this paper.","cats":["2"],"entities":[[1609,1770,"negative"]],"Comments":[]}
{"id":248,"text":"This paper addresses long-text generation, with a specific task of being given a prefix of a review and needing to add the next five sentences coherently.  The paper proposes adding two discriminators, one trained to maximize a cosine similarity between source sentences and target sentences (D_{coherence}) and one trained to maximize a cosine similarity between two consecutive sentences.  On some automatic metrics like BLEU and perplexity, an MLE model with these discriminators performs a little bit better than without.\n\nThis paper does not include any manual evaluation, which is critical for evaluating the quality of generated output, especially for evaluating coherence and cohesion.  This paper uses the task setup and dataset from \"Learning to Write with Cooperative Discriminators\", Holtzman et al., ACL 2018.  That paper also includes many specified aspects to improve the coherence (from the abstract of that paper \"Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.\").  But this paper:\n--Does not compare against the method described in Holtzman et al., or any other prior work\n--Does not include any human evaluations, even though they were the main measure of evaluation in prior work.\n\nThis paper states that \"To the best of our knowledge, this paper is the first attempt to explicitly capture cross-sentence linguistic properties, i.e., coherence and cohesion, for long text generation.\"  There is much past work in the NLP community on these.  For example, see:\n \"Modeling local coherence: An entity-based approach\" by Barzilay and Lapata, 2005 (which has 500+ citations). \nIt has been widely studied in the area of summarization, for example, \n\"Using Cohesion and Coherence Models for Text Summarization\", Mani et al., AAAI 1998, and follow-up work.\nAnd in more recent work, the \"Learning to Write\" paper that the dataset and task follow from addresses several linguistically informed cross-sentence issues like repetition and entailment.  \n\nThe cosine similarity metric in the model is not very well suited to the tasks of coherence and cohesion, as it is symmetric, while natural language isn't.  The pair:\n\"John went to the store to buy some milk.\"\n\"When he got there, they were all out.\"\n\nand \n\n\"When he got there, they were all out.\"\n\"John went to the store to buy some milk.\"\n\nwould have identical scores according to a cosine similarity metric, while the first ordering is much more coherent than the second.\n\nThe conclusion says \"we showed a significant improvement\": how was significance determined here?\n","cats":["1"],"entities":[],"Comments":[]}
{"id":249,"text":"The paper describes a general software suite for deploying executables of quantum algorithms to specific hardware.  It leverages both AI Planning and Constraint Programming to form a general solution that allows the user to specify the hardware architecture and algorithm, which is then automatically compiled to the hardware.  A GUI shows the goal graph, machine graph and Quantum Circuit Compilation in a format that can aid the user in understanding the way in which the algorithm is running on the hardware.   A key benefit of this software suite is its generality due to its use of general problem solving techniques, which means that the same software can be applied to many different algorithms and, perhaps more importantly, many different hardware types.  \n\nThe paper is easy to read, clearly well motivated, and generally of high quality.  The paper would be a great addition to the SPARK workshop.  \n\nMy only minor suggestion is to switch to an Author (Year) style of citation when a citation refers to a work.  For example, prefer \"Author et al. (YEAR) discuss ...\" to \"(Author et al., YEAR) discuss\".\n\np3: cross-talks constraints -> cross-talk constraints","cats":["0"],"entities":[[767,910,"positive"]],"Comments":[]}
{"id":250,"text":"The authors investigate the role of entropy maximization in SAC and show that entropy regularization does not do what is usually thought: in the examples they investigate, where the output of the policy network needs to be squashed to fit in the action space domain, squashing would result in having only action at the boundaries, but entropy regularization maintains some intermediate values, hence exploration. From this insight, the authors replace entropy regularization by a simpler normalization process and show equivalent performance with their simpler Streamlined Off-Policy (SOP) algorithm. Then they introduce a second \"Emphasizing Recent Experience\" mechanism and show that SOP+ERE performs better than SAC.\n\nA good point for the paper is that the entropy regularization  study is very nice, more papers in the field should show similar detailed analyses of internal processes. But the paper suffers from a few serious weaknesses:\n\n- The TD3 mechanism goes beyond the Double Q-learning (or DDQN) mechanism of Van Hasselt et al: it takes the min over two critics. This should be explained properly.\n- the title, abstract and introduction insist more on SOP, but performance improvement seem to result more from ERE. If this is possible, studying the performance of SAC + ERE would disambiguate the relative contribution of both mechanisms.\n\nAbout gradient squashing issues, the authors main mention de gradient inverter idea from this paper:\n\n@article{hausknecht2015deep,\n  title={Deep reinforcement learning in parameterized action space},\n  author={Hausknecht, Matthew and Stone, Peter},\n  journal={arXiv preprint arXiv:1511.04143},\n  year={2015}\n}\n\nThe authors should also probably also cite (and read the latest arxiv version of):\n@inproceedings{ahmed2019understanding,\n  title={Understanding the impact of entropy on policy optimization},\n  author={Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},\n  booktitle={International Conference on Machine Learning},\n  pages={151--160},\n  year={2019}\n}\n\n\nMore local points:\n- \"without performing a careful hyper-parameter search\": so how did you choose these hyper-parameters? I see what you mean, but this is a very vague and slippery statement.\n- I do not find the 23*4 images in Appendix B much useful\n- Fig 3 seems to be repeated in Fig 4. Can't you just remove Fig 3?","cats":["1"],"entities":[],"Comments":[]}
{"id":251,"text":"This was submitted as a workshop paper but I think with just a little more detail it would be a strong contender for a regular conference paper. The authors show how a combination of primal-dual estimation for Lagrangian problems with neural ODEs for density estimation can be used to solve the regularized optimal transport problem on high dimensional spaces. They show with numerous experiments on challenging domain-to-domain problems that the joint probability distribution they learn can be used for meaningful joint generation tasks, such as pix2pix-like style transfer in the image domain. Overall this paper was a pleasure to read - clearly motivated, tackling an important problem and using state-of-the-art methods to achieve it. I would have appreciated a little more discussion of the stability of gradient ascent\/descent for solving the Lagrangian multiplier formulation of their objective, as I have found these kinds of problems very hard to work with in a stochastic domain, but overall the paper was compelling and timely.","cats":["0"],"entities":[[0,144,"positive"],[597,739,"positive"],[741,1039,"positive"]],"Comments":[]}
{"id":252,"text":"This paper proposes to use uncertainty estimates from an ensemble of action-values, to provide a weighting on the updates in Q-learning. The main idea is to use the sigmoid of the negative of this uncertainty in the next state, to produce a weighting between 0.5 and 1 to downweight updates with high uncertainty targets. This uncertainty estimate from the ensemble is also used to improve exploration, in a combined algorithm called Sunrise that leverages learning an ensemble in these two ways. \n\nThe idea of using weighted Bellman updates is, as far as I know, novel. The evidence for the idea, however, needs more work. First, the weighted update in Eq (4) is not motivated from first principles. Second, the empirical evidence is weak because the experiments highlighting the role of the weighting do not demonstrate significant differences.  \n\nThe first issue is the justification for the approach. The ensemble of Q-learning agents is trained using the weighting, derived from that ensemble. There are natural questions as to the interaction between the ensemble uncertainty estimates and the ensemble estimates. Does it result in any instability? What is the final point of convergence? Does it change the solution?\n\nBut, one could argue that that is not much of a problem, since the weighting w(s,a) is always between 0.5 and 1, so it is not that skewed. Then the question arises how much it is helping, and why this small reduction in weight helps. This is particularly important to ask, considering the algorithm requires an ensemble to be learned, with subsets of data used for each action-value. There is a lot of effort expended for that weighting.   \n\nThe experiments then do include ablations, to examine the effect of these weightings. Unfortunately, the results are inconclusive. The experimental time spent must have been large to get all the results in this paper, across so many environments and algorithms. But, the ablations themselves are not sufficiently in-depth to provide insight into the idea and algorithm. The results in Figure 2 are key, since that figure examines Sunrise with and without the weighting. Due to the variance across runs, with only 4 runs, there are large standard errors (and so even larger 95% confidence intervals); it is hard to conclude that weighting is helping. The additional results in Figure 5 in the appendix have a similar issue.\n\nThe results in Figure 3, which motivate the exploration utility, are more clear in Cartpole. This provides some motivation for learning ensembles, so they can be used for exploration. But, this exploration approach with ensembles is an existing method. The main novelty in this work is the weighting. \n\nI highly recommend taking a few domains and carefully studying the impact of the weight. More runs would help for significance, as well as parameter sensitivity analysis to gain insight into the generality of the improvement. Sometimes performance gains are from hyperparameter tuning, rather than from the utility of an idea; here, you really want to know if and why this weighting improves performance. \n\nAs a more minor comment, Sunrise is pitched as combining three ideas for using ensembles: your weighting, bootstrapping and UCB exploration. However, I see Sunrise as combining two ideas: weighting and UCB exploration. The Bootstrap DQN approach gives you a way to learn your ensemble of bootstrap models, so that it provides a useful uncertainty estimate. Given that ensemble, you can then use it to compute a weighting and optimistic action. It would be more clear to separate it out that way, rather than saying \"Furthermore, since our weighted Bellman backups rely on maintaining an ensemble, we investigate how weighted Bellman backups interact with other benefits previously derived from ensembles: (a) Bootstrap; (b) UCB Exploration.\" The bootstrap is arguably not a benefit, but an approach to obtain confidence (uncertainty estimates). \n \nMinor comments:\n1. Bootstrap DQN is listed under \"Ensemble Methods in RL\", rather than under \"Exploration in RL\", but is it an exploration approach.\n2. \"Recently, Kumar et al. (2020) showed that this error propagation can cause inconsistency and unstable convergence.\" The terms inconsistency and unstable convergence should be explained, since they seem like technical terms. \n3. Bellman backup seems to be used to describe the squared error to the expectation over next action, in Equation (2), and then to a stochastic sample of the action in (4). Which is it?\n4. What is meant by the signal-to-noise in Q-updates? \n5. A natural baseline to include is to tune an agent that uses random weights between 0.5 and 1 in the update, but keeping other parts of Sunrise the same. The ablation removes the weighting all together, which is also important to include. But, it's worthwhile observing if random weights performs similarly, especially if that agent is tuned. \n\n------------ Update\nThank you for the clear reply. Unfortunately, I remain concerned about the significance of experiments. I mentioned above that 4 or 5 runs is typically not enough, and because the standard errors are overlapping, the differences could be due to chance. The addition of a result with 10 runs is a good step. But, as part of the reply, the authors state: \"Figure 3(a) shows the learning curves of all methods on the SlimHumanoid-ET environment over 10 random seeds. First, one can not that SUNRISE with random weights (red curve) is worse than SUNRISE with the proposed weighted Bellman backups (blue curve). Additionally, even without UCB exploration, SUNRISE with the proposed weighted Bellman backups (purple curve) outperforms all baselines. This implies that the proposed weighted Bellman backups can handle the error propagation effectively even though there is a large noise in reward function.\" However, if you look at this figure, the error bars all still overlap. 10 random seeds is still not enough. \n\nI am also not confident that the issue will be remedied, as the authors additionally state in the rebuttal: \"we believe that SUNRISE is evaluated in a broad collection of domains in the RL literature and the performance gap is also noticeable.\" An insignificant gap across many domains does not tell us anything. Actually, if you take the runs and tried to do significance tests by pooling all the runs across environments, then maybe the result might actually be significant. But, of course, there will be higher variance due to differences in the environments, so it is not obvious this would be true. Nonetheless, this could be a natural next step.","cats":["1"],"entities":[],"Comments":[]}
{"id":253,"text":"The authors study the identifiability of latent variables under observation of high-level variables that are generated via a polynomial function. The central result is identifiability of the latents up to affine functions under certain assumptions on the polynomial as well as support of the latents.\n\nThe paper is clear to read and the main ideas are conveyed quickly.\n\nA major flaw with the writing is the notation of interventional data. In fact, the authors use $z_{-i} \\sim \\mathbb{P} (Z_{-i}\\mid z_i = z^*)$, which does *not* denote the interventional data when $z_i$ is intervened via *do*, but simply the conditional probability after observing $z_i$ as a constant. The main outline of the paper as well as the motivation are thus misleading, even if it might have been a non intentional error (still a very major one). Upon initial reading, this mathematical mistake led me to question the correctness of the results. Nonetheless, the most important assumption seems to actually be the non-emptiness of the support of the latent variables (in particular Ass. 4) which can hold both for actual interventional data as well as observational data where we only observe a constant $z_i$. \n\nWhile I haven’t checked the proofs in detail, on first glance they seem to be correct and rely on functional analysis, in particular Theorem 1.\n\nI believe the paper would much benefit from even just a toy example to show how the latents can be learned, and it shouldn’t be too involved to code this down for a small example. As such, an important question and discussion for the results are the practicality — could a polynomial decoder give good empirical results on real-world data? \n\nSince I cannot give more nuanced recommendations other than reject or accept, I decided to give a score of two as I believe the mistake can be fixed quickly and this is a workshop where we encourage a wide range of discussion, and I strongly advise the authors to correct the notation and make sure that the theoretical results still hold. Also, a discussion like the one mentioned above could be of value for the paper and discussions in the workshop.","cats":["0"],"entities":[[302,369,"positive"],[1758,2019,"positive"],[2021,2133,"positive"]],"Comments":[]}
{"id":254,"text":"This paper studies the problem of multi-label learning for text copora. The paper proposed a latent variable model for the documents and their labels, and used spectral algorithms to provably learn the parameters.\n\nThe model is fairly simplistic: the topic can be one of k topics (pure topic model), based on the topic, there is a probability distribution over documents, and a probabilistic distribution over labels. The model between document and topic is very similar to previous pure topic models (see more discussions below), and because it is a pure topic, the label is just modeled by a conditional distribution.\n\nThe paper tried to stress that the model is different from Anandkumar et al. because the use of \"expectations vs. probabilities\", but that is only different by a normalization factor. The model defined here is also very strange, especially Equation (2) is not really consistent with Equation (7). \n\nJust to elaborate: in equation (2), the probability of a document is related to the set of distinct words, so it does not distinguish between documents where a word appear multiple times or only once. This is different from the standard bag-of-words model where words are sampled independently and word counts do matter. However, in the calculation before Equation (7), it was trying to compute the probability that a pair of words are equal to v_i and v_j, and it assumed words w_1 and w_2 are independent and both of them satisfy the conditional distribution P[v_i|h = k], this is back to the standard bag-of-words model. To see why these models are different, if it is the model of (2), and we look at only distinct words, the diagonal of the matrix P[v_i,v_i] does not really make sense and certainly will not follow Equation (7). Equation (7) and also (9) only works in the standard bag-of-words model that is also used in Anandkumar et al. (the same equations were also proved).\n\nThe main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments. The reviewer feels this alone is not enough contribution.","cats":["1"],"entities":[],"Comments":[]}
{"id":255,"text":"This paper studies the teacher ensembles setting for differentially private learning. In this setting, each teacher holds part of the training set and trains a local model. The student uses unlabeled examples to query teacher model. Then the student trains a model from scratch using the examples labeled by teachers.\n\nIn order to make the labeling process differentially private, previous work uses noisy argmax mechanism. Each class of label is assigned with a count number. The student first queries the same example to multiple teachers. To guarantee differential privacy, the counts are perturbed by noise before releasing. Then, because of the post-processing property of differential privacy, the argmax operator on such noisy counts are still differentially private.\n\nThis paper proposes to add a constant c to the largest count before perturbing and releasing the counts. The authors argue this would improve the accuracy of the noisy argmax operator and yield the same privacy loss as previous approach. However, adding a constant c would increase the sensitivity and therefore degenerates the privacy guarantee. The added noise cannot guarantee the privacy if all others are the same as previous work. To see this clearer, for example, if c=0, then one sample point can at most change the count by 1. If c>0, then one sample point can change the count by 1+c. Because of this, the proposed method cannot guarantee the amount of differential privacy as the paper claimed.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":256,"text":"The paper proposes the Low-Rank Global Attention (LRGA) module augmented to GNNs to improve generalization power. In particular, given an input graph, the model runs the LRGA module and the GNN module to aggregate node representations on this graph. The input and the outputs of these two modules are concatenated at each layer, followed by a single fully-connect layer (m5) to produce input for the next layer. The LRGA module applies the self-attention mechanism [1], but replacing the softmax layer by the global normalization. \n\nPros. The results are promising.\n\nCons. \n\ni) The motivation to propose LRGA by replacing the softmax layer in the self-attention mechanism [1] by the global normalization is not well enough. The graph self-attention networks (such as [3,4]) show competitive results, and they can be applied for large graphs. Thus, LRGA is incremental and not technically sound.\n\nii) The paper does not discuss the most closely related work, Dual Graph Convolutional Networks (DualGCN) [2]. The architecture LRGA+GNN is similar to DualGCN. Changing from using GCN to another GNN is straightforward, thus the work lacks novelty.\n\niii) The roles of m1, m2, and m3 are similar to the query, key, and value matrices in the self-attention mechanism, respectively. But why LRGA employs m4? m4 does not have a specific role as it can be placed outside LRGA and put inside Equation 1. \nNote that [5] shows that using the vector concatenation\/sum-pooling\/LSTM over different layers can improve the performance. But, I do not see the role of X^l in Equation 1. What is it?\n\niv) Given the same GNN module with the same hidden size, the proposed LRGA+GNN has much larger parameters than GNN. This limitation restricts to use of deeper layers. \n\nMinor things: Parentheses in Equation 2 should use between m1 and m2, not m2 and m3.\n\n[1] Attention is all you need. NIPS 2017.\n[2] Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification. WWW 2018.\n[3] Graph-Bert: Only Attention is Needed for Learning Graph Representations. https:\/\/arxiv.org\/abs\/2001.05140\n[4] Hyper-SAGNN: a self-attention based graph neural network for hypergraph. ICLR 2020.\n[5] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.\n\n=======================\nAfter reading the authors' response:\n\ni. As shown in (new) Table 3 in the revised version, the results of using the global normalization are not better than that of using the softmax layer in the self-attention mechanism. Hence the motivation is not enough.\n\nii. To have the faster computation, we have SGC[1], FastGCN[2]. To have powerful GNNs, we have GIN[3]. Inspired by DualGCN, we can build a new combination (e.g., SGC+GIN) together with using the vector concatenation\/sum-pooling\/LSTM over different layers [5] to further improve the performance and have a faster computation. That's reason why the novelty of LRGA+GNN is weak.\n\n[1] Simplifying Graph Convolutional Networks. ICML 2019. [2] Fastgcn: Fast learning with graph convolutional networks via importance sampling. ICML 2018. [3] How Powerful are Graph Neural Networks? ICLR 2019. [5] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.\n\nI keep my score unchanged.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":257,"text":"## Summary\nThe paper proposes a new neuro-symbolic framework, which can perform learning to translate instructions to grounded robot plans. The addressed problem is to solve planning problems where the input is given as a pair of a state as a depth image and natural language instruction. The task is difficult since the agent needs to understand both the visual state and the natural language instruction and to perform planning on top of them. To address the problem, the paper proposes a new framework, that consists of language reasoner, visual extractor, visual reasoner, and action simulator. Each component processes information so that the entire system can execute symbolic programs defined in the DSL in a differentiable manner to solve the visual planning problem. In the experiment, the proposed approach outperformed a neural baseline, and moreover, it showed strong generalization results in terms of the number of objects and the number of steps of the planning. \n\n## Pros\n- The paper is very well written\n- The paper has a good technical quality, i.e., everything is formulated without technical flaws\n- The proposed approach is novel in the sense that it solves visual planning problems using the NS-CL approach beyond VQA tasks\n- The paper leads to many important applications\n\n## Cons\nThe paper is overall well-written, however, I noticed some minor points that can be addressed.\n- In Fig. 1, Visual Extractor has Relational embeddings, however, it is not explained in the main text. If not the case I'm missing something, please explain it properly somewhere in the paper.\n- In line 107, I think Action simulator should be capitalized as Action **S**imulator\n\n### Questions\n- The limitation of the proposed approach is not discussed in the paper. What would be the limitation?\n- Is the proposed framework capable of parallelized batch computation? The implementation of neural networks in general can process a given batch of examples in parallel on GPUs. For the proposed approach, if the user gives several examples as input, are they processed in parallel? If not, how long does it take to train the proposed model? Would it take longer compared to the neural-based baseline?","cats":["0"],"entities":[[990,1020,"positive"],[1023,1117,"positive"],[1120,1203,"positive"],[1248,1294,"positive"]],"Comments":[]}
{"id":258,"text":"This work presents a deep learning approach for error detection of automated segmentation pipelines. The model uses the previously published pix2pix conditional GAN model to learn the original image from the segmentation. In the test phase the predicted image is compared to the original image using a CNN, and an error map is generated. \nNovel approach, nice initial validation. The methods shows good performance, but some false positives that should still be addressed.\n","cats":["0"],"entities":[[339,380,"positive"],[380,472,"positive"]],"Comments":[]}
{"id":259,"text":"The manuscript focuses on clinical natural language processing of electronic health records. More precisely, it addresses a text classification task called information extraction or named entity recognition from these clinical records. Its contributions include developing an embedding model to capture clinical prototypes (CPs), via supervised contrastive learning, and presenting experimental evidence of these learnt CPs capturing attribute-specific semantic relationships and being helpful in subsequent clinical natural language processing task of information retrieval and clustering of clustering of physiological signals. I find this text processing methodology interesting, carefully described, and supplementary to other studies.\n\nHowever, unfortunately, the authors demonstrate limited understanding of the related literature. First, the second and third paragraph of the Introduction section have many sentences that require references to be inserted. Second, and more importantly, the Related work section seems to not capture the key papers and trends of the field (I suggest reading some systematic reviews or surveys on clinical natural language processing, information extraction, and information extraction by, for example, Wendy Chapman, Carol Friedman, and Pierre Zweigenbaum), and, for example, as illustrated by the ImageCLEF and CLEFeHealth evaluation labs, computer vision tends to proceed faster than text analytics (see, e.g., https:\/\/www.researchprotocols.org\/2018\/7\/e10961\/), and not the other way around as claimed by this section.\n\nIn addition, to feel convinced of the presented experimental evidence, I would have wanted to see statistical significance tests, confidence intervals, effect sizes, or similar presented. I could not find this methodology described in the manuscript or its outcomes, although the narrative repeatedly referred to significant performance gains. Please clarify.\n\nAs my main minor comment, I would like to see a clearer separation of the materials, methods, and experiments sections from results, as well as including clearer justifications of this study design. For example, the aforementioned significance topic has not been addressed sufficiently. Another illustration of somewhat difficult task for the reader is to understand the experimental design as a whole and be convinced of this study being rigorous is the Experimental Results section including a lot of methodological details as opposed to only obtained outcomes.\n\nI also suggest including a conclusion statement as well as embedding more evidence (e.g., evaluation materials and methods plus obtained indicators of performance gains, such as measure values and effect sizes) to convince the reader in the abstract. To continue, please remember to punctuate equations and formulae. \n\nFinally, typically the Methods, Experiments, and Experimental Results sections would have been written using a past tense to emphasise a finished (as opposed to an ongoing) study where materials, methods, and experiments have already been chosen, justified, and completed. Most importantly, please avoid having inconsistent tense in these sections (see, e.g., Section 4.5. using a past tense whereas almost all others are having a present tense).","cats":["2"],"entities":[[765,838,"negative"]],"Comments":[]}
{"id":260,"text":"This paper proposes to improve the segmentation quality of boundary areas in medical images. It proposes a loss function that is inspired by Laplacian of Gaussian (LoG) filtering for edge detection. This proposed method is claimed to be light-weighted.\n\nPros: \n1)\tThis loss function is inspired by Laplacian of Gaussian (LoG) filtering, this formulation is suitable for the task of medical image segmentation.\n2)\tThis paper is well-written.\n3)\tThis loss seems to be easier to implement than competing methods and has competitive results.\n\nCons:\n1)\t-- As inspired by LoG, this loss is not novel.\n2)\t-- The paper says it does not require post-processing. However, the convolution operation seems to be post-processing.\n3)\t-- The authors argue that this loss is light-weighted, but there is no quantitative evaluation on the computational cost and time consuming compared with other works.\n4)\t-- The results are only comparable to other methods. It also would be interesting to see the combination of this loss with other methods.\n\nBasically, this is a paper with a simple idea and insufficient experiments. As this is a short paper I recommend weak accept, but it is actually not good enough. I would not be upset if it is rejected.","cats":["2"],"entities":[[413,439,"positive"],[1105,1189,"negative"],[1191,1230,"negative"]],"Comments":[]}
{"id":261,"text":"Training large DNNs with stochastic optimizers is effective at relatively small batch sizes. Existing techniques to distribute training across many devices achieve high device utilization, but also lead to an increased effective batch size that yields diminishing returns of the optimization. This paper studies a new schedule which features high device utilization at small batch sizes. This is demonstrated on a 52 billion parameter model on 64 V100 GPUs by comparison to other schemes.\n\n---\n\nMiscellaneous comments:\n\n- L89: Missing space between \"models\" and \"and\"\n- L252: \"we use\" → \"uses\"\n","cats":["1"],"entities":[],"Comments":[]}
{"id":262,"text":"The authors train ResNet-50 networks on a mixture of ImageNet data and BigGAN samples and show that replacing ImageNet data with BigGAN samples leads to a decrease in performance.\n\nPositives:\n- This \"data-replacement\" experiment is very natural to make, therefore I am glad it was done.\n- The metric for identifying BigGAN failures per class is also an interesting byproduct.\n\nRemarks:\na) \"per-class FID\" is said to be likely \"making the per-class estimates unreliable\" due to high variance. I would still like to see what the per-class FID gives and how the best and worst-performing classes compare with the one found by this method. Indeed, even if per-class FID could perform worse, it has the great benefit of not necessitating to train new Resnet-50 networks.\nb) I would be interested to see some samples generated with the best truncation for replacement, and for addition, in order to have a better idea of what kind of \"diversity\" we are talking of (this could replace one of the two subplots of Fig. 2 since top1\/top5 follow the same trends).\n\nOverall, I think this paper and its experiments is a nice contribution to the workshop.","cats":["0"],"entities":[[194,286,"positive"],[289,375,"positive"],[636,764,"positive"],[1062,1140,"positive"]],"Comments":[]}
{"id":263,"text":"This paper investigates the representations learned by RNN-based RL agents for solving structured prediction problems when trained with both policy gradient and value-based methods. It studies the conditions leading to state aliasing and highlights strategies to prevent this situation. The hypothesis is that state aliasing happens when different states share the same optimal action and can result into a failure to converge to the optimal policy. The authors study this phenomenon using LSTM and GRU networks on synthetic (toy) environments, and validate their hypothesis.\n\nThe paper is clear, well written, and very relevant to the workshop.","cats":["0"],"entities":[[577,645,"positive"]],"Comments":[]}
{"id":264,"text":"This paper highlights the problem of model overlearning - learning more than it is trained to do. Thus, there is leak of privacy and sensitive attributes of images during test\/ inference time.\n\nPros:\n1. The paper is well written and easy to follow\n\nCons:\n1. There is very little novelty in this paper - the notion of overlearning is well established in the literature (Osia et al., 2018; Chi et al., 2018; Wang et al., 2018). This paper merely reinstates, what is being already told in the literature.\n\n2. In fact, there are many defence mechanisms proposed in the literature, for example \"Anonymizing k Facial Attributes via Adversarial Perturbations\" IJCAI 2018 - where the authors are performing data perturbations to minimize overlearning. This paper does not suggest or propose any method for solving the issue of overlearning\n\nIn summary, this paper repeats a well established problem of overlearning, showing experiments that are already shown in literature with known datasets, and also NOT proposing a solution to minimize overlearning (as many papers already proposed in literature). \n\n3. Additionally, the experiments are very weak - the authors still perform experiments using LeNet variants and AlexNet, and for text using a textCNN. Why did the authors not perform experiments using more state-of the art CNN\/RNN models. Did they not observe overlearning in these models?\n\n4. As for section 4.4, it is pretty understood that lower layers of a DL model, learns very basic low-level features from the images such as edges, corner. Reinstating that, and calling it the reason for overlearning is not very convincing. \n\nAs of now, I find the paper very weak, till a solution to avoid overlearning is not proposed as a part of this paper.","cats":["1"],"entities":[],"Comments":[]}
{"id":265,"text":"The paper introduces a novel way of explaining why a planning problem is unsolvable, via a combination of choosing an abstraction level (in a hierarchy of projections: the maximally abstract version which is still unsolvable) with generating the first unsolvable landmark at that level (landmarks being taen from the maximally concrete solvable version). The paper conducts first user studies and runs some computational experiments.\n\nOverall, this is a nice contribution at a mature stage. Definitely an accept for XAIP. The issues I see are:\n\nThe introduction makes much too broad claims about the originality of the problem addressed. The authors pretend that this is the first work aiming at explaining unsolvability to humans. However, the 2010 work by Goebelbecker et al had exactly the same aim. It's got \"excuse\" in the title but says \"explanation\" already in the abstract. In any case, this is the exact same objective. The techniques suggested are quite different, and so everything is fine content-wise. But the presentation needs to be corrected. \n\nFurthermore, although the abstraction part of this work relates very closely to previous work by Chakraborti et al on model reconciliation, that work is mentioned for the first time on the last page of the paper. It must be mentioned in the introduction already. \n\nA technical issue is that the constraint-compilation sigma(M) may not actually be defined for a more abstract model, namely if the predicates\/planning-state properties referred to in the automaton's transition function are abstracted away. So Proposition 5 is flawed as stated. Actually the automaton has to be defined relative to a task to even be able to formulate this problem. In this sense, Proposition 4 also is flawed as stated. I believe this issues are fixable so don't view them as reason to reject. But they mst be fixed, and it needs to be made clear how this will be done. \n\nIt also must be made much clearer in Section 4 that, and exactly how, \"the methods discussed in earlier sections\" can be used. \n\nI disagree that there is \"no direct way of extracting meaningful subgoals\" (introduction; statement repeated in similar way later on) just because a planning problem is unsolvable. This actually seems to me one more instance of a tendency to formulate claims too broadly. Simple counter-example: What about shared preconditions? If g is a goal and p is a precondition of all actions achieving g, then certainly p is a \"meaningful subgoal\", even if g is unsolvable. Right? Such necessary subgoal analysis is actually all over the place in the landmarks literature. Don't get me wrong, I like your solution to look at the first solvable abstraction layer and use standard landmark definitions there (which indeed per se rely on solvability). But you need to put your approach into the right frame. It is one possible approach. Not the only possible one. \n\nMinor comments:\n\n \"we can also use these subgoals to produce exemplar plans and illustrate their failures alongside the unachievable subgoals\" ==> How so? These subgoals are unsolvable after all. Are you referring here to the generation of abstract plans and pointing out failures in more concrete models?\n\nDef 3 last Pi has an M subscript missing?\n\n\"function free fragment of classical planning\" does not translate to an unambiguous definition in my mind. Do you mean continuous state variables? Or fo you intent to refer to functional strips here as well? If you include statements like this, please be precise.\n\nI'm a little uncertain about the significance of the user studies' results. Playing the devil's advocate, I would surmise that the alternatives provided to the landmarks, based on example plans, are just bad in an obvious\/trivial way. The vast majorities of user votes to this effect certainly agree with that perception. Please add a few more words discussing your views\/your conclusions from these studies.\n\n\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":266,"text":"This paper proposes a Transformer-based architecture for generating an amino acid sequence for a protein, given its 3D structure.  The authors define custom geometric features, and feed it to a model that has elements of a Transformer and graph convolutional neural network.\n\nThe main weakness is that the experiments section is limited:\n- Direct comparison with graph convolutional neural networks is missing, despite this being a more standard way to do deep learning over graphs.\n- There should be ablations of the different features explored, and perhaps comparisons to simpler featurization schemes that have been proposed in the past. \n- There is a comparison with SPIN2, but there are some weird methodological issues, namely that pseudocounts were added post hoc to prevent infinite perplexity. There should be a way to fix numerical stability issues directly, without having to add these pseudocounts.\n- RNN baselines seem to basically learn unigram frequencies, which either suggests they were not tuned properly or that they were too weak baselines, and some slightly better baselines should also be explored.\n- The task of mapping structure to amino acid sequence was motivated by the goal of protein generation. However there is no actual evaluation of the generated sequences, only perplexity.\n\nThere were also some points of confusion:\n- In 2.1, the authors mention that they can handle both \"rigid backbone\" and \"flexible backbone\" problems, but then exclusively discuss the rigid case. Since their featurization depends on having the 3D coordinates of all backbone amino acids, which seems to be a hallmark of the \"rigid\" setting, it is unclear how this extends to the \"flexible\" setting.\n- It is unclear how the decoder works, especially because the j-th amino acid sequence is added to the edge features e_{ij}. This would seem to make it hard to use the standard masking trick to decode--do you have to recompute the entire set of features for each step of decoding?","cats":["1"],"entities":[],"Comments":[]}
{"id":267,"text":"The paper presents a specific diagram of COVID-19 data tracking, monitoring, and collection. The work is practically meaningful and valuable to various communities for future studies:\n\n1. The paper presents a clear and comprehensive process from collecting test samples to final dashboard exhibitions, which provides valuable paradigm experience for data collecting and processing, particularly for college and education communities.\n\n2. The collected data are valuable for public policy and AI modeling communities. For instance, the work uses Wifi data for individual monitoring and contact tracing, which may help establish contact networks and provide a better understanding of how disease can spread within schools. \n\nDespite the meanings of the data collection process, we wish to understand more about the collected data. For instance, it would be good to include non-private or non-sensitive statistical analysis and visualization of the data for the presentation or the final paper. ","cats":["0"],"entities":[[188,433,"positive"],[438,719,"positive"]],"Comments":[]}
{"id":268,"text":"The authors present and analyze a quantum computing algorithm for learning GMMs.\n\nI think this paper cannot be accepted because it violates formatting guidelines. Also, I think it is not appropriate for ICLR since it assumes knowledge of quantum computing that most people at this conference would not have, and I as a reviewer do not possess, and hence cannot evaluate this paper. For example, I do not know bra-ket notation.\n\nIf the ACs disagree, I am happy to revise my review for this paper and try to be more thorough.","cats":["1"],"entities":[],"Comments":[]}
{"id":269,"text":"This paper is well written and motivated. The paper presents an approach that details how the model reconciliation process can be used for deception. This includes explanations that are in fact, lies. The paper considers lies of omission and commission, as well as multiple example walkthroughs. Highly relevant to the workshop and should spark interesting discussion.\n\nThis paper raises a number of very interesting questions for human-robot interaction:\n\nWould humans prefer a lie rather than the truth if it's simpler? Perhaps some of the best explanations , under certain circumstances, could be lies?\n\nIn the conclusion the paper states that most of the deceptive behavior described in the paper needs to be explicitly programmed. I wonder if a more developed agent capable of rebelling might lie to enforce ethical principles, perhaps in the case the goal asked of the agent is one of ill-intent. In such circumstances, the rebellion process might inform the deceptive behavior without it having to be explicitly programmed.","cats":["0"],"entities":[[0,41,"positive"],[296,368,"positive"],[370,455,"positive"]],"Comments":[]}
{"id":270,"text":"The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them. The idea is tested on very small data sets (80 and 50 examples, respectively). The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets.\n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove. Is the point that GloVe is a bad algorithm? That these steps are general? If the latter, then the experimental results are far weaker than what I would find convincing. Why not try on multiple different word embeddings? What happens if you start with random vectors? What happens when you try a bigger data set or a more complex problem?","cats":["1"],"entities":[],"Comments":[]}
{"id":271,"text":"This manuscript introduces a new approach for transfer learning of multi-label classification models. The paper is clearly written and presents compelling results on the efficacy of the method, especially on hard instances where images contain multiple objects with significant size differences.","cats":["0"],"entities":[[102,295,"positive"]],"Comments":[]}
{"id":272,"text":"I welcome the general direction of the work: trying to understand the dynamics\nof neural networks is a complex undertaking that a large community of\nresearchers is pursuing, so the study of simplified models is a promising\navenue.\n\n### Setup of the study\n\nIn studying the *limiting* dynamics however, the authors limit themselves to a\nsetup where I don't see any immediate connections to learning or representations\nof neural networks. The dynamics the authors describe happen *after* resuming\ntraining of a pre-trained neural network, thus I feel like their setup restricts\nthe potential impact of the results of this study.\n\n### Clarity \/ novelty of the results\n\nI found the article hard to read at times because the authors repeatedly qualify\ntheir observations as \"surprising\", \"contrary to common intuition\",\n\"nonintuitive\", etc. I think these qualifiers can be mistaken as claims of\nnovelty etc. and would hence use them more sparingly. For example, the fact that\nneural networks continue to move through their weight space has been\nwell-established for quite some time now, cf. for example [Jastrzebski et\nal. '17, Chaudhari & Soatto, 18, Baity-Jesi et al. '18, and many more]. Hence I\ndidn't find the observation in Figure 1 \"surprising\" (p 2 after the equation),\nwhich underlines the subjectiveness of these claims - or else I maybe reading\nthe figure incorrectly?\n\nAnother example: I would consider it well-established that OU processes whose\ndiffusion matrix is not isotropic do not follow the naïve Gibbs distribution,\nbut instead equilibrate in a modified potential (see for example Section 5.3\n\"Potential conditions\" of Gardiner's \"Handbook of stochastic methods\" etc.)\nFurthermore, modified losses arising through SGD dynamics have been studied in a\nnumber of recent deep learning papers, some of which are cited by the authors \n\nOther claims about the significance of the results should equally be clarified\nin my opinion, for example:\n\n> The expectation that the training trajectory would reflect the underlying \n> anisotropy of the training loss driving the dynamics is also wrong;\" (p. 9)\n\nIn my understanding, this study is only concerned with the *limiting* dynamics\nof learning, and hence conclusions about the training cannot be drawn\nimmediately?\n\n### Separation of experimental from theoretical results\n\nThe authors should be lauded for trying to connect their theoretical results\nwith empirical results on deep networks. Again though, I think the presentation\nof the results should be revised to clarify which predictions are actually\nderived from theory.  Take the exponent of anomalous diffusion (bottom of Fig\n6): it cannot be estimated of the global displacement (12), as the authors\nexplain. Instead, the authors evaluate the dependence of the diffusion constant\non learning rate, batch size and momentum parameter directly from a simulation\nby fitting a power-law to the empirical displacement. I would present this\nresult separately, as it is not a theoretical prediction, and thus presenting it\nin a section entitled \"Predicting the diffusive behaviour of the limiting\ndynamics\" could cause confusion in my opinion.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":273,"text":"The authors present an algorithm for postprocessing neural networks to ensure calibration under domain shift.\nCalibration under domain shift is an interesting challenge that has been receiving increasing attention and tackling this in an unsupervised manner is an interesting approach. However, I have 2 major concerns regarding the approach presented by the authors.\n\nWhat makes calibration under domain shift useful and appealing is that the model is then robust against any changes in the test distribution that can occur during the life cycle of a model. These often include erroneous\/samples (corresponding to truly OOD samples), but also gradual domain shift, where the test distribution continuously moves away from the training distribution (e.g. due to a continuous drift in user behaviour\/change in customer base) or unforeseen changes. My first major concern is regarding the requirements for UTS, which render this approach not very useful in many of these practical  applications: UTS first requires knowledge of and access to the test distribution; in addition it assumes that the distribution of the labels remains unaffected under domain shift. These assumptions are violated in the practical applications described above, in particular those where a gradual, continuous domain shift occurs - in this case, access to the test distribution is difficult since it changes continuously. On this note I also would have liked to see some analysis on how performance depends on the number of samples that are available from the test set, since in practice this might be substantially smaller than the full test set used.\nFurthermore, I find the assumption that the distribution of labels remains unchanged problematic (q_s(y) = q_t(y) and even q_s(y|x)=q_t(y|x)): once sufficiently out-of-domain, labels become meaningless and predictions for truly OOD samples should have maximum entropy. Even for small domain shifts in practical applications it is not clear why q_s(y|x)=q_t(y|x) should hold and it would have been useful to see a discussion and some robustness analysis on this.\nFinally,  the algorithm requires re-calibration whenever the test distribution changes, which in practice is  often not clear (and part of the reason why dealing with predictions under domain shift is so challenging). \n\nIn addition to doubts on practical applicability, my second major concern is regarding the depth of the evaluation.\nFirst, while the authors present some comparisons to probabilistic methods, I am missing a crucial comparison to Evidential Deep Learning (Sensoy et al, NeurIPS 2018), which results in far superior performance than deep ensembles, SVI or dropout. Importantly, the comparisons to probabilistic approaches presented by the authors are very limited. The big advantage of those approaches is that, once trained, no further recalibration is necessary and well calibrated predictions can be made for any level of domain shift, whereas UTS requires a recalibration step for very level of domain shift. That is why I think it is crucial to not only show one arbitrarily picked level of domain shift for each dataset\/perturbation, but calibration across all levels of domain shift, as for TS and TS-Target; since no recalibration is required for those probabilistic approaches \n this is very straight-forward and would be very informative - especially since e.g Figure 5 shows that UTS has only very minor advantages over TS in many settings. \nI appreciate that the authors report some performance in terms of ECE in the supplement, but I think it would be very informative to report performance in terms of ECE for all domain-shift experiments: The Brier score conflates accuracy with calibration (see eg the 2 component decomposition), whereas ECE directly quantifies calibration and is hence easier to interpret and arguably the more meaningful measure when quantifying calibration. \n\nMinor:  I find the manuscript lacks clarity. Aspects such as the definition of calibration as well as implications and interpretation of Proposition 1 should be described in more detail in the manuscript. \n","cats":["1"],"entities":[],"Comments":[]}
{"id":274,"text":"The paper proposes to do a coupled inference over pairs of geographically close images instead of a single image for satellite imagery. The coupling is done with an average pooling of the feature vectors when the neighbouring patches are detected to be similar enough based on a threshold on the L2 distance of these features. The method is applied to tasks of estimating crowding population, and diseases density, from satellite images. \n\nThe paper have little novelty. The approach reduces to a smoothing method over pairs of neighbouring patches, that is only activated sometimes based on a hard threshold. This seems arbitrary and there are many competing approaches that could be applied. \nOne could think about taking all the geograpical neighbourhood of a patch into account when making a prediction, e.g. with a coarse-to-fine prediction approach; the aggregation of features can be learned and more sophisticated than average pooling. Using a single-image baseline is not fair. The discussion is not up to the level of ICLR and offers mostly guesswork.","cats":["1"],"entities":[],"Comments":[]}
{"id":275,"text":"Update: I have read the author responses. As mentioned before I don't have the background to carefully assess the experiments and will have to rely on my fellow reviewers here. However I stand by my opinion that the experimental results would need to be very strong to warrant an acceptance, since the conceptual contribution is relatively limited.\nI was also disappointed by the authors unwillingness to back up their claims of \"theoretically convergent\" with a proof, or at least a theorem. Therefore I still tend towards rejecting the paper.\n-----------------------------------------------\nSummary\nThe present work proposes to use the recently developed Stochastic Gradient Langevin Dynamics (SGLD) to compute mixed approximate equilibria in two-player reinforcement learning, following the methodology proposed by hsieh et al for generative adversarial networks. The authors report practical improvements compared to pure strategies computed as proposed by Tessler et al.\n\nDecision\nThe idea of using randomized strategies for two-player reinforcement learning is interesting and natural. However, as the authors note, mixed strategies are classical in game theory. Furthermore, the adaption of the methodology of hsieh et al is straightforward, limiting the strength of the theoretical contribution. \nUnfortunately, the theoretical contribution is not stated consistently, since in the introduction, the paper states \"Our paper precisely bridges this gap between theory and practice in previous works, by proposing the ﬁrst theoretically convergent algorithm for robust RL\", but this claim is missing in the abstract or conclusion, and there is no theorem that justifies this rather strong claim. \nIn my opinion, this paper should only be accepted if it provides very convincing numerical experiments, which I am not qualified to assess. I happy to increase my score if the experimental results are deemed strong by the reviewers with more expertise in practical reinforcement learning.\n\nQuestions to authors\nYou write \"Our paper precisely bridges this gap between theory and practice in previous works, by proposing the ﬁrst theoretically convergent algorithm for robust RL\". What is the exact mathematical statement here? Does this refer to the algorithm that is used in the numerical experiments?","cats":["1"],"entities":[],"Comments":[]}
{"id":276,"text":"Summary: This paper discusses some features of simple_rl, a framework for RL in Python that emphasizes simplicity and tools for reproducibility.  This is a nice workshop paper but would benefit from a clearer discussion of the related work.  \n\nNotes: \n  -SimpleRL is an algorithm for RL experiments in Python.  \n  -After creating agents and MDPs, an experiment log as a json is produced.  \n  -Practitioners can share a copy of the experiment file to ensure reproducibility.  However with docker or containers this should always be achievable, unless there’s some guarantee that all of the seeds are in the experiment file?  \n  -Consists of MDP objects and agent objects.  \n  -Main design goal is simplicity.  \n  -MDP has “transition function” and “reward function” objects.  I wonder how well the structure generalizes to model-based RL?  \n  -Some utilities for reproducing results from the json files.  \n  -Plotting utilities are included.  \n\nComments: \n  -Section 2 could do a better job of making it clearer how the simplicity of simple_rl isn’t achieved by the other libraries.  Nonetheless, it’s still a nice overview.  \n","cats":["0"],"entities":[[146,240,"positive"]],"Comments":[]}
{"id":277,"text":"This paper proposes a method for automatically generating accompaniments using Mel-spectrograms as inputs to a CycleGAN. Overall I think the paper requires significant revision and additional work before it can be accepted as a conference publication. \n\nTitle: \n\n-The title is misleading. The title claims that the proposed model is for \"Automatic Music Production\". However the actual task considered is more restrictive. The authors propose a model for automatic accompaniment. Music Production involves many other tasks like mixing, mastering and so on, none of which are a part  of this study. The title should therefore be updated to be more specific. \n\nAbstract: \n\n-\"Despite consistent demands from producers and artists...\": I think this sentence should be rephrased to motivate the need for automatic accompaniment from a different angle. If not, the authors should present some justification for the demand for this technology from artists and producers. \n\n-\"Automatic music arrangement from raw audio in the frequency domain\": why not simply say automatic music arrangement\/accompaniment in the Mel-frequency domain? I find the raw audio part of the description unnecessary and confusing. \n\n-The authors claim that the they are the first to treat music audio as images and then apply techniques from computer vision. However, treating spectrograms as images is the current standard for many MIR tasks like music transcription, chord recognition and so on e.g. \"An end-to-end Neural Network for Automatic Music Transcription\": https:\/\/ieeexplore.ieee.org\/abstract\/document\/7416164\/. There are hundreds of other publications that are similar to this approach. \n\nIntroduction: \n\n-The authors claim that automatic accompaniment in the waveform\/frequency domain has many advantages. However they fail to motivate the short-comings of this approach. Namely the lack of source separated training data and the extreme difficulty in source separation for music recordings. It  would also be useful to cite a review paper or some of the many publications on automatic accompaniment generation in the symbolic domain so that the reader can find references to this problem which has an extensive literature already. \n\n-The authors mention that they use the Demucs algorithm for source separation. However they do not provide any details whatsoever about this approach, especially the downsides. A quick scan of the paper reveals that the algorithm introduces severe artefacts under various conditions. \n\n-The authors mention the low-computational cost of their proposed method, however they do not satisfactorily quantify this claim. Firstly, is computational cost an issue? Does this algorithm have to run on a mobile device? Will it be run in a streaming setting? These questions are not answered in the paper. \n\nRelated Works:\n\n-The authors cite many papers on music generation in the waveform domain however they do not cite any of the extensive literature on music generation in the symbolic domain. This literature is extremely relevant to the work presented in this paper. \n\n-\"Nevertheless, only raw audio representation can produce, at least in the long run, appealing results in view of music production for artistic and commercial purposes.\" Why is this the case? Why is generating music in the symbolic domain and then using state-of-the-art synthesisers not an appealing direction? This point isn't made clear in the paper. \n\nMethod:\n\n-There are no details provided about the Demucs algorithm used to separate the source training data into various channels like vocal, bass, drums etc. How big was the model? Did the authors train the model themselves? Did they use a pre-trained model? Were there any artefacts present in the source separated tracks? Are there any downsides to this algorithm? Are there any alternatives to this algorithm? Do the artefacts not interfere with the  downstream task? \n\n-A reference\/citation about the Mel scale would be useful. \n\n-There are no details about the CycleGAN used in the paper. How big is the model? What is the architecture? How was it trained? What flavour of gradient descent was used for training? What are the hyper-parameters? Was the model trained on a single GPU? \n\nExperiments:\n\n-How was the subset of pop music selected? How was the metadata filtered to obtain the 10000 tracks used for training? If the filtering algorithm cannot be outlined, then it would be useful to provide a list of the 10000 tracks used for training, for the purpose of reproducibility. \n\n-How did the authors arrive on the 4 attributes quality, euphony, coherence and intelligibility? Is there some theory that suggests that these 4 attributes would be useful in determining whether the accompaniment is somehow good? These attributes have been presented without justifications and citations. \n\n-The features (STOI, FID) used to compare the automatically generated accompaniment have also been presented without much justification. Why is it that these features  are an adequate representation of the generated audio? \n\n-I found the description of the grades and the subsequent comparison in Figure 3 difficult to follow. I think the description needs to be significantly more rigorous. \n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":278,"text":"Overall seems fine to me. Some nits:\n- Any sort of model outside the ResNet family would have been nice to see.\n- Figure 1 seemed crowded and it would have been useful to exclude some of the less necessary lines. Also the two different styles of dashed lines are hard to pick apart at first. Is it necessary to visualize the trends across \\rho, or could you just have used the best \\rho value and put them in a table?","cats":["1"],"entities":[],"Comments":[]}
{"id":279,"text":"This paper aims at merging Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs so that the agent can make use of expert trajectories, even when the experts are suboptimal. This paper aims at addressing this by theoretically using the \"Free Energy Principle\", which the authors define in the abstract as a unified brain theory that explains perception. \n\nThe paper tackles an important problem and develops many theoretical functionals. The approach is then tested on a few benchmarks from the DeepMind Control Suite where the approach is reported to work well.\n\nMy main concerns are related to the clarity of the paper, which does not allow me to understand some key parts.\n\nIt is unclear what are the minimized loss functions: it is mentioned that equations 27-29 sum up all objective functions. Since F_t and G_{t+1}^{RL} are appearing twice, does it mean that those losses have a two times more important contribution than for instance G_{t+1}^{IL}? In addition how exactly are equations 27-29 derived from the other equations?\n\nNotations are not always consistent:\n- F_{IL} F_{RL} are sometimes with curly F sometimes not (equations 27-29 and the few lines that follow Figure 1 page 5).\n- First line equation 7 and first line equation 10: Why is there a subscript for F and not G? In other parts of the paper, a subscript is used for G as well.\n\nSome sentences are unclear:\n- \"in RL, using the value function to predict rewards in the long-term future is essential to avoid a local minimum and achieve the desired goal.\" What kind of local minimum does that refer to?\n\nA few typos:\n- \"Then We (...)\"\n- \"the agent easily fall down\"","cats":["1"],"entities":[],"Comments":[]}
{"id":280,"text":"Summary: \n\nThis paper proposes Deep Coherent Exploration that unifies step-based exploration and trajectory-based exploration on continuous control. There exists a prior work that bridges a gap between the two exploration methods for linear policies, and this paper generalizes the prior work for various deep RL methods: on-policy (A2C, PPO) and off-policy (SAC). Finally, Deep Coherent Exploration enhances the performance of baseline algorithms and has better performance than prior works (NoisyNet, PNSE) on Mujoco tasks.\n\nPros:\n\n+ For combining the proposed method with on-policy learning, this paper derives the log-likelihood of whole trajectory recursively.\n+ For on-policy methods (A2C, PPO), the proposed method has large performance gain on Mujoco tasks.\n\nCons:\n\n- The idea of this paper directly follows GE [van Hoof et al., 2017] and is not much different from GE.\n- For SAC, the proposed method is not much effective and it even degrades the performance of the HalfCheetah task.\n- The paper focuses on exploration, but the experiments only focus on the return performance of simple Mujoco tasks.\n- In order to show the superiority of the proposed method, additional experiments on pure exploration or sparse rewarded tasks are needed.\n\nMinor concerns:\n\n* In background, there is no explanation about step-based and trajectory-based exploration.\n* For the off-policy case, there is insufficient explanation for why they use single sigma and the connection point of the proposed method and eq (5).\n","cats":["1"],"entities":[],"Comments":[]}
{"id":281,"text":"Pros:\n- SOTA results\n\nCons:\n- generally written in an unclear way \n- Title should say \"zero-shot\"\n- The indices of the covariance matrices in Fig 1 appear flipped\n- Figure captions lack important information (for example, in Fig 1 there is no mention that the bottom VAE is for the class embeddings, and they also use c in the figure but c(y) in the text)\n- not clear whether the results in Fig 2 for ImageNet are over multiple seeds, no error bars.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":282,"text":"Strengths:\n\nThis paper evaluates the success of deep neural networks by running extensive experiments that show the tradeoff between space and frequency. \n\nWeakness:\n\nThe meaning of the word frequency is unclear in the context of this paper.\n\nThe main idea behind theorem \"eigenspace restructuring\" which replaces one eigenvector\/value computation with a set of part based eigenvalue\/eigenvectors  has limited novelty.  The novelty is in its use in neural network architecture analysis.\n\nThe authors might want to consider evaluating the performance of a system based on the amount of data employed and based on how data was acquired (observational vs experimental studies).  Systems that employ data from observational studies are known to be susceptible to selection bias and spurious correlations, as opposed to data from experimental studies that are employed in objective causal inference.  \n\n\nMissing reference:\nAs the authors might recall, Vasilescu etal. in their ICPR 2020 paper have advocated replacing the SVD computation with a set of part based SVDs for which they provided a closed form mathematical derivation.  Based on this mathematical derivation, they developed  the Incremental Hierarchical M-mode Block SVD which was demonstrated experimentally. \n\n@inproceedings{Vasilescu20,\n\nauthor={Vasilescu, M. Alex O. and Kim, Eric and Zeng, Xiao S.}, booktitle={2020 25th International Conference of Pattern Recognition (ICPR 2020)}, title={Causal{X}: {C}ausal e{X}planations and {B}lock {M}ultilinear {F}actor {A}nalysis}, year={2021}, location={Milan, Italy}, month={Jan}, pages={10736--10743} }\n\n\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":283,"text":"This paper extends existing casual learning methods to the nonstationary video cases, where the casual structures are changing throughout time. The proposed model leverages a recurrent model GRU to extract temporal information, which is intuitively correct because this will capture the changes of casual graphs over time. Since there is no GT casual graph as direct supervision, the authors employ state-space model and perform variational inference on it. The proposed model achieves superior performance compared to one previous work on one dataset\n\nDetailed comments:\n- I completely agree that the evolution of casual structure in nonstationary videos is indeed an important and new problem. The usage of GRU to tackle this problem seems reasonable to me\n- However, since there is no GT casual graph in the dataset, the author doesn't visualize the casual graph learned in any case. It's unclear to me whether SSM and eq3 can really help the model learn the correct underlying casual graph\n- There are many typos\/writing errors in the paper. At the beginning of Section 2, the referred model architecture figure is missed. In Table 1, the baseline V-CDN is mentioned but missed in the table\n\nNonetheless, I still believe the discovery of casual structure in nonstationary videos is an important paper. So I recommend acceptance of this paper to the workshop.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":284,"text":"This paper proposes an efficient method for survival analysis using neural networks. I'm not familiar with the topic, but the paper is well-written and initial results seem to look good. ","cats":["0"],"entities":[[0,187,"positive"]],"Comments":[]}
{"id":285,"text":"In this paper, the authors investigate generalizing VAEs to problems where correlations can be found between data points. They leverage known results (namely, writing the  distribution on tree-shaped graphical models as a function of pairwise joint distributions and marginals; for general graphs, they use mixture of tree distributions in a fashion reminiscent of tree-reweighted belief propagation) from graphical models to derive tractable evidence lower bound for the problem of interest.\nThey report improved results on a variety of datasets (spectral clustering, collaborative filtering and link prediction) compared to vanilla VAEs and a graph net based approach.\n\nThis is a good paper; the ideas are original, technically interesting, and well presented (there are some issues with language but nothing distracting), and results are convincing.\n\nThere was a slight missed opportunity in explaining in further details how the new bound interacted with sampling from the posterior (this is currently hidden in appendix D: the authors leverage properties of gaussian distributions; in general it would be more complicated to sample from the joint knowing only pairwise\/marginals).\n\n\n","cats":["0"],"entities":[[672,852,"positive"],[854,924,"positive"]],"Comments":[]}
{"id":286,"text":"The paper's presentation is very clear and I appreciate the idea to combine meta-learning and the options framework, but I do not think that the experiments fully demonstrate the potential of the method. First of all, the experiments in the paper focus on navigation. Ant Maze environment is not a challenging task for other hierarchical RL methods. The difficulty comes from forbidding the agent from observing the maze layout. However, since the maze size is rather small, the number of possible layouts is also small. It is possible that that the agent simply remembers the solution to all map layouts and uses the meta-learning trajectories to identify the maze layout. The experiments do not demonstrate the generalizability of the novel map layout. In this case, why not simply learn a recognition model to reconstruct the maze layout [1] and then learn a goal-conditioned policy? The approach is not significant unless it can generalize to a novel layout in a larger maze. Otherwise, I would doubt that we should use search instead of meta-learning for solving complex planning tasks. Besides, since the number of options is small, and the results are not very interpretable, if we need options is questionable to me. The paper lacks a MAML-like meta-learning baseline to demonstrate that options are necessary.  \n\nBesides, I am confused by the plots in the paper. Fig 4 shows that FAMP reaches 1200+ score after 40 episodes, but according to the pseudocode and Table 2, in each update step of the meta-agent, it needs to sample N\\times L+1\\times k episodes, which is larger than 40. Does this mean that there is no need to learn the meta-agent?\n\nIn short, the paper has good motivation, however, the experiments have flaws and are not strong enough to demonstrate the potential of their approach. I hope the authors can apply their approach to more complex domains and show its significance.\n\n[1]Multi-task Batch Reinforcement Learning with Metric Learning","cats":["0"],"entities":[[0,203,"positive"],[1654,1804,"positive"]],"Comments":[]}
{"id":287,"text":"This paper describes three alternate approaches for scheduling awake and sleep cycles for a space rover.  While awake the rover uses more energy than it produces and can only recharge its resources when it is asleep.  The three approaches vary in terms of soundness and completeness.   All approaches are sound but only one is complete.  After presenting the approaches an empirical evaluation is given comparing both the quality and the runtime of the solutions.  The evaluation shows that the naïve max duration approach. which is sound but not complete,  performs worst in terms of solution quality and runtime.  While the sound and complete linear approach produces the best quality solutions its runtime is worse than probe approach which has comparable quality.   \n\nPlanning activities for a Mars rover is highly relevant to SPARK. \n\nThe initial figure could explain why it might be useful to extend an awake period while the rover is idle.  The figure clearly shows the two cases but gives no intuition as to why one case might be preferable to the other.\n\nThe probe approach depends critically on the heuristic choice of the probe point.  More discussion and\/or an evaluation of strategies for choosing the probe point would be valuable given that the analysis points to the probe algorithm being a good compromise between quality and runtime,\n","cats":["1"],"entities":[],"Comments":[]}
{"id":288,"text":"The paper introduces the idea of using cyclical learning rate schedules to benchmark the performance of different algorithmic and model changes. Usually, benchmarking these methods requires training for multiple different budgets (e.g. for 16, 32, 64, ... epochs), but cyclical learning rates allow an approximation of these results with a single training run (for the full duration).\n\nThe paper introduces a neat and practically helpful method for faster benchmarking of algorithmic and\/or model changes. The experiments show that the cyclical training can consistently provide a decent approximation of the \"true\" tradeoff curve while requiring only half of the training costs.\n\nFeedback:\n- Line 67 mentions that the experiments report top-1 validation accuracy. Judging from the results, I would guess that this describes the `final` achieved performance (i.e. what the final iterate produces) and not the `best` achieved performance (i.e. what the `best` parameters observed during training achieve), right? Otherwise, the tradeoff curves should never be decreasing (for the cyclical learning rate). However, in Line 73 you describe \"we then stored maximum accuracy values at the end of each cycle\". I don't understand what set the `maximum` refers to. I would be interested in seeing the results using the `best` performance instead of the `final` (would be sufficient to show it in the appendix). This would not require any further experiments.\n- Could you provide some more details on how you set the (upper) learning rate of both the standard and the cyclical training? Line 72 mentions using a value of 2.048 for the cyclical training. Could it be that some methods just work better with this learning rate (without providing better performance at a tuned value)?\n- I would be interested in hearing your thoughts on whether this type of benchmarking \"trick\" could also be used in other domains, besides ImageNet training. Would you anticipate any problems using the cyclical learning rate schedule in other domains?\n\nNits:\n- Line 56: Should probably use a \\citep, e.g. \"[Huang et al., 2017]\". This occurs multiple times.\n- Line 60: doubling of \"to\".","cats":["0"],"entities":[[386,505,"positive"]],"Comments":[]}
{"id":289,"text":"This paper proposes to simply learn\/optimize the deep kernel parameters and hyperparameters of the GP using the data from all tasks (equation 9) and use such a deep GP kernel for BO. Instead of maximizing the log marginal likelihood of a single dataset (as is typically the case for a learning task), they propose to maximize the sum of log marginal likelihoods over the datasets of all tasks, which I view to be an incremental technical contribution. This paper is not about innovations in the acquisition function in BO. Their proposed approach outperforms the tested methods on 3 benchmark datasets.\n\nA drawback of their proposed method (equation 9) is that in contrast to some existing meta-BO algorithms, it cannot exploit the GP posterior means and variances when such information is available from previous BO tasks. \n\nConsidering the diversity of the types of datasets in the AdaBoost experiment, can the authors give an interpretation of the max likelihood estimates of theta and w in equations 8 and 9 in the context of this experiment?\n\nFor the GLMNet and SVM experiments, would it be possible to instead combine all the datasets over tasks and construct a *joint* likelihood over them in equation 9 instead of a sum of likelihoods over tasks? How would the results differ in this case?\n\nFor the experiments conducted, large amounts of data are drawn from many available previous BO tasks, especially for GLMNet and SVM. This seems to be in disagreement with the setting of BO where the unknown objective functions are expected to be costly to evaluate (e.g., in hyperparameter optimization). In the context of BO, it would be meaningful to consider the practical setting where only small amounts of data are available from a few expensive BO tasks. In this case, how would the proposed approach perform compared to the tested methods?\n\nAn empirical comparison with the state-of-the-art meta-BO algorithm: weighted GP ensembles (Feurer et al. 2018) should be included.\n\nFrom the results presented in Table 1 and Fig. 2, it does not seem like only a few shots\/BO iterations are needed for the experiments performed in this paper. How do the results compare when only a few shots are used?\n\nEquations 9 and 10: Shouldn't the log marginal likelihood be over y's instead of f's?\n\nEquation 13: Exactly how is the loss function L(f^(t),X) defined? The author mentions that it is a normalized regret. The exact expression is needed here. In particular, I have noticed that f^(t) is not in bold: does this mean that only 1 \"test\" point is selected per task t?\n\nPage 6: Can the authors provide the exact details on \"the settings being sampled in proportion to their performance according to the predictions\"?\n\nThe authors can consider doing experiments on hyperparameter optimization of larger-scale CNNs, which is commonly seen in BO works.\n\n\nMinor issues\n\nPage 3: adaption or adaptation\n\nThe following reference would be relevant to the context of meta\/transfer BO:\n\nZ. Dai, B. K. H. Low, and P. Jaillet (2020). Federated Bayesian optimization via Thompson sampling. In Proc. NeurIPS.","cats":["1"],"entities":[],"Comments":[]}
{"id":290,"text":"### Summary\nThe paper proposes a neuro-symbolic approach to solve reasoning tasks over KBs. The proposed LMLP approach uses logic rule templates and in-context learning of LMs for answering a relational query. To evaluate  LMLP, the authors conducted experiments on two datasets. In doing so, the paper aims to answer: what is the right representation for in-context samples? How does natural language explanation compared with symbolic provenance when acting as prompts? The conducted experiments show that eliciting LMs with logic rules and in-context learning, leveraging LM’s pre-trained knowledge, could be sufficient for solving reasoning tasks over KBs.\n\n\n### Strength\nThe proposed method is well described and supported by illustrations and examples, which makes it easy to understand.\n\nWell-written related work is compressed in the main text and extended in the Appendix.\n\nExperiments seem to be well conducted.\n\n### Weaknesses\nHowever, while the experimental results are displayed in Tables 2 and 3, and further details are discussed in the Appendix, a discussion in the main text is missing. \n\nSince I do not see other major weaknesses and the remaining parts are well written, and the experiments seem to be well conducted, I would still vote for acceptance if the authors add a discussion of the experiments similar to the ones in the appendix. I suggest moving Tables 3 and 4 to the Appendix instead.\n\nMinor:\nLine 171 Typo: We probes\nThe first bullet in the checklist: Section ??\nwhat is the right representations for -> \"representation\" or \"are\"","cats":["0"],"entities":[[676,793,"positive"],[1106,1235,"positive"]],"Comments":[]}
{"id":291,"text":"The paper looks at the problem of computing interventions from an external observer in the plan of an actor in the presence of an active or passive adversary. I had some trouble situating the work (though the related works section at the end certainly helps). \n\n> Assumptions: The assumptions made in the framework should be clearly stated. Section 3 starts with a list of them, but this is largely incomplete (e.g. some more appear in later sections and some more are implicit and are never mentioned at all). I would strongly suggest laying out what all the assumptions on each of the three agents are and how that impacts the intervention framework. Are the agents taking turns? \n\nThe actor seems to be a vanilla optimal agent, while the observer is bearing the computational burden of having to reason about attacks. This is different from agents that directly reason about the observer model (c.f. The landscape of Interpretable Agent Behavior [Chakraborti et al. ICAPS 2019]); This is the same trade-off in goal recognition design as well (with behaviors like legibility). I wonder what the differences are in the context of the intervention setting? Would be good to have that discussion in the related works.\n\nWhy are G_d and G_u part of the input? Given that the observer has the full model of both agents, interventions are exactly and precisely computable (c.f. Optimal Interdiction of Attack Plans [Letchford and Vorobeychik]). Why generate features and then classify? Is this purely a computational consideration or am I missing something?\n\nFinally, and this is minor, it feels very strange to see \"we use machine learning to determine...\" in a technical paper. This is akin to \"we used AI\" in a magazine. Why not just say what it is? A classifier, a decision tree, etc. \n","cats":["1"],"entities":[],"Comments":[]}
{"id":292,"text":"This work is concerned with developing a model that can produce a manipulation program in order for a robot to manipulate its surrounding to reach a goal state given an initial state. The work focuses here on a neruo-symbolic approach that includes several specialized submodules. Despite these differences all submodules can be trained end-2-end without intermediate supervision. The experimental results are promising.\nOverall this is a well written paper with a very relevant topic and proposed method for this workshop. I am hesitant on giving this paper the highest score due to two reasons that are somewhat correlated. The overall methods section in \"Technical Approach\" is quite minimilistic and scarce in details. E.g. particularly the description of the visual extractor is kept very short. Even if it is mainly based on previous work I believe it would be helpful to have some more information overall in this technical approach section. This leads to one of the more important contributions of the work, the single loss function (section 3.5), to be less obvious, i.e. how the gradient flows through to all initial modules. And particularly in my view this is the most interesting feat of the work, i.e. that training all submodules works via this one loss. Maybe adding more explicit mathematical notations in Section 3 could help for the comprehensability here.  ","cats":["0"],"entities":[[421,523,"positive"]],"Comments":[]}
{"id":293,"text":"I thank the authors for their submission. I believe the investigated content is relevant and timely and would perhaps benefit from the discussion in a community such as the one of ICLR. Please find my comments below, as potential points of discussion.\n\nHigh-level comments:\n* generally, the paper does a good review of existing literature and aims to relate two important subfields (abductive vs contrastive explanations) using rules of logic, particularly using the minimal hitting set relationship \n* beyond showing that such a duality holds between abductors and contrastive explanations, I believe the experimental section should further explore comparisons with related work such as Dhurandhar et al. (as cited in earlier sections) and Rebeiro et al.\n* furthermore, the idea of using FOL for generative contrastive explanations (also sometimes called counterfactual explanations) has been explored before (e.g., [Karimi et al.]);\n* on the presentation of material, there seemed to be an underwritten requirement to be familiar and have a background in logic and verification, which makes me wonder whether ICLR is the right venue here (I also apologize for not being to provide much feedback on the technical front)\n\nMinor comments and nits:\n- the footnotes seemed to contain important details, but the number of footnotes seemed overwhelming and hurt the flow of reading\n- [footnote 9] seems incorrect; e.g., non-linearities in MLPs or RBF kernels in SVMs cannot be encoded as first-order logic\n- perhaps figure 2 can be redone to more visually demonstrate the benefit of using the proposed method; if the provided explanations aren’t visually appealing (relatively), then perhaps consider a non-image-based dataset?\n\n[Ribeiro et al.] https:\/\/homes.cs.washington.edu\/~marcotcr\/aaai18.pdf\n[Karimi et al.] https:\/\/arxiv.org\/abs\/1905.11190 ","cats":["0"],"entities":[[276,381,"positive"]],"Comments":[]}
{"id":294,"text":"This is a well-written, clear, and relevant paper to the conference.  The paper introduces a framework for explaining scheduling and planning failures by generating minimal conflicts and relaxations. The work is well motivated by a description of planners at NASA and is used as a running example throughout the paper. The problem of explaining why a plan fails is a core focus of this workshop thus this paper will be highly relevant to the audience.\n\nSmall comments \/ grammar:\n\t* 6.1: \"natural language sentences out the explanations\" - \"out of\"?\n\n\n\n","cats":["0"],"entities":[[0,67,"positive"],[200,318,"positive"],[319,453,"positive"]],"Comments":[]}
{"id":"ukUut7k50d","text":"Different from existing standard encoder-decoder networks, it proposes an explicit model for correlating fixed (inhale) and moving (exhale) image features. The features are extracted at sparse key points and transformed into a compact representation by CNN. Then a displacement map is calculated to measure their dissimilarity and further represented as displacement embedding.\n\nI think the performance of the proposed method could be affected by the number of key points and the size of the displace locations, although it is understandable not to include all the details due to the page limit. Also, there might be a possibility of estimating unnaturally aggressive deformations since they are estimated based on key points only. In the experiment shown in Table 1, it shows improved performance compared to other deep learning-based methods, including VoxelMorph and less obviously improved result compared to other algorithms for large deformations. ","Comments":[],"cats":["1"],"entities":[]}
{"id":"uineFvBjwqD","text":"Summary: The paper  presents a data augmentation framework for zero-shot cross-lingual transfer learning. The framework uses different types of data (labeled source data, unlabeled source data, automatically generated augmented data) for training a model for the target language. Experiments are conducted on three different tasks: Named Entity Recognition (NER), Natural Language Inference (NLI) and paraphrase identification (PAWS). The approach combines multiple components together namely self-training, augmented sentence generation and confidence penalty.\n\nData Augmentation:\nThe paper states that data augmentation has been successfully used in images but not so much in  text (excluding back-translation). In fact, replacing creating augmenting text by masking and replacing word has been used in NLP both before and after pre-trained LMs such as BERT, etc.\n\n- Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations. Kobayshi. NAACL 2018\n- Conditional BERT Contextual Augmentation. Wu et al. 2018.\n- AUG-BERT: An Efficient Data Augmentation Algorithm for Text Classification. Shi et al. CSPS 2019\n- A lexical and frame-semantic embedding based data augmentation approach to automatic categorization .... Wang and Yang. EMNLP 2015.\n\nThe paper argues that generating new samples for data augmentation using the vicinity distribution of the source and target samples is better than back-translation since you cannot transfer the labels in sequence tagging tasks with back translation. However, similar problems would occur with vicinity distributions based augmentation since even changing a single word may result in changing the meaning of the sentence and hence the label (as the paper argues in Section 3.3.) . Given that, it would be useful to see more discussion\/experiments comparing the two types of augmentation strategies especially that the self-training step can leverage augmented data without labels\n\nExperiments:\nThe paper provide a lot of interesting ablation and analysis. However one of the key questions that I couldn't get an answer to is what is the value of each source of data and could they be used in any different way. For example, what happens if we only do data augmentation for the source only, or the target only or by using translation or back-translation, etc. \n\nOn a related note, it looks like the specific order in which the datasets are used is important as shown in the experiments but it is not very clear what is the intuition behind that choice and whether other choices were considered or tried.\n\nOther questions:\n\nWhat is the benefit of Successive cross (vs. successive max)?\n\nWikiann is much bigger and has covers 40 languages. any reason why only 3 languages were considered from Wikiann and other languages from CoNLL?\n\n -----\nEdited after authors responses:\nI would like to thank the authors for the detailed response and the changes they have made to the paper. \n \n- Regarding contextual data augmentation (Kobayshi et al., Wu et al., etc.): Thanks for pointing out that these method use the labeled data to finetune the LM to make sure that words are replaced with other \"label-compatible\" words. Note that the comparison is not intended to necessarily show that the proposed method outperforms these baselines. Rather it is intended to guide the reader into making a decision about which method is more appropriate for which problem. If the findings are that the performance is comparable but one method will eliminate the additional finetuning step, that would be a useful finding to share. Also , it is not clear that  these methods would require labeled data in the target language for the finetuning or not.  For example, can the source labeled data be used for the finetuning step?\n\n- Regarding translation: Cross-Lingual transfer via Machine Translation does suffer from the label transfer problem for sequence tagging tasks (transferring labels for sentence-level tasks is straightforward). However, there has been several methods to address this in the literature by using unsupervised word alignment  (e.g., Yarowsky et al., 2001; Ni et al., 2017) or attention weights from NMT models (Schuster et al., 2019), heuristic approaches (e.g., Ehrmann et al., 2011) or co-learning alignment and tagging (Xu et al., 2020).\n\nA detailed comparison and discussion of the trade-off between the performance of each method and the resources they require would make the paper much stronger ","Comments":[],"cats":["0"],"entities":[]}
{"id":"SMMg9-chRzc","text":"This submission is very badly written, it is not clear even on a basic level what is going on. The allusions to word2vec perhaps seem as if there is a 1-hot encoding of datapoints in the first layer which are embedded and then penalized to generate other datapoints, but which ones? neighbors? distal points? If it is just the point itself then this is no different than an autoencoder with a reconstruction loss. This has to be clarified. \n\nThe results of the MNIST embedding look very unconvincing as tSNE is showing better seperation. Also to compare visualizations I would suggest both looking at Moon et al. https:\/\/www.nature.com\/articles\/s41587-019-0336-3  for both the PHATE method as well as metrics such as ARI and DeMAP that are used for comparison.  ","Comments":[],"cats":["2"],"entities":[[0,94,"non-productive"],[95,439,"ify"]]}
{"id":"Wiv9mUy4jsb","text":"The authors present a new adaptive gradient method based on Nesterov acceleration. The results look very promising and the work is well presented. ","Comments":[],"cats":["0"],"entities":[]}
{"id":"2MOwojoQoyY","text":"This paper develops a new, more efficient way to approximate the Fisher using a layer-wise local approximation, for use with natural gradient descent in deep learning. The manuscript is well written and easy to understand, and provides compelling results on the efficacy of the proposed method on certain problems when used in conjunction with the Shampoo optimizer.","Comments":[],"cats":["0"],"entities":[]}
{"id":"SyefBu5O3m","text":"This paper proposes simple metrics for measuring the \"information density\" in learned representations. Overall, this is an interesting direction. However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain. And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared. \n\n+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important. \n\n+ The proposed metrics and simple and intuitive.\n\n+ It is interesting that a few units seem to capture most task specific information. \n\n- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here. As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task. Yet the metrics proposed depend on supervision in the target domain. If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set. It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks. I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue. \n\n- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed. The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.\n\n- The CFS metric depends on a hyperparameter (the \"retention ratio\"), which here is arbitrarily set to 80% without any justification.\n\n- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'. \n","Comments":[],"cats":["1"],"entities":[[1788,1919,"ify"]]}
{"id":"ZuM4hIEX36","text":"**Strengths**\n\n1. The empirical evaluations of this paper are comprehensive and solid, the results and findings are interesting. \n2. The paper is well-organized and clearly written. \n\n\n**Weaknesses**\n\n1. Despite good empirical efforts, the motivation of this paper seems somewhat unclear. Given that the encoder-decoder paradigm dominantly governs machine translation (also from the experiment part of this paper that LMs underperform EncDec most of the time.), for what reason should we need to consider a shift to a unified language model for such a seq2seq task? If the zero-shot transfer is the case, why not directly fix the off-target issue for EncDec and preserve the good of translation performance, instead of changing the paradigm? I feel like the authors didn't convey an incentive for this. \n2. The paper conducted extensive experiments to show how scaling affects LMs for MT, however, few suggestions based on the findings are given for future development of MT. \n3. The setting of CausalLM seems a bit weird that a unidirectional encoding behavior makes obviously no sense. \n\n**Questions**\n1. Section 5 seems to mainly examine p and L_{\\inf} In equation (7), whereas \\alpha remains undetermined. How was the value of \\alpha determined for diagrams in Figure 3? Get fitted from Figure 2 I guess? \n","Comments":[],"cats":["1"],"entities":[[980,1088,"non-productive"],[1275,1308,"ify"]]}
{"id":"4H1jTVEwu_K","text":"In this paper, the authors investigate a risk-based ring vaccination strategy. Ring vaccination is a vaccine allocation strategy that vaccinates the contacts and contacts-of-contacts of an infected case. Here, the authors use an agent-based model to simulate an Ebola outbreak and test a variant of ring vaccination that prioritizes individuals within the contact-of-contact network with the highest risk (with risks estimated from the model). They show through their simulations that risk-based ring vaccination is significantly more effective than ring vaccination without prioritization, especially when more doses (100 or 200) of the vaccine are available.\n\nStrengths\n+ Risk-based ring vaccination is a nice idea and well-motivated\n+ The authors clearly demonstrate the effectiveness of this strategy through simulations\n+ The model is largely motivated by prior literature and uses parameters from prior work\n\nWeaknesses\n- The results feel almost like a foregone conclusion given the model, since they use risks from the model to decide which individuals to prioritize. It would be useful to establish, especially through mathematical analysis if possible, if we should be \"surprised\" by the results, or the settings that must hold true for risk-based to be significantly more effective.  \n- A lot of design decisions are made within the model, eg, levels of contact and types of contact within households\/across households, disease parameters, etc. While it helps that parameters were mostly set based on prior literature, it would be useful to conduct sensitivity analyses to see how model results vary based on the decisions made.\n- Unclear if authors were the first to do risk-based ring vaccination. Also, unclear how realistic this model is in real life, since their simulation uses the individual's \"real\" risk from the model to determine prioritization. In reality, it seems hard already to get an infected person's contacts and contacts-of-contacts; would be even harder to know levels of contact\/risk between all these people.","Comments":[],"cats":["0"],"entities":[]}
{"id":"ryx6YWDLo4","text":"This paper addresses the problem of optimizing dynamic ridesharing in a setting that accounts for social preferences of requestors.  The primary technical contribution is a heuristic, real-time algorithm for matching requests to drivers that seeks to balance operational utility (vehicle operator's perspective) with user value.  The model of user value was derived from an extensive survey. A series of evaluations based on real-world taxi data shows that the approach improves user value without significantly affecting operational value for the service provider. \n\nThis is quite a nice paper:  interesting and timely topic area, technical solution well matched to the problem, solid experimental methodology, and good results.  The presentation and level of scholarship in the paper is high. \n\nA couple of comments:  \n-\tThe paper states that 5 minutes was set as an acceptable threshold for increased ride time by users.  But surely that threshold would depend on the length of the ride?   \n-\tThe increase in ride time is simplistic in that it accounts only for distance traveled.  More realistically, there would also be \"stopping time\" associated with pulling over, passengers entering\/exiting the vehicle, retrieving luggage from the trunk, etc.  \n-\tThe population model was derived from a large-scale survey with \"demographic information ... drawn from the actual Chicago demographic distributions\".  Won't those demographics differ from the demographics of people who use ride-sharing services?\n-\tThe overall algorithm design separates Trip Formation from Trip Dispatch, leading to two separate heuristic planning algorithms.  Did you consider a uniform optimization algorithm that would combine both functions?   \n\n","Comments":[],"cats":["0"],"entities":[[1410,1504,"ify"]]}
{"id":"Qm3pJMoqY4","text":"##########################################################################\nSummary:\n\nThe paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording. Furthermore, the paper validates that an end2end transformer model trained on both camera and radar data outperforms an end2end transformer model only trained on camera data and hence, supports the argumentation to add a radar sensor to an automated vehicle.\n\n##########################################################################\nReasons:\n\nOverall, I vote for rejecting the paper. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. The result that camera and radar data combined will outperform camera data only is expected. Using detections in images to guide the radar reverses its advantage to work well in adverse weather conditions compared to the camera.\n\n##########################################################################\nPros:\n\n* Interesting and relevant topic\n* Training results support claims\n\n##########################################################################\nCons:\n\n* Lack of algorithmic novelty\n* Use of Faster R-CNN (slow)\n* Using camera to select most important radar regions contradicts the stated advantage of radars to perform better in adverse weather conditions\n","Comments":["A good example of a critical yet academically productive review."],"cats":["0"],"entities":[]}
{"id":"mhHFcN_MZJ","text":"The authors conduct empirical studies illustrating the phenomenon of slingshot mechanism as an inductive bias of adaptive optimizers, such as Adam, which promotes generalization. This encourages further work to better understand the dynamics of adaptive optimisers for training machine learning models. ","Comments":[],"cats":["1"],"entities":[]}
{"id":"RjBWM8Q4Cc","text":"The authors present and evaluate five different methods for estimation of a motion vector from a series of 3D OCT volumes. The application is interesting and the proposed network architectures are intuitive and seem appropriate for the problem at hand. The use of 4D convolutions has not been explored extensively, and it is nice to see an application for them.\n\nSome minor remarks:\n\n- Please include the resolution (size in voxels as well as mm) of the input volumes.\n\n- What do the 12 outputs of the network represent exactly? If I understand correctly, the ouputs are 3D motion vectors (3 numbers), times 3 time points $\\Delta s_{t4}, \\Delta s_{t5}, \\Delta s_{t6}$. Would this not make 9 outputs?\n\n- In Figure one, it seems that one of the outputs is $\\Delta s_{t_0}$. Should this not be $\\Delta s_{t_n}$?\n\n- Data has been generated using smooth curved trajectories. It would be interesting to know how these were generated, and whether this resembles real data.\n\n- It took me a while to relate the 83Hz in the text to the 12ms in the Table, maybe make this more explicit.","Comments":[],"cats":["0"],"entities":[]}
{"id":"lhkV2ZAMP-","text":"The key idea in the paper is to use functional prior that is completely uncertain about prediction of any class. To achieve this , the idea of introducing Dirichlet distribution after neural network is used from Evidential Deep Learning (EDL) paper. \nFrom table 1, it is clear that ECE is much lower for the proposed method. However, I have following concerns:\n1. It is not clear why calibration is reported and not simple measures of uncertainty like variance or entropy? Also, I would be convinced that the variance would increase for out of distribution test samples because you used a prior that enforced uncertainty of all labels. Now, it is difficult to connect  use of prior and improvement in ECE.\n\n2. What is the experimental setup? Did you train on some other dataset and test on skin lesion dataset?\n\n3. Last line of section 1: \"it can distinguish distributional versus data uncertainties\". How?\n\nOverall, the idea is fine.\n","Comments":[],"cats":["1"],"entities":[]}
{"id":"HJg4ZAZ0tH","text":"The paper considers the problem of out-of-distribution (OOD) sample detection while solving a classification task. The authors tackle the problem of OOD detection with exploiting uncertainty while passing a test sample through the neural network. They treat outputs of (some) layers in a NN as random Gaussian-distributed variables and measure uncertainty as variance of these Gaussians. Then when uncertainty is high, OOD is detected.\n\nThe overall idea behind the paper could be interesting, but its realisation in the current form is questionable. \n\nThe paper seems totally misusing the reparametrisation trick and stochastic outputs of layers in NNs. Eq. (2) is not the objective of variational inference that seems to be required for stochastic outputs and the reparametrisation trick as presented before the equation. The objective misses the KL-divergence term! Without it what would stop a neural net to set sigmas to 0 and forget about the stochasticity altogether? Not to mention that the current objective is not mathematically justified.\nIf there is no mix and error in eq. (2) and the networks were trained using this loss (and based on provided code they were using this loss), my wild guess of explaining why this may give best results in the experiments is that the models were trained for surprisingly small number of epochs. Therefore, a hypothesis would be that this small number of epochs did not allow the networks to switch sigmas to 0. \n\nUntil the authors can clarify and justify the objective, I will vote for rejection only based on this ground.\n\nHowever, there are other issues in the paper as well. First of all, its clarity. It seems that the paper requires a lot of polishing.  The first paragraph of this review is based on my assumptions from the paper since I am not completely sure I understand it correctly. More about the clarity issues below\n\nFor strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Also related work section does not look like an exhaustive overview.\n\nSome of the detailed comments:\n1.\t\"In other words, in-distribution samples possess more features that convolutional filters react to than OOD samples\" \" first of all, this sentence is not easy to parse. Secondly, it is unclear, why this should be true. If OOD samples are still natural images, they would contain edges just the same as in-distribution samples. That is the power of deep learning enabling transfer learning, that low-level features are the transferable across different data and tasks. Therefore, the claim that \"Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples\" requires more elaboration and arguments\n2.\tThe arguments of the next paragraph regarding uncertainty of deeper layers should be larger for OOD samples are not very convincing either.  It is either requires a definition what the authors mean here as uncertainty, or it is not necessarily true that absence of fixed regions for embeddings leads to higher uncertainty. \n3.\t3rd and 4th paragraphs in Introduction have too many repetitions of phrases between each other. Compare, e.g. the first sentences of the paragraphs or the last sentences. \n4.\t\"One cause of the abovementioned problem is that their approaches\" and similarly the next paragraph: \"their approaches\" stylistically sound wrong. It is appropriate in the previous paragraph since there is a link to \"previous studies\". It seems that \"these approaches\" or \"the existing approaches\" would be a better choice for this and the next paragraph.\n5.\t\"Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick\" \" conventional discriminative models do not estimate mean and variances of the features. The issue of estimating uncertainty is addressed by several special methods such as Bayesian variational methods used in the referred papers. Therefore, in order to use a reparametrisation trick one need to firstly choose a special class of models, which is not obvious from the text.  \n6.\t\"Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.\" \" the size of the validation dataset? In any case neither size of the validation dataset nor the validation dataset itself are not hyperparameters (should not be hyperparameters for out-of-distribution detection). The number of classes can hardly be called a hyperparameter also. \n7.\t\"depends on the difference in the Dirichlet distribution of the categorical parameter <\"¦> In our work, the distribution of the logit of the categorical parameters\" \" what is\/are this\/these categorical parameter(s)?\n8.\t\"Further, they estimate the parameter of the Dirichlet distribution using a DNN and train the model with in-distribution and OOD datasets\" \" this sentence may mislead to impression that the proposed method does not need OOD dataset for training, which does not seem to be the case, since \\lambda and \\theta are trained based on OOD samples\n9.\t\"because they will not be relevant to the classification accuracy\" \" who are they?\n10.\t\"and \\epsilon is the Gaussian noise\" \"> the standard Gaussian noise\n11.\t\"where z^0 = x\" \" it seems this should be placed somewhere earlier when z^l is introduced since z^0 is not used in eq.(2) after which this text is placed\n12.\tIt is unclear how \\lambda^l and CNN \\theta are learnt\n13.\tIt is unclear how the values of features d(x) are used to detect OOD samples\n14.\t\"comparison methods, and models\" \" not clear what models mean here\n15.\tMissing references to datasets in the main text. At least reference to Appendix A.2 is required\n16.\t\"We used 5,000 validation images split from each training dataset and chose the parameter that can obtain\" \" which parameter? \n17.\t\"All the hyperparameters of ODIN\" \" a reader does not know yet that ODIN is used for comparison\n18.\t\"which consists of 100 OOD images from the test dataset and 1,000 images from the in-distribution validation set\" \" it is a bit confusing to call OOD dataset as a test dataset in this context\n19.\t\"We tuned the parameters of the CNN in Equation 4 using 50 validation training images taken from the 100 validation images. The best parameters were chosen by validating the performance using the rest of 50 validation images.\" \" this is confusing. What parameters do the authors talk about in the second sentence if not the parameters of the CNN?\n20.\t\"We used TNR at 95% TPR, AUROC, AUPR, and accuracy (ACC),\" \" Some elaboration is required, at least the reference to Appendix A.1. What is the changing threshold for AUROC and AUPR? Why AUPR-In and AUPR-Out are considered and only a single AUROC is considered. What is the positive class for AUROC?\n23.\t\"For LeNet5, we increased the number of channels of the original LeNet5 to improve accuracy\" \" do the authors mean that they allowed RGB images as input rather than greyscale? If yes, this explicit explanation would be preferable \n24.\t\"We inserted the reparameterization trick\" \" not the best word choice. Reparametrisation trick is a computational\/implementation trick\/method and it is hard to say that it can be inserted into a network. I believe what the authors mean is that they inserted mean\/std outputs instead of point outputs. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic. \nAlso, it is unclear when the authors say they insert it to the softmax layer. According to Section 3 the softmax layer is never considered to output means and stds.\n25.\tThe numbers of epochs for training NNs are very small for LeNet and WideResNet in the experiments. Did the models manage to converge during this short training?\n\nMinor:\n1.\t\"These data were also used\" -> \"this data\"","Comments":["Plenty here that could be *improved*... The decision to highlight sentences here seems completely dependent on \"toxicity\" threshold choice, though."],"cats":["2"],"entities":[[552,1048,"non-productive"],[1049,1457,"ify"],[1625,1704,"non-productive"],[1706,1840,"ify"],[1841,1876,"ify"],[2158,2378,"non-productive"],[4556,4621,"ify"],[5192,5276,"non-productive"],[6260,6608,"non-productive"]]}
{"id":"HygQp_7b9r","text":"# Summary\n\nThis paper trains a network to mimic simple known algorithms in a way that guarantees that they generalize to\nout-of-distribution test instances. The network mimics the algorithms by running repeatedly in a loop where each\niteration of the loop runs a Transformer and outputs a mask that tells the next iteration the inputs to process. The\nsetup is tested on sorting, adding, and graph algorithms, and found to learn regular number representations that\nsupposedly aid generalization.\n\n# Review\n\nThis paper has an admirable and useful goal, but the way it is currently implemented and presented is not ready for\npublication at ICLR.\n\nMy main issue is with the training\/testing setup and its presentation. The authors assume a certain structure of an\nalgorithm (for instance, the iterative structure of recursive selection sort), delegate one or more modules inside this\nstructure to be implemented by neural networks, and train them only.\nMost of the \"strong generalization\" is coming from the fact that the iterative structure is fixed. The work abstracts\nout the most complex parts of each algorithm. In Figure 3, for instance, the NN must learn to find the smallest element\namong the non-masked-out ones on the input, return it, and mask it out. This is a much simpler task than the whole\nsorting algorithm. Training the network to solve \"find_min\" != claiming that the network solves and strongly generalizes\non \"sort\".\n\nImportant training details are left unspecified. How is the data for training NEEs generated? For instance, for training\nthe network in Figure 3, do you trace the whole selection sort on a randomly generated list, and collect the\nintermediate input\/output pairs for \"find_min\"? If so, it's absolutely unsurprising that the process also works for\nlonger lists -- see above. Are composable NEEs, like the three networks in Figure 7, trained jointly or separately? Do\nthey observe their own outputs that are fed into subsequent NEE networks, or are the previous outputs teacher-forced,\nor are they pre-trained? Many of these details need to be clarified precisely to make the experimental setup\nverifiable.\nSome important details are presented factually without any motivation. For example, why does Figure 5 use\nSHIFT and XOR? Why, in general, the next mask produced by a NEE is XORed with a previous one instead of replacing it?\n\nI liked the embedding visualizations, which clearly demonstrate structure in the latent space driven by (a) the\nbinary number representations, and (b) the addition task objective. In addition to regular ordering structure (needed to\nimplement addition), the latent space also clearly exhibits regularities inherent to the binary representation, such as\nthe shift by 64 in Figure 8a. While interesting, this only confirms the findings of Shi et al., albeit in a more pure\nexperimental setting.\n\nIn summary, the scope of experiments and presentation of results would need to be significantly improved in order for\nthis work to reach the quality bar of ICLR.","Comments":[],"cats":["1"],"entities":[[2139,2209,"ify"],[2858,3019,"ify"]]}
{"id":"chccuZzEknh","text":"This paper proposes a method to train student policies when a teacher policy with extra information is available. The proposed method combines two objectives, the usual reward and minimizing the cross-entropy between student and teacher polices, i.e. push the student to imitate the teacher. The novelty of the method is to adaptively set the weight applied to the imitation objective, this is done by viewing the problem as a constrained optimization where the policy must be at least as good as some other target policy. The target policy is set to be another parameterized policy, trained without extra information nor teacher guidance to maximize reward, and the weight is adapted via the Lagrangian dual.\n\nThis seems like a nice straightforward workshop contribution. The writing is good and I was able to understand the paper fairly well, and this seems like a good use of constrained optimization. \n\nI do wonder if there is a simpler interpretation that exists that does not rely on access to a reference policy $\\pi_R$; in some deeper sense, what this lagrangian adjustment captures is a desire to unconstrain the policy improvement step when the policy improvement direction has a \"negative angle\" to the policy imitation direction. I recall some papers (e.g. [1]) constraining gradient steps in a multitask setting by projecting the gradients onto the right subspace so as to avoid interference. Perhaps something similar could be interesting to the authors. I also agree with the authors that per-state weights would be a very interesting contribution.\n\n[1] Gradient Surgery for Multi-Task Learning, Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, 2020","Comments":[],"cats":["0"],"entities":[]}
{"id":"1m_YuhCMZ5Q","text":"1. Originality: although the idea is not presented before, the k-reconstruction idea that views graph as a set of k-subgraphs has some connection to k-WL or k-GNN. As we know that each node in k-WL is a k-size super node which corresponds to a k-subgraph. And the k-WL algorithm over these k-size super nodes are permutation invariant which can be looked as implementation of the f_w function used in the paper. I wish the author provide more formal analysis to connect these two type of models, figuring out their difference and similarity. Nevertheless, from my end the presented k-reconstruction GNN is too complicated and is more like a toy for theoretical analysis instead of real-world useable model. \n\n2. Quality: the author provides a great literature review for expressiveness of GNN. The author also introduced the graph reconstruction theory in section 2. For the definition of reconstructible function, I wish the author can provide more insight of why introducing these defnitions in section 2. A trivial example of reconstructible function is a function that always output a constant representation for all graphs. In section 4 the author use these reconstructible function to characterize the expressiveness of k-reconstruction neural network, but before this I don't see any value of introducing these definitions. I suggest the author rewrite the section 2 part with giving some insight of why introducing these definitions from graph reconstruction theory. For section 3, the presented model looks like theoretical toy model to me. This is very similar to the relational pooling paper but I don't get the value of this model . The contribution of relational pooling is more like the universal approximation ability on graphs but this has been studied extensively. For section 4, the author provided several theorems to characterize the expressiveness of the designed k-reconstruction networks, however these theoretical results are not \"tight\" at all, most of these theorems just tell us that the k-reconstruction is not worse than GNNs. No amazing theoretical result shows that the k-reconstruction network achieves the best expressiveness (higher than k-WL\/k-FWL). Also, the improvement on real world datasets are not much noticeable although it does show some ability in synthetic dataset. \n\n3. Clarity: In general the written is ok, but need some improvement to provide more insight in the flow to help reader understand the paper much easier. \n\n4. Significance: my impression is that the paper is more for theoretical analysis instead of real-world usage. I would like to see some more practical model that can make use of the author's analysis. The author did lots of approximations and it's really hard to know how much expressiveness left after these approximations. ","Comments":["An example of a review that could benefit from quite a bit of rewording in a more neutral sense."],"cats":["1"],"entities":[[1550,1644,"ify"],[2056,2184,"ify"]]}
{"id":"czzjj7Pbyw","text":"Authors proposed a hierarchical fusion framework to integrate random classifiers. It is an interesting work and could have clinical impacts. Experiments show a large improvements from each individual classifier. \n\nMajor concerns:\n\n - Methods section is too short for readers to really understand HFRPC. It is necessary to summarize the two-level using equations. How to choose weights for each base classifier ? Also, Eq 1 is not clear due to the format issue.\n\n - Experiment section lacks lot of details. what value you are going to predict in survival prediction ? Did you predict if the patient can survival more than 2 years ? Also, how many radiomics features are extracted and did you use feature selection ?\nIn survival prediction, it usually reports C-index or AUC if the target survival year is specified.\n\n - Technical novelty is limited.","Comments":[],"cats":["1"],"entities":[[465,714,"ify"]]}
{"id":"r8QlT7O9wWc","text":"Authors argue that combining iterative inference procedures (e.g. EM) with neural networks is challenging due to differentition through the inference process. They propose to make amoritzed iterative inference differentiable by implicit function theorem. This implicit differentiation decouples forward and backward pass and improves stability and tractability. They experiment with slot attention module in SLATE.\n\n### General comments:\nTypo in paragraph 2 of the introduction: \"e.g., n computes Î¸t+1 n â f (Î¸t n, xn )\", should it be f_w?\n\nTable 1 would've been more readable if it had 5 (1+2x2) columns, Data | FID_ours | FID_slate | MSE_ours | MSE_slate .\n\nI think the title 4.1 is wrong. It states \"does implicit differentiation stabilize the training of slot attention?\", but then talks about performance improvements. On the other hand, section 4.2 talks about training stabilization, namely it's first sentence is: \"To further understand the benefits of implicit differentiation, we then ask whether it stabilizes the training of slot attention without the need for optimization tricks like learning rate decay, gradient clipping, and learning warmup.\"\n\nFigure 4 is unclear what the shaded lines mean.\n\n### Pros:\nShows that implicit differentiation can stabilize the training and reduce the need for optimization \"tricks\". \nThe paper definitely fits the workshop theme, is technically correct, novel and clearly written. \n\n### Cons and room for improvement:\nI'm a bit skeptical that whether all the optimization tricks could be removed, as the authors only ablated by by removing individual tricks (figure 4). It would also be nice to see experiments with the original SlotAttention module, to see whether this improvement only applies to the SLATE model or is generally applicable for iterative inference in object-centric methods.\n\n### Final thoughts\nOverall I think this is a valid workshop paper, as it shows that a simple (in terms of lines of code) change can improve training of the Slot Attention module. Therefore I recommend acceptance.\n","Comments":["Nothing screams \"toxic\" (or non-productive, discouraging, etc.) but it also very distinctly feels like it lacks the encouragement of the best reviews. Hence my '1'. Just adding a few suggestions or recognitions would make this so much better. Almost makes me want to highlight areas on that basis only."],"cats":["1"],"entities":[[664,1163,"ify"],[1165,1212,"ify"]]}
{"id":"B1xnxSh4h4","text":"The paper presents an approach for incrementally learning a probabilistic domain model in PPDDL. The motivation is that specifying complete domain models is challenging in one shot and that this problem is compounded in non-stationary environments. The paper introduces a novel framework, which is supported by a solid evaluation. Whereas the main issue with the current approach is learning overly conservative preconditions, it seems that this should be addressable within the current framework.\n\nThe approach to tackling the non-stationary environments is to only use the current example for generating training data for learning. Previous learning is communicated through a previous domain model, a reliability measure (to indicate the completeness of the previous model) and a list of state action pairs that led to failed action execution. This framework leads to a clean separation between past examples and derived knowledge and the current learning example. However, it does seem quite a specific scenario and I didn't find a convincing argument for why no previous data is retained. E.g., what happens if the framework is exposed to a small problem?\n\nThe factors involved in the reliability score seem reasonable, but little discussion is provided. This section as a whole would be improved with more motivation and better building of intuition. There are lots of parameters here and how any of these are set or how they should be set in a new environment is not clearly discussed. \n\nThe results indicate that the approach provides an effective learning framework. The analysis of the contribution of different factors is particularly informative. Some suggestion of how important using Gourmand for exploitation would be interesting, particularly in relation to Table 3. But overall, the current analysis of stationary environments is thorough and provides a solid base. It will be interesting to see how it acts in non-stationary environments and how the collection of parameters impacts on this.","Comments":["Clear, specific, constructive, and encouraging enough. Has (but could benefit from more) actionable suggestions."],"cats":["0"],"entities":[]}
{"id":"H1lHi3l_FE","text":"Summary: This paper introduces three loss terms (L_cls, L_KL, L_sim), and shows performances each loss term. This work achieves reasonable performances without explicit class labels. I accept this paper.\n\nNotes:\n- The paper introduces the semantic similarity loss which is to learn similarity in the embedding space given similarity between targets. In the ablation study, the loss function plays an important role in improving results. \n- Also, they introduce the KL divergence term between the embedding distribution and binary target distribution. The paper also shows that the performances without the KL loss. With the KL loss, the proposed model shows substantial improvement.\n- The paper did experiments on different hash code lengths and showed better performances on the longer hash code\n\nI strongly accept this paper. This paper introduces novel loss functions and demonstrates that their method shows improvement in fully supervised and weakly supervised settings.","Comments":["Not much improvement recommended at all, but not toxic. Highly neutral tone overall, I feel."],"cats":["1"],"entities":[]}
{"id":"2zTspBhxFau","text":"The paper is clearly written and well structured.\n\nI believe the question that the authors actually address, whether auxiliary tasks should be used separately or in conjunction with the main task, is important, and their results should be of interest to the community.\n\nHowever, I think the general framing of their paper in abstract \/ introduction is misleading. At no point do they train a model from scratch (i.e. without pre-training) with their proposed methods. They do justify this with the high cost of pre-training and the convenient availability of pre-trained models, which ironically would be my main criticisms of actually foregoing generic pre-training. So although they raise the question whether pre-training is necessary, they then don't actually compare against a model that is not pre-trained. Rather, they show that after pre-training it might not be necessary to further pre-train on large amounts of data just for domain adaptation.\n\nI think the paper would be much stronger if they did not defer their main question (\"Should we be pre-training?\") to future work, but rather tested their method with the typical MLM auxiliary task on a newly initialized Transformer model.\n","Comments":["A strong critique, but ultimately seems to genuinely engage with the work and provide actionable suggestions."],"cats":["0"],"entities":[]}
{"id":"B1eXpM0L6m","text":"This paper suggests a source of slowness when training a two-layer neural networks: improperly trained output layer (classifier) may hamper learning of the hidden layer (feature). The authors call this \"inverse\" internal covariate shift (as opposed to the usual one where the feature distribution shifts and trips the classifier). They identify \"hard\" samples, those with large loss, as being the impediment. They then propose a curriculum, where such hard samples are identified at early epochs, their loss attenuated and replaced with a requirement that their features be close to neighboring (in feature space) samples that are similarly classified, but with a more comfortable margin (thus \"easy\".) The authors claim that this allows those samples to contribute through their features at first, without slowing the training down, then in later epochs fully contribute. Some experiments are offered as evidence that this indeed helps speedup.\n\nThe paper is extremely unclear and was hard to read. The narrative is too casual, a lot of handwaving is made. The notation is very informal and inconsistent. I had to second guess multiple times until deciphering what could have possibly been said. Based on this only, I do not deem this work ready for sharing. Furthermore, there are some general issues with the concepts. Here are some specific remarks.\n\n-\tThe intuition of the inverse internal covariate shift is perhaps the main merit of the paper, but I'm not sure if this was not mostly appreciated already.\n\n-\tThe paper offers some experimental poking and probing to find the source of the issue. But that part of the paper (section 3) is disconnected from what follows, mainly because hardness there is not a single point's notion, but rather that of regions of space with a heterogeneous presence of classes. This is quite intuitive in fact. Later, in section 4, hard simply means high loss. This isn't quite the same, since the former notion means rather being near the decision boundary, which is not captured by just having high loss. (Also, the loss is not specified.)\n\n-\tSome issues with Section 3: the notions of \"task\" needs a more formal definition, and then subtasks, and union of tasks, priors on tasks, etc. it's all too vague. The term \"non-computable\" has very specific meaning, best to avoid. Figure 2 is very badly explained (I believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but I had to figure it out on my own). The whole paragraph preceding Figure 3 is hard to follow. I sort of can make up what is going, especially with the hindsight of Section 4, since it's basically a variant of the proposed schedule (easy to hard making sure all clusters, as proxy to classes, are represented) without the feature loss, but it needs a rewriting.\n\n-\tIt is important to emphasize that the notion of \"easy\" and \"hard\" can change along the training, because they are relative to what the weights are at the hidden layer. Features of some samples may be not very separable at some stage, but they may become very separable later. The suggested algorithm does this reevaluation, but this is not made clear early on.\n\n-\tIn Section 4, the sentence where S_t(x) is mentioned is unclear. I assume \"surpass\" means achieving a better loss. Also later M_t (a margin) is used, when I think what is meant is S_t (a set). The whole notation (e.g. \"topk\", indexing that is not subscripted, non-math mode math) is bad.\n\n-\tIf L_t is indeed a loss (and not a \"performance\" like it's sometimes referred to, as in minus loss), then I assume larger losses means that the weight on the feature loss in equation (3) should be larger. So I think a minus sign is missing in the exponent of equation (2), and also in the algorithm.\n\n-\tI'm not sure if the experiments actually show a speedup, in the sense of what the authors started out motivating. A speedup, for me, would look like the training progress curves are basically compressed: everything happens sooner, in terms of epochs. Instead, what we have is basically the same shape curve but with a slight boost in performance (Figure 4.) It's totally disingenuous to say \"this is a great boost in speed\" (end of Section 5.2) by saying it took 30 epochs for the non-curriculum version to get to its performance, when within 4 epochs (just like the curriculum version) it was at its final performance basically.\n\n-\tSo the real conclusion here is that this curriculum may not have sped up the training in the way we expect it at all. However, the gradual introduction of badly classified samples in later epochs, while essentially replacing their features with similarly classified samples for earlier epochs, has somehow regularized the training. The authors do not discuss this at all, and I think draw the wrong conclusion from the results.\n","Comments":[],"cats":["2"],"entities":[[950,1002,"non-productive"],[1003,1199,"non-productive"],[1200,1262,"ify"],[2086,2249,"ify"],[2319,2558,"non-productive"],[3447,3542,"non-productive"],[4208,4480,"non-productive"],[4602,4911,"non-productive"]]}
{"id":"NQzFKWwARw","text":"quality: This is a well-written paper which tackles and interesting clinical problem, has a well-described framework and experiments and does a good evaluation.\n\nclarity: Of course more details would be nice, but considering the brevity of the submission framework for short papers, the paper is very clear.\n\noriginality: I am not aware of the background clinical literature in this area, but it seems a novel application.\n\nsignificance: The significance of the clinical solution is high and the presented algorithm seems to perform well enough to actually be a possible solution in the future.\n\npros:\n- the paper has a very nice twist of making the network robust, but excluding certain image modalities during training\n- the multi-task learning approach to learn the labels all at the same time is also very appropriate for this problem\ncons:\n- all abbreviations (OS, T1ce) should be introduced, not everyone has the same background in the clinic or in MRI to understand this\n- why are there so different balances in training, validation and testing data? why were they not all divided in the same way in terms of cases?\n\nCAVEAT: The authors state themselves in the abstract: \"This short paper only contains a brief summary and selection of results from a manuscript that will shortly be submitted to Neuro-Oncology.\" Is this allowed according to MIDL guidelines?","Comments":["low 1\/high 0; seemed so positive but clearly requires improvement in the wording of the 'cons' section."],"cats":["1"],"entities":[[847,977,"non-productive"],[980,1122,"non-productive"]]}
{"id":"oeU5_5nU8tj","text":"The authors propose another DARTS-based method for neural architecture search in AutoML with constrained model size. The general presentation of the work is comprehensive, but in many places unclear. \nThe reviewer recommends to check the work \"RC-DARTS: Resource Constrained Differentiable Architecture search\" by Jin et al. (2019), which also considers the number of parameters as a constraint. The algorithms in both works seem to be quite different, which is why I propose to accept this work.","Comments":[],"cats":["1"],"entities":[]}
{"id":"aFs-QMp8L0","text":"**Strengths**\n* This paper is overall clearly written and easy to follow. The method is easy to understand and relatively straightforward. There are enough details to fully reproduce the training. \n* To my knowledge, evolving the optimal loss function for auxiliary RL tasks seems to be a novel approach. The central thesis of better representation learning in RL is a problem of good practical value.  \n\n**Weaknesses**\n\nMy major concern is the weak experimental results. To elaborate: \n\nFor pixel-based Deepmind Control Suite, the strongest baseline that the authors compare to is CURL (Laskin et al., 2020). However, this is quite an outdated baseline. The authors *ignore at least 2 recent strong baselines*:\n\n* Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. Yarats et al. ICLR 2021 Spotlight. https:\/\/openreview.net\/forum?id=GY6-6sTvGaf. The algorithm is known as \"DrQ\"\n* Reinforcement Learning with Augmented Data. Laskin et al. NeurIPS 2020. Also known as \"RAD\". \n\nBoth papers (neither cited in the paper) are published on top ML conferences before June 2021, so it is fair to request comparison with these prior SoTAs per the ICLR review guideline. Both papers above are about a very simple idea - vanilla reinforcement learning with simple data augmentation can be an exceptionally strong baseline. In fact, if we compare table 1 of the \"DrQ\" paper (Yarats et al.) with table 3 of this paper, we will see that DrQ *beats AARL-Pixel in 7 out of 12 tasks. For the other 5 tasks, none of the gains of AARL-Pixel is statistically significant.*\n\nThis indicates that even **simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL**, which greatly undermines this paper's contribution. \n\nFurthermore, in section 3.1, Fig. 4, the author does mention \"SAC with data augmentation\", which appears to use the same scheme as \"Reinforcement Learning with Augmented Data. Laskin et al.\" (RAD). However, SAC + augmentation (blue dashed line) consistently underperforms CURL (blue dotted line) in Fig. 4, which contradicts the results in RAD. In addition, the numerical results indicated by the lines also disagree with Table 1 in the RAD paper. I believe there are factual errors in these plots. \n\nFor state-based DMControl experiments, the paper claims in section 3.2 that \"there is no data augmentation in the state- based setting.\" This is not true. Section 5.4 in the RAD paper discusses simple ways to augment low-dimensional states, and shows that they are highly effective in boosting performance. \n\nMinor comment: table 1 has a misnomer. \"Inverse dynamics\" typically means action inference, instead of predicting the previous state. ","Comments":["Could absolutely be interpreted as overly critical, even if it is specific. It's not particularly encouraging."],"cats":["1"],"entities":[[610,654,"ify"],[655,710,"non-productive"],[1592,1786,"ify"]]}
{"id":"uw3oRgahlL","text":"Summary:\nThe paper suggests an online learning algorithm to find the right amount of resources for serverless containers to reduce idle resources while meeting the user SLOs.\n\nPros: \n- Serverless functions are aimed in reducing developer effort in managing the resources and delegating that to the platform. The motivation of the paper is aligned with this design goal. The paper suggests delegating the right-sizing task to the platform so the developer does not need to explore this design space.\n- Serverless platforms are beneficial in the unpredictable environment, i.e., when the qps is not constant or pre-determined. A dynamic approach for right-sizing is aligned with this benefit of serverless functions\n- The resource right-sizing can bring dollar value to the vendors by increasing resource efficiency. \n- A white-box approach as presented in the paper can be insightful for the algorithm and can potentially results in better resource efficiency, compared to a block-box algorithm.\n\n\nCons:\n- On the other hand, a white-box approach might not be well received by customers. It could also allow developer to manipulate the learner algorithm.\n- There are currently some state-of-the-art serverless offerings from major vendors, e.g., Azure functions, that do not require user to specify the memory\/cpu for the function. This is against the assumption\/motivation of the paper.\n- The on-line right sizing will impact the debug-ability of the serverless function. That is, if the SLO is not met, how the developer is supposed to debug their service to tackle the issue?\n- A complicated algorithm such as on-line learner would bring computation and memory overhead to the serverless platform, hence negating the purpose. This overhead needs to be studied.","Comments":["Just feels blunt. Not particularly encouraging. If it were, easy '0'. Also, seems to be more of an evaluation of the practical value of the paper, rather than an evaluation of the paper itself."],"cats":["1"],"entities":[]}
{"id":"BfGxLM6BPW5","text":"This work presents a novel dataset, called LOGICINFERENCE,  which was developed for evaluating the compositional generalisability of AI models, specifically for evaluating the performance of models to handle tasks of logical inference. Apart from this another goal of the dataset is to evaluate how well a model trained on this dataset can transfer learned knowledge to other tasks (although this is only mentioned as a step in future work). \nThe dataset consists of several interesting logic subtasks presented in semi-formal logical notation as well as natural language and contains fairly reduced number of samples, making it an interesting challenge, particularly for more data hungry deep learning models. The authors provide a small baseline evaluation, focusing on different versions of the T5.1.1 model, indicating the challenge of the dataset.\n\nIn summary the work is well written and the dataset seems interesting for initiating research in more structured models for logic inference. I believe it offers interesting points of discussion for the workshop.\n","Comments":[],"cats":["0"],"entities":[]}
{"id":"rJxIe2_Z5E","text":"This paper introduces a new GAN model (LGGAN) for labeled graph generation. The output of LGGAN generator contains two parts: labels of each node (represented by one-hot vectors) and the adjacency matrix. The discriminator is a graph convolutional network that outputs graph-level scalar probability of the input being real data. Specifically, the author uses JK-Net to parameterize the discriminator. Empirical results show that LGGAN outperforms state-of-the-art baselines such as DeepGMG and GraphRNN in terms of the MMD evaluation metrics.\n\nThe proposed LGGAN model is very similar to MolGAN. In both model, the generator outputs the adjacency matrix and node label, and the discriminator is parameterized as GCN. The major difference is in the architectural choices of discriminator (JK-Net v.s. Relational GCN). Unfortunately the paper does not compare with MolGAN. It should be very easy to adapt MolGAN model for the datasets used in this paper. The reviewer is also concerned why LGGAN is not compared on molecule tasks. The \"specialized evaluation method\" has been established by previous work (e.g., MolGAN) and it's not hard to run at all.\n\nMoreover, I have several important questions that needs to be clarified:\n1) Since node labels and adjacency matrix are discrete values, how did the LGGAN propagates the gradient from the discriminator to the generator?\n2) The author mentioned in Section 3.1 that the node attributes are not included in the discriminator. If so, the discriminator only focus on the adjacency matrix and the node labels would be mostly random. How would LGGAN learns the distribution of labeled graphs like Protein?\n","Comments":["Just not particularly encouraging."],"cats":["1"],"entities":[[1030,1151,"ify"]]}
{"id":"Hy7uO8PlG","text":"*Quality*\n\nThe paper is easy to parse, with clear diagrams and derivations at the start. The problem context is clearly stated, as is the proposed model.\n\nThe improvements in terms of average log-likelihood are clear. The model does improve over state-of-the-art in some cases, but not all.\n\nBased on the presented findings, it is difficult to determine the quality of the learned models overall, since they are only evaluated in terms of average log likelihood. It is also difficult to determine whether the improvements are due to the model change, or some difference in how the models themselves were trained (particularly in the case of Z-Forcing, a closely related technique). I would like to see more exploration of this point, as the section titled \"ablation studies\" is short and does not sufficiently address the issue of what component of the model is contributing to the observed improvements in average log-likelihood.\n\nHence, I have assigned a score of \"4\" for the following reasons: the quality of the generated models is unclear; the paper does not clearly distinguish itself from the closely-related Z-Forcing concept (published at NIPS 2017); and the reasons for the improvements shown in average log-likelihood are not explored sufficiently, that is, the ablation studies don't eliminate key parts of the model that could provide this information.\n\nMore information on this decision is given in the remainder.\n\n*Clarity*\n\nA lack of generated samples in the Experimental Results section makes it difficult to evaluate the performance of the models; log-likelihood alone can be an inadequate measure of performance without some care in how it is calculated and interpreted (refer, e.g., to Theis et al. 2016, \"A Note on the Evaluation of Generative Models\").\n\nThere are some typos and organizational issues. For example, VAEs are reintroduced in the Related Works section, only to provide an explanation for an unrelated optimization challenge with the use of RNNs as encoders and decoders.\n\nI also find the motivations for the proposed model itself a little unclear. It seems unnatural to introduce a side-channel-cum-regularizer between a sequence moving forward in time and the same sequence moving backwards, through a variational distribution. In the introduction, improved regularization for LSTM models is cited as a primary motivation for introducing and learning two approximate distributions for latent variables between the forward and backward paths of a bi-LSTM. Is there a serious need for new regularization in such models? The need for this particular regularization choice is not particularly clear based on this explanation, nor are the improvements state-of-the-art in all cases. This weakens a possible theoretical contribution of the paper.\n\n*Originality*\n\nThe proposed modification appears to amount to a regularizer for bi-LSTMs which bears close similarity to Z-Forcing (cited in the paper). I recommend a more careful comparison between the two methods. Without such a comparison, they are a little hard to distinguish, and the originality of this paper is hard to evaluate. Both appear to employ the same core idea of regularizing an LSTM using a learned variational distributions. The differences *seem* to be in the small details, and these details appear to provide better performance in terms of average log-likelihood on all tasks compared to Z-Forcing--but, crucially, not compared to other models in all cases.","Comments":["Very specific and constructive. Encouraging? Close enough, maybe. I'd say borderline 0\/1."],"cats":["0"],"entities":[]}
{"id":"r1xpLmpJK4","text":"Summary: This paper conducted an excellent quantitative and qualitative review of the state of the reproducibility for ML healthcare applications.  I learned a great deal from reading it!  \n\nNotes: \n  -Reproducibility is especially important in health due to safety concerns.  \n  -Review of 100 ML4H research papers relating to reproducibility\n  -ML4H has more issues with data and code access.  \n  -Proposes recommendations to make research more reproducible.  \n  -In 2018, 12 healthcare tools using ML got FDA clearance.  (cool!)\n  -Quantitative and qualitative review showing ML4H has worse code availability data availability and dataset variety than other ML subfields.  \n  -The choice of evaluation metrics is reasonable but a bit limited.  \n  -Privacy issues make it difficult to release health data publicly.  \n  -ML4H papers are more likely to report mean\/stdv than other fields in ML.  \n  -Only 19% of ML4H studies used multiple datasets.  \n  -The issue of preregistration in ML is interesting.  \n  \nComments \/ questions: \n  -Does failure to reproduce basic research papers in ML4H really lead to problems for production health systems?  Presumably there are many steps of validation beyond the basic research papers.  \n  -I really like the reproducibility taxonomy: technical, statistical, and conceptual reproducibility.  Technical reproducibility refers to getting the exact results, so includes things like implementation.  Statistical reproducibility is equivalence but only up to the statistical properties being the same (so the results follow the same distribution).  Conceptual reproducibility means that the idea works as long as the concept is preserved.  \n  -Figure 1 is quite nice\n","Comments":[],"cats":["0"],"entities":[]}
{"id":"LBBH0MoRM_v","text":"The authors propose a unifying framework for reinforcement learning based on the dual form of seeing RL as a convex problem with linear constraints. They also use this framework to propose a new algorithm for imitation learning from arbitrary data that relaxes the assumption from previous work that requires expert coverage of the problem space. The authors are thorough in their framework and method. While the empirical results of their proposed method lacks analysis, the major contribution of this work is in their theoretical framework.\n\nHowever, the paper is only vaguely related to reusing prior computation for RL, in that the proposed method is for imitation learning with offline datasets. For this reason I rate the submission lower than the quality of the paper would suggest.","Comments":[],"cats":["1"],"entities":[]}
{"id":"klltXDz10r","text":"This paper refines the existing finite-sample bound of double Q-learning. This paper considers the rescaled linear schedule of the learning rate and claims that the sample complexity bounds are improved in the sense the dependence on all main parameters (epsilon, 1-gamma, L, D) have been improved.\n\n\nOriginality: The paper follows the work in Xiong et.al 2020, but has made some non-trivial improvements. The nested SA representation of double Q-learning is interesting and the proving techniques seem new. \n\nQuality: The technical quality of this paper looks reasonably good to me although I have only roughly checked the proofs. The high level idea of the proof makes sense to me. I haven't found any flaws in the proof sketches.\n\nClarity: Overall the paper is well written. \n\nSignificance: I am not sure whether the significance level of this submission meets the standard of ICLR or not. The main reason is that the bounds in this paper are on the expectation of L1 norm of the iteration error. The bounds in the Xiong et.a. (2020) are high probability bounds. Is it fair to make comparisons between these two? The L1 bound is  weaker than the mean square bounds and I am not fully convinced how meaningful such results are. In addition, the implications of the proposed theory on algorithm design have not been fully verified on numerical examples.\n\nPros:\n1. The linear learning rate schedule is considered.\n2. The nested SA representation is interesting and the proof technique looks new.\n\n\nCons:\n1. The bounds on the iteration errors are in the L1 sense and it is unclear how meaningful these bounds are. Is the comparison with Xiong et.al (2020) really fair? \n2. Does the theory in this paper lead to any new insight for design and tuning of double Q-learning? Any numerical justifications for these new insights?\n \n\n","Comments":[],"cats":["0"],"entities":[]}
{"id":"-pKvzWAHsYf","text":"### Pros\n- This work takes a problem-driven approach to improve the performance of a reinforcement learning agent on VizDoom. Although most of the RL literature starts from conceptual ideas to experimental results, I consider the present work to be valuable as it assembles and confronts some well-known techniques in RL to concrete problems.\n- The impact of the action wrapper and the rule-guided policy search is a great example that sometimes less is more. Using simple tricks on top of generic methods can sometimes be much more effective than complicated approaches.\n\n### Cons\n- The quality and clarity of the writing is problematic. Besides the numerous spelling mistakes, the imprecise language often confuses the reader. For instance:\n\t* \"...the state transition is not static\", we generally talk about stationary or non-stationary environments. I'm not sure what static means in this case?\n\t* \"..., while they are enabled to act diversely when the environment dynamics changes.\" The term `diverse` in the context of reinforcement learning is most often associated with Quality-Diversity that aims to generate a large collection of diverse solutions\/policies. It is unclear whether the authors want to highlight the ability of their agent to adapt to changing dynamics (few-shot learning for instance) or if they want their agent to learn a policy performing well against opponents displaying a diverse set of strategies. For this reason, the method name, Diversified Strategic Control, is confusing as well.\n\t* The title mentions \"General Reinforcement Learning\". I'm not sure what the authors try to communicate? Besides the fact that general reinforcement learning feels undefined, the described approach uses tricks specific to FPS, like the statistical map provided as extra features to the agent's network or the rules defined in the 2 RGPS losses.\n- As highlighted by the authors, the proposed approach consist of a combination of existing techniques. Although replicating and using these techniques require significant engineering effort, these can't be considered novel contributions. Other works that took a similar work setting, often made novel and original contributions, e.g. AlphaStar and their league system.\n- The work is certainly complex from an engineering point of view: handcrafted curriculum learning, hindsight experience replay, distributed training strategy. As such, particular attention to providing all the required information to replicate the work has to be initiated. This aspect of the work is deeply lacking.\n\t- The values of all the hyper-parameters are not provided e.g. optimizers, losses coefficients, training parameters, etc.\n\t- Many terms and aspects are not defined. The authors don't explain: how they applied Policy Distillation, how the prioritized self-play has been implemented and put in action, what's the training dynamic (how long each stage lasts), where do the baselines come from (were they re-implemented or available as part of the competition?), etc.\n\tThese missing bits of information would make the replication of the results nearly impossible to achieve and don't help the reader understand how the approach fully works and how it was implemented. I would encourage the authors to make a few steps in this direction as the lack of transparency on their method is puzzling.\n\n### Questions\n- Figure 5 only provides the score for a few goals. Why is that, and could the performance of the agents for the other goals be provided in the appendix?\n- Regarding the training at stage 3, the authors mention that the selection of goals should be infrequent to avoid goal switching. Have the authors tried to constrain the switching of goals by restricting the goals to be adjacent to the current agent's goal, or by introducing a penalty in the reward?\n- How are the end-of-life events processed? When the agent dies, is it considered the end of the episode? What are the implications for the hidden state of the LSTM, does it persist over the 10 minutes of the Deathmatch?\n- What are the motivations behind having chosen PPO? The work requires applying Hindsight experience replay and importance sampling which increases the variance of the gradient. Have the authors tried using an off-policy algorithm that suits more easily HER?\n- I would like additional clarifications on the goal selection. If I understand correctly the third head can decide the goal. If, for instance, the agent detects an enemy in an area that is not its current goal. How can the agent decide to change its strategy given that the third head is only activated once the agent reaches its current goal?\n\n### Minor Comments\n- Hindsight Trust Region Policy Optimization appears twice in the bibliography.\n- Figure 2 is really hard to understand. It would benefit from a more extensive caption describing in greater detail each component and the transitions from one stage to the next.\n","Comments":[],"cats":["1"],"entities":[[584,728,"non-productive"],[854,898,"ify"],[1573,1862,"ify"],[2393,2550,"ify"],[2717,3215,"non-productive"],[3216,3340,"ify"]]}
{"id":"SkeeJ9ku3Q","text":"In this paper, the authors proposed to learn a stacked classifier on top of the outputs of well-known transfer learning models for transfer learning. The authors claimed that their proposed solution can avoid negative transfer.\n\nTechnically, there are no contributions. The proposed solution is a straight-forward A+B, where both A and B are well-known. Specifically, in the proposed solution, different well-known transfer learning models are used as the 1st level classifiers to generate intermediate outputs, then a stacked classifier is trained with the intermediate outputs as its inputs. Stacking techniques are also well-known in ensemble learning. Therefore, I do not see any new technical ideas. \n\nMoreover, the proposed solution cannot really avoid negative transfer. If two domains are indeed very different, the performance of the basic transfer learning models would be very bad, e.g., worse than random guess. In this case building a stacked classifier cannot help to boost the final performance. \n\nThe datasets used to conduct experiments are all of toy sizes.\n\nThere are a lot of typos and grammar errors. The format of citations in the main text are incorrect. \n\nIn summary, the quality of this paper is far below the standard of top conferences.","Comments":["Not remotely encouraging, but certainly specific and professional (even if very blunt). Hard to identify any specific sentences though! Maybe a textbook 1?"],"cats":["1"],"entities":[]}
{"id":"SkgJtsr4YN","text":"This is a nice submission. The authors show how a generalized entropy regularized policy formulation encodes a variety of approaches to seq2seq learning, from maximum likelihood estimation to reinforcement learning implemented using the RAML, SPG, and data noising methods. Besides the theoretical insight, the authors show how by implementing an easy-to-hard paradigm that resembles curriculum learning leads to improvements on two tasks: machine translation and summarization.\n\nThe paper is likely to generate good discussions, especially with respect to other approaches to sequence learning that are not yet encompassed by the proposed framework.\n","Comments":[],"cats":["0"],"entities":[]}
{"id":"cEdTt9ECmjL","text":"The contribution of the paper is the use of Newton's method for solving the subproblem (5). The main part of the paper is focused on the computation of derivatives and results related to them, many of them very basic. I think this part is not relevant, and in any case might be included in the supplementary material. \n\nThe modification of the method seems to be of importance, given the complexity discussed (even though it might not work fast for some vectors), but I think the presentation should be dramatically improved.\n\nThe texts is repetitive, the toy example in Fig.1 is not explained or motivated at all, and there are some technical parts with details or omissions. For instance, in line 26 it should say that the vector to be projected is $y$; problem (1) should be actually argmin; on the line 30, the convexity of $\\Delta_k$ has to be used, otherwise the statement is false, Newton's algorithm doesn't have a stopping criteria, it runs for a fixed number $T$ of iterations, but $T$ is not an input, and is not discussed as it should.\n\nAlso, please proofread and check for typos, missing words, etc, and please be consistent with the notation, for instance in some parts $[]_+$ is used, and in other parts just $\\max(0,\\cdot)$, for the same argument.\n\nI tend to think that the result is important, but unfortunately I think the paper needs more work.\n","Comments":["1 borderline 2"],"cats":["1"],"entities":[[218,318,"ify"],[320,525,"ify"],[527,1047,"non-productive"]]}
{"id":"NoE09WESWRZ","text":"Deep neural networks are known to be brittle, and can lead to dangerous consequences if left unverified. Forward reach set computation can be used as a basic primitive to verify properties of deep neural networks used in a robotic setting. There has been a rising interest in verifying larger neural networks used in safety critical setting. \n In this paper,  the authors propose a way to compute reachable sets for a neural network in a backward sense. Starting from the outputs of the neural network, and  then work it's way to the inputs. This is an interesting way to look at the problem itself,  but as the authors point out it is an intractable problem.\n\nMy concern about this paper is I don't see the use of a pre-image computation algorithm as being very useful. A forward reachability tool works pretty well for the size of neural networks considered in the paper. Pre-image computation does not provide any advantage in terms of scalability, as is apparent from the experiments. Moreover, almost any safety constraint that needs to be verified with system dynamics in the loop always should ideally work forward in time. Thus for the neural network controller from the inputs to the outputs. \n\nCartpole example : The authors come up with rules about, which output behaviors are correct for a few of the input regions. Then use this as a specification for the verification algorithm. But the very specifications, comes from reasoning about the forward behavior of the system dynamics itself. The idea of forward reach sets computation would generalize much better to a wide range of examples therefore.  Without the need to come up with such handcrafted rules. \n\nThe authors do make a convincing case for the ACASXu example. But this example is less interesting given the amount of attention it has received recently. ","Comments":["Harmless, professional, nothing obviously discouraging. Just don't quite feel like I can call it a 0."],"cats":["1"],"entities":[]}
{"id":"HXIS3TQQeDQ","text":"The paper aims to make use of prior knowledge in RL, specifically in the case where it is known that certain sets of states are functionally equivalent and should have the same value. In addition to normal TD learning, it proposes updating states' value based on states that are known to be in the same functionally equivalent set, enabling faster learning. \n\nFirst, I'm not fully convinced about the novelty of this approach compared to the rich literature on works like bisimulation state abstractions, successor representations, etc. Second, my sense is that in practice, especially with realistic state\/action spaces, it will be quite difficult for a human user to specify these invariances and know this set of equivalent states. Also, if the human does know this, I wonder if there are better ways of incorporating the prior knowledge, for instance, shouldn't the human then design a state abstraction that removes this ambiguity? It makes sense that this method works on Solitaire, but I'm not totally convinced there are many other domains where such a method would be practical to use. ","Comments":["Specific, respectful... Encouraging? Constructive with clear and supportive feedback? No. 1."],"cats":["1"],"entities":[]}
{"id":"fOBU1wV6enr","text":"Strengths:\n+ Welcome empirical study of fine-tuning strategies specifically for source code applications.\n+ Good use of statistical analysis helps correct for small dataset.\n\nWeaknesses:\n- Empirical analysis is too narrow, focusing on a single task and dataset, as well as very small models.\n- Technical contributions, such as the fine-tuning of a single decoder layer, appear very ad-hoc.\n- Overall results provide no actionable new insight into the use of fine-tuning for software engineering.\n\nFine-tuning and other domain adaptation methods are rightfully becoming important subjects in machine learning driven software engineering research, so an empirical study of this process is relevant. However, the approach in this work is too narrow to provide much in the way of actionable insights. The analysis is based on a single paper and dataset, on which it considers mostly long-established fine-tuning methods. The results are exactly as expected; complete-model fine-tuning is somewhat slower, but effective; fine-tuning just one or two layers is a bit faster and slightly less accurate (depending on compute), etc. Indeed, much of the motivation is very generic to the motivation for, and problems with, fine-tuning in general. The use of statistics to avoid drawing unwarranted conclusions is a welcome addition and further reinforces that no new results are found.\n\nIn terms of the original motivation, which identifies key factors such as privacy and memory\/compute footprint, the results do not provide a particularly satisfying conclusion. The analysis entirely considers other open-source projects, so it shines little light on fine-tuning behavior within corporate code-bases, which may well have much more different characteristics to the training data than this test set. And the compute results in Figure 4 suggests that simply fine-tuning the full model is already the best solution at around 1 PF-second, which translates to much less than a minute on virtually every commonly used GPU. That seems like a _very_ small cost for adapting to a new project, which somewhat undercuts the need for any other analysis here. On the other hand, arguable more product-relevant issues such as working memory usage and inference time\/cost are not addressed by this work.\n\nThe main technical novelty in this work stems from the addition of fine-tuning a single decoder layer. This is based on an analysis that shows that higher layers are updated (rather) slightly more in terms of parameter change when fine-tuning the full model. This feels very ad-hoc; it falls well short of more grounded analyses of fine-tuning different layers such as in Yosinski et al. (NeurIPS 2014), and its motivation suffers from the clear problem that parameter changes in full-model fine-tuning are not necessarily indicative of which layer \"needs\" to be fine-tuned the most. Updates naturally cascade across layers, so the behavior in Figure 2 seems natural when updating an entire model. The results indeed do not show a substantial difference between this and updating another layer (embedding\/output), except perhaps at very specific compute values, for which no significance test was applied.\n\nDetailed Comments:\n- Introduction: please add references to substantiate the claims in the first two paragraphs\n- P2, top paragraph: \"performances\" -> \"performance\"\n- P2: the first two bullet points in the introduction paint a very similar picture, mostly because of the second half of the first, which also essentially talks about privacy.\n- The use of the word \"Custom\" throughout the paper is rather odd. For one, it is often used interchangably with \"customized\", but those evoke rather different ideas -- \"custom\" suggests a specially designed model for a new task, which does not resonate the contributions here. \"Customized\" should really just read \"fine-tuned\"; I see no reason for another term. The variant called \"Custom fine-tuning\" (2.2) would be better called something like \"Full model fine-tuning\".\n- P3, last paragraph: \"finetuining\" -> \"finetuning\" (twice)\n- Table 2\/3: why use such long IDs, instead of simply numbering the projects for this paper?\n- What motivated fine-tuning both embeddings and the output layer, besides following Lu et al. -- are these weights tied in this model?\n- Figure 2 is rather hard to read, primarily in terms of both axes' labels. Please also explain \"(Other, 1)\"\n- Comparing matching rates on abstracted code mostly makes sense in terms of ignoring identifiers, but not distinguishing between different objects of the same type means this metric does not capturing semantic correctness. Please clarify to what extent this impacts the results; ideally ablate this latter decision to better isolate the effect of just naming conventions.","Comments":["blunt. specific."],"cats":["1"],"entities":[]}
{"id":"v0wwNgiqS","text":"In the introduction the authors state that none of the existing deep learning approaches to lobe segmentation use explicit knowledge from pulmonary fissures. This is not true, the Gerard and Reinhardt 2019 reference uses pulmonary fissures as an input channel. This method is currently the leader in the LOLA11 challenge which should also be mentioned.\n\nIt seems the proposed method requires a lung segmentation as a precursor which distinguishes left and right lungs (for cropping input). This needs to be explicitly mentioned. It should also be explained in the methods how this was obtained in this work.\n\nDuring training patches of size 60 are used, however, it is unclear what is done during inference. Are the same patch sizes used? Are non-overlapping patches used? If not, how are patch results merged?\n\nFigure 2 shows the \"mean distance to visible fissure\". It is unclear how this is calculated. For this to be calculated the evaluation data ground truth would need to include annotations of just visible fissures, however, based on the description it seems only complete lobe segmentations are available, i.e., all extracted fissures would be extrapolated and include both visible and non-visible fissures.","Comments":["Undoubtedly respectful, specific, and constructive. Encouraging? Borderline 0\/1."],"cats":["1"],"entities":[]}
{"id":"BJx-6PRtnQ","text":"This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled.\n\nThe paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting.\nHowever, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? \nBesides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational.  \nNext, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable.\nFinally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. \n\nQuestions:\n- Could you elaborate why DAL strategy does not end up doing just random sampling?\n- Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers?\n- How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case?\n- How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible?\n\nSmall comments:\n- I think in many cases citep command should be used instead of cite. \n- Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network?\n- Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)?\n- It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed.\n- I am not sure \"discriminative\" is a good name for this algorithm. It suggested that is it opposite to \"generative\" (query synthesis?), but then all AL that rank datapoints with some scoring function are \"discriminative\".","Comments":[],"cats":["0"],"entities":[[392,540,"ideal"],[591,648,"ideal"],[1522,1675,"ify"]]}
{"id":"r42eqqUWkzc","text":"Summary: the authors propose a novel learning setup, Cascade, that tasks the agent to intervene on the initial conditions of the scene to achieve a counterfactual outcome, given an observed scene. The agent is provided with an instruction that gives the agent a counterfactual goal to achieve, provides a hint on which object to intervene on, and provides constraints the agent must satisfy. This is challenging problem and the authors provide a method for solving this task.\n\nStrengths:\n- A novel data structure, the Event Tree, that make searching through possible futures efficient\n- A value function for focusing the search\n- Leveraging observed data to inform the search\n\nWeaknesses:\n- the proposed approach requires much domain knowledge of the underlying system in order to build the event tree. Such domain knowledge may not necessarily be available, or expensive to get, in applications.\n\nOverall, however, I see this as a novel and interesting benchmark and very much relevant to the workshop theme.\n","Comments":[],"cats":["0"],"entities":[]}
{"id":"SklcjZbP5r","text":"This paper propose a modified U-net architecture to segment the stagnant zone during silo discharging process. It lacks novelty and the improvement is marginal.  More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception.\n\n[1]Waktola S, Grudzien K, Babout L. Stagnant zone segmentation with U-net[C]\/\/2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE). IEEE, 2019: 277-280.\n","Comments":[],"cats":["1"],"entities":[[111,160,"ify"]]}
{"id":"BO_gDxO-kMq","text":"Summary: the authors apply the Invariant Causal Representation Learning (iCaRL) framework to imitation and reinforcement learning.\n\nStrengths:\n- provides results on the conditions under which it is possible to expect generalization in representation learning, dynamics learning, and policy learning\n\nWeaknesses:\n- the method builds upon the non-linear ICA framework to identify latent variables. However, it is not clear in what sense these latent variables can be considered \"causal\" to the observed variables. As far as I know, from Elements of Causal Inference by Peters, Janzing, and Scholkopf, what makes a DAG \"causal\" is that it is the functions (i.e. mechanisms) that relate the variables are independent, and the paper was not clear how the non-linear ICA framework enforces the independence of mechanisms.\n\nThis paper was quite dense and had most of the results in the appendix. For example, the paper set up the imitation learning problem but did not present its proposed solution to this problem in the main text.\n","Comments":[],"cats":["1"],"entities":[]}
{"id":"BjKUyY7YDbn","text":"Paper strengths:\n+ The reparameterization of hyperparameters to make them derivable seems good. The idea is a good idea, even if already existing in the literature.\n\nPaper weaknesses:\n- This paper is not novel, a lot of existing work already tested this idea and showed that it already works. See the following references:\n\nFranceschi, L., Donini, M., Frasconi, P. & Pontil, M.. (2017). Forward and Reverse Gradient-Based Hyperparameter Optimization. Proceedings of the 34th International Conference on Machine Learning, in PMLR 70:1165-1173\n\nStamoulis, D., Ding, R., Wang, D., Lymberopoulos, D., Priyantha, B., Liu, J., & Marculescu, D. (2019). Single-path nas: Designing hardware-efficient convnets in less than 4 hours. arXiv preprint arXiv:1904.02877.\n\nZhang, C., Ren, M., & Urtasun, R. (2018). Graph hypernetworks for neural architecture search. arXiv preprint arXiv:1810.05749.\n\nTwo of those references involve hypernetworks, with the same idea as proposed in this paper, which is define a larger network containing all subnetworks embedded within the hyperparameter search space.\n\n- The selected benchmarks are outdated.\n- Going back to the first point, the methods selected for comparison with the proposed approach are also outdated.\n- The paper is poorly written, hard to read and the ideas are not clearly stated.\n","Comments":["It feels like it's trying to be helpful, but is just too blunt for me."],"cats":["2"],"entities":[[186,292,"non-productive"],[1090,1127,"non-productive"],[1130,1242,"non-productive"],[1245,1324,"non-productive"]]}
{"id":"SJlBb0_QYB","text":"This paper proposes an estimator to quantify the difference in distributions between real and generated text based on a classifier that discriminates between real vs generated text.  The methodology is however not particularly well motivated and the experiments do not convince me that this proposed measure is superior to other reasonable choices.  Overall, the writing also contains many grammatical errors and confusing at places.\n\nMajor Comments:\n\n- There are tons of other existing measures of distributional discrepancy that could be applied to this same problem.  Some would be classical approaches (eg. Kullback-Leibler or other f-divergence based on estimated densities, Maximum Mean Discrepancy based on a specific text kernel, etc) while others would be highly related to this work through their use of a classifier.  Here's just a few examples: \n\ni) Lopez-Paz & Oquab (2018). \"Revisiting Classifier Two-Sample Tests\n\": https:\/\/arxiv.org\/abs\/1610.06545 \nii) the Wasserstein critic in Wasserstein-GAN\niii) Sugiyama et al (2012). \"Density Ratio Estimation in Machine Learning\"\n\nGiven all these existing methods (I am sure there are many more), it is unclear to me why the estimator proposed in this paper should be better. The authors need to clarify this both intuitively and empirically via comparison experiments (theoretical comparisons would be nice to see as well).\n\n- The authors are proposing a measure of discrepancy, which is essentially useful as a two-sample statistical test.  As such, the authors should demonstrate a power analysis of their test to detect differences between real vs generated text and show this new test is better than tests based on existing discrepancy measures.\n\n- The authors claim training a generator to minimize their proposed divergence is superior to a standard language GAN. However, the method to achieve this is quite convoluted, and straightforward generator training to minimize D_phi does not appear to work (the authors do not say why either).\n\n\nMinor Comments:\n\n- x needs to be defined before equation (1). \n\n- It is mathematically incorrect to talk about probability density functions when dealing with discrete text. Rather these should be referred to as probability mass functions, likelihoods, or distributions (not \"distributional function\" either). \n\n","Comments":["'Toxic' would be the wrong word to describe this review, but it nonetheless could be improved a bit to promote a healthier peer-review environment."],"cats":["2"],"entities":[[350,433,"non-productive"],[829,855,"ify"],[1087,1380,"ify"],[1827,2001,"non-productive"]]}
{"id":"sSa7o2B8zSM","text":"Pros:\n1. Standard training schemes and architectures cannot guarantee accuracy and certified robustness at the same time. The work focuses on addressing this issue and therefore the problem considered here is an important and challenging one.\n\n2. The construction of IBP-MonDEQ is novel and is among the first attempts to create certified implicit networks. While the interval analysis is relatively simple, the authors provide non-trivial theoretical analysis and guarantees.\n\nCons:\n1. Some of the details about theoretical formalism and empirical evaluation was not clear to me (see my comments below).\n\n2. The practical relevance of the certified robustness obtained on the more challenging CIFAR10 dataset presented here is not clear as the results do not advance the state-of-the-art. For example, the best-certified robustness for CIFAR10 for epsilon=2\/255 achieved here is 51% with the corresponding standard accuracy as 64%. This is significantly worse than that obtained by the state-of-the-art which is 60.4% robustness and 78.4% accuracy as reported by COLT (https:\/\/openreview.net\/pdf?id=SJxSDxrKDr). Similarly for epsilon=8\/255, the best-certified robustness and accuracy achieved here are 30% and 44% respectively. These are again worse than the state-of-the-art bounds of 35% robustness and 50% accuracy from L_oo nets (https:\/\/arxiv.org\/pdf\/2102.05363.pdf).\n\nI have a few other questions:\n\n1. Since the interval abstraction is a complete lattice. Therefore, fixed points exist for any monotone (wrt interval inclusion) neural network. Is the class of networks that you identify in Theorem 3.1 and 3.3 a subset of those provided by the classical Knaster-Tarski theorem (https:\/\/en.wikipedia.org\/wiki\/Knaster%E2%80%93Tarski_theorem)?\n\n2. Is it possible to define a class of DEQs that have multiple fixed points? \n\n3. Can one equivalently identify a class of fixed-point obtaining implicit networks for other analysis types? e.g., CROWN, DeepPoly, or Zonotopes?\n\n4. The authors should consider providing an intuitive meaning of symbols (e.g., M, D) used in equations (3.4) and (3.7). The text in the evaluation says that M is a learnable parameter which makes things clearer but it comes too late. An example showing instances of W that satisfy these equations and those that do not would also help in improving the readability of the paper.\n\n5. Is it possible to train IBP-MonDEQs for perturbations that cannot be exactly captured by intervals?\n\n6. Is the certified robustness in Table 2 for explicit networks computed using IBP analysis? If yes, will the numbers improve with a complete verifier or with a more precise analysis like Crown or DeepPoly? This is important as it seems that the IBP-MonDEQ cannot be analyzed with anything else besides the IBP analysis while the explicit networks support other analyses.\n\n7. What are the implications of computing a post fixed-point by replacing the equality constraint in eq. (3.6) with interval inclusion, i.e., compute a fixed-point such that the interval of RHS is included inside that for LHS? ","Comments":["Specific, constructive, acknowledged the positives and was respectful. Not the best review I've ever seen, but I think it best fits into '0'."],"cats":["0"],"entities":[]}
{"id":"R074PlNeOBO","text":"Review Summary\n--------------\n\nRecommendation: oral or poster\n\n__Relevance for workshop: 10\/10__\n\nInvestigates the factors on how pre-trained visual representations can be helpful for downstream embodied AI \/ RL tasks.\nFits very well with the workshop topic.\n\n__Scientific quality: 9\/10__\n\nLarge-scale empirical study that investigates different conditions (source dataset size, diversity, model size).\nProcedures are sound.\n\n__Paper quality: 9\/10__\n\nWell written.\n\n(Points:  lowest: 0\/10 means, highest: 10\/10, >=5 means a recommendation to be included to the workshop)\n\n\nPaper Summary\n-------------\n\nThe paper is a large-scale study about pre-trained visual representations (PVRs) for embodied RL tasks.\nIt investigates the influence of different conditions on the performance of PVRs: the size of source datasets, diversity of source datasets, the scale of trained models, and type of models.\nIt evaluates the performance of different trained PVRs on several target RL tasks.\nKey findings show that larger source datasets, higher diversity, and larger scale of model result generally in a better PVR.\nNonetheless, the paper shows exceptions for specific tasks\/conditions.\nThey also trained and provide the current SOA of PVRs in the embodied AI domain.\n\n\nMajor Points\n------------\n\nI didn't find major points in your current paper and I think it could be published as is.\nThe points here are meant for potential future research or to enrich the paper.\n\n1) You currently analyzing the influence of the source datasets on the resulting downstream performance of the PVRs.\nIt would be interesting to analyze the PVRs themself, such as: how they represent certain features or how the representation change depending on the different source dataset conditions.\nAnd how might different representation types aid the downstream tasks.\nThis could help to understand why you found exceptions where larger, more diverse source datasets do not improve performance over smaller, less diverse datasets.\n\n2) Your goal is to study the influence of different conditions, such as dataset size and dataset diversity on the trained PVRs.\nYou do this by using \"real\" datasets and target tasks where conditions such as diversity are difficult to measure and to control.\nYou might consider constructing an \"artificial\" dataset where you have more control over diversity and how well the data fits the downstream task.\nThen you could directly manipulate these factors and see their effect on the downstream performance.\n\n3) Influence of training time.\nIt would be great to see the influence of the training time on the PVRs.\nThe current paper shows their performance after a set amount of training epochs.\nIt would be interesting to see how well PVRs do that are trained after a different amount of epochs.\nMaybe also, if it is necessary to train them to \"completion\" on their source dataset.\n\n\nMinor Points\n------------\n1) I can not find how many (seeds\/repetitions) you ran per downstream task to compute their mean performance and the std.\n2) Tables 2 and 4: The light grey text (line 1) is a bit too light. Could be darker.\n3) Sometimes you write \"ViT\" and other times \"VIT\", but I guess you mean the same thing?\n4) Chapter 5.1: All your datasets include the \"Ego4D\", thus you could avoid this name in your abbreviations to make them look less complex.\n   You could use \"Man+Nav\" to make the M and N easier to recognize for what they stand for.\n","Comments":["Not powerfully encouraging, but specific and constructive and clearly supportive in some way. Hard to call it a 1, so 0."],"cats":["0"],"entities":[[3110,3195,"ify"]]}
{"id":"K9rks4lk8CC","text":"This work proposes to constrain the regularization term of Conservative Q-Learning by enforcing to lower-bound the learned Q value by the (FA-estimated) empirical Q value of the dataset. Such lower-bounded Q values are said to be _calibrated_. This change is motivated by the observation that pretrained Q-values obtained from CQL get very negative, and that when switching to the online phase the Q-values jump back to values closer to the average return. By avoiding these extremely negative values during training, Cal-QL empirically appears to learn faster during the online finetuning phase.\n\nSome thoughts:\n- The language around bounding is a bit confusing at times. From what I understand, $Q^\\mu$ acts as a **lower-bound** to $Q_\\theta$. We want $Q_\\theta(s, a)$ to be **at least as high as** $Q^\\mu(s,a)$. I'm not sure it's ideal to say that $Q_\\theta$ _upper bounds_ $Q_\\mu$, because $Q_\\theta$ is the quantity of interest. Usually the thing that is _bounded_ is what we are looking for, and it _bounded by_ other (often fixed) quantities. I think it would be easier to understand if the wording was \"we lower-bound $Q_\\theta$ by $Q^\\mu$\". \n- The word \"unlearning\" also may not be ideal, and feels like it's doing a lot of work, especially given that the evidence is about (a) the magnitude of Q-values (b) the success rate. During online training, the model is effectively learning about its environment, learning _not_ to take certain actions; that this happens in parameter space and not just in value & performance space matters. I would love to see a deeper analysis of what happens during this phase of low success rate.\n- In 4.2, I fail to see why this speculation is correct: \"the policy optimizer would not unlearn $\\pi$ in favor of a worse $\\mu$ upon observing new online data since $\\pi$ still attains a larger value under the learned $Q_\\theta$ function\". The only thing that a calibrated Q value tells us is that _predicted_ action-values are lower-bounded. This does _not_ imply that actual returns are. For both CQL and Cal-QL, the OOD $(s, a)$ are OOD, and the only difference is the prior we put on the solutions the function approximation is allowed to find, which may impact generalization or numerical stability.\n\nOverall the paper is a good workshop contribution. It takes a problem, investigates it, proposes an effective solution and benchmarks it. The writing is good. I think some aspects could be stronger, and things dug deeper, but this is otherwise good work by the simple metric that I learned something new.","cats":["0"],"entities":[[1544,1636,"positive"]],"Comments":[]}
{"id":"kf8TrSGeLr1","text":"Summary:\nThe paper has two main contributions: \n1. It proposes a minimalist expansion to the BRAC algorithm that uses a mean-squared error for the action deviation penalty and has the popular TD3+BC algorithm as a special case.\n2. Explores design choices in the practical implementation of the algorithms, such as the use of layer normas and batch sizes.\n\nStrengths:\nThe proposed algorithm is pretty simple and straightforward. The implementation details are useful and seem to make a significant difference in the overall results. \n\n\nWeaknesses:\nThe paper evaluates it‚Äôs results on the standard d4rl benchmark locomotion tasks. With so many papers, approaches, design choices and hyper-parameters fine -tuning (from the paper: ‚ÄúŒ≤1 and Œ≤2 parameters from Equations 3 and 4 are carefully tuned‚Äù)  it is not clear whether the results are indeed significant or a matter of over-fitting to the datasets. It would be helpful to evaluate the more complex domains of the benchmark as well. This is also a completely offline single-task RL approach, which might not be the best fit for the workshop theme. \n\n\nConclusion:\nI would still recommend and accept as the proposed approach is simple and still achieves good results. The design experiments are useful to practical implementations and there is a renewed interest in this recently. \n","cats":["1"],"entities":[],"Comments":[]}
{"id":"BDjPqT77la","text":"Strengths:\n\n- The paper tackles an interesting problem and it also applies when we have no access to the taxonomy.\n- The presentation is relatively clear and self contained.\n- The technical details appear sound.\n\nWeaknesses:\n\n- The novelty\/originality is limited: box embeddings, probabilistic semantics of their intersections, gumbel boxes, bessel volume, etc are all adopted from existing literature. Propositions and the corollary appear more like observations rather than results. \n\n- The overall setup is not well-motivated. While I see why box embeddings can be useful for encoding class interactions and their hierarchies (which are in the form of directed acyclic graphs), it is not very clear to me whether one needs a probabilistic semantics as is used in this work. \n\n- The BoxE paper (Abboud et al, BoxE: A Box Embedding Model for Knowledge Base Completion, NeurIPS, 2020), which is also a box embedding model (proposed for link prediction), is very relevant for this work: It shows that boxes can capture arbitrary relational hierarchies in the space. This is shown for binary relations and therefore it is more general, but it clearly applies to class hierarchies which are only a special case. This suggests that one can model hierarchies using box embeddings in an even simpler way, where box containment implies a subclass relationship (without the probabilistic interpretation). If we already know the class taxonomy, we can even inject this information to the space, i.e., if C is a subclass of D, one can enforce the corresponding C-box to be contained in the corresponding D-box in the space, etc. That is, if the class taxonomy is known, one can provably enforce these using the ideas presented in (Abboud et al). If the taxonomy is not known, then it can still be learned.\n\n- The probabilistic interpretation of boxes may bring in some value, but it is not clear to me whether this is the case, after reading the paper. In fact, the above-outlined approach would achieve the similar goals in a simpler way in my understanding - This may possibly result in an even stronger baseline than the given baselines. Notice that when a C-box is contained in the D-box after training, any prediction for class C will be consistent, since it will also be a D by the space configuration (and there are many possible configurations that can achieve the same thing). \n\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"rJghN08aFr","text":"The paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space. The abstract space is learned utilizing both model-free and model-based losses, and the behaviour policy is based on planning combining the model-free and model-based components with an epsilon-greedy exploration strategy. Learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space.\n\nAs it stands, I am leaning towards rejecting the paper, for the following reasons.\n(1) while the idea proposed is interesting, the current work rather explores it in a limited manner which is unsatisfactory.\n(2) I think the presentation of the bonus itself -- novelty search (Section 4), which is the core of the paper, is rather unclear. (3) The assumption of deterministic transition dynamics may be ignored in favour of games which seem to be our benchmarks, but the results presented for the control tasks, Table 1, are not statistically significant, and the paper is missing details about the architecture\/sweep for the baselines experimented with. \n(4) Parts of the paper is rather unclear\/feels disconnected -- for instance, the interpretable abstract representation bit; this was a loss in the original work, and seems to be just mentioned arbitrarily here while the loss isn't really used (unless it is used, and not mentioned in the paper).\n(5) Overall, the proposed reward bonus is a heuristic whose specific design choice isn't statistically shown to be useful (Ablation in Appendix), and the empirical results comparing to other methods are underwhelming.\n\nHere are my main points of concern which I hope the authors address in the rebuttal:\n(1) Designing reward bonuses to induce exploratory behaviour in the agent has seen a surge of publications in the Deep RL literature in recent years. The key property all these methods aim for is a bonus that pushes the agent to the boundaries of its current \"known region\", and then rely on the stochasticity due to epsilon-greedy to cross that boundary -- pushing this boundary further. While this is different from exploration to reduce uncertainty, it is nonetheless a reasonable approach leading to competitive policies when evaluated in deep RL. But a characteristic all these bonuses aim for is that they fade away with time -- for instance count-based bonus are inversely proportional to visit counts, or prediction error bonuses go to 0 as the prediction becomes more accurate. But what do these novelty bonuses converge to? Is it just a stationary value based on consecutive loss parameter (in which case the hope is they don't affect the external reward scale, they just shift it uniformly)?\n(2) What exactly are the nearest neighbours? Is it a search based on the data in the buffer or is it a notion of temporal neighbours?\n(3) If it's temporal, why would there ever be biased for some states -- \"We do this in order..novel states\".\n(4) I was completely unable to understand the section in the Appendix which is making a case for the ranked weighting. If you have a succinct explanation for the heuristic it'd be great.\n(5) Further, as a heuristic it is mentioned that l2 norm may not be effective if the dimensionality of the representation space is increased. So why the heuristic? I think it either needs more empirical validation, or a theoretical justification.\n(6) While the evaluation scheme used in the paper to quantify the exploration of the behaviour policy is interesting -- y-axis of plots in Figure 4 for the Labyrinth task -- why\/what exactly is the role of Figure 2? Is the interpretability loss used here? Is it to reason for utilizing e-greedy instead of a purely-greedy behaviour? I think this is a little unclear, and can be better clarified. Further, the distinction of primary and secondary features is interesting, but their clear demarcation is rather questionable in more complicated domains -- in the abstract space.\n(7) Do you have a hypothesis for why the 1-step value functions are not sufficient for decision making in this simple domain -- labyrinth - with the abstract representations?\n(8) If model-based algorithms get more steps to learn shouldn't model-free too? I'm not sure I understand the reasoning for the experiment design choice.\n(9) Whats the architecture used for Bootstrap DQN? It needs to have multiple heads -- but based on the current architecture that doesn't seem likely.\n(10) Are the extrinsic rewards ignored in learning -- \"only focus on intrinsic rewards\" (Section 6.2.2)? If they are for the proposed method, are they for the competitors too? If so why, and what is the reward for Bootstrap DQN?\n(11) I think the Discussion section raises interesting points about interpretability and metric learning, but I do think the conclusions drawn are a little inflated.\n(12) The ablation study in Section D of the Appendix is not statistically significant -- so why is wighted reward useful? Please comment.\n(13) How would stochasticity in transition dynamics affect the abstract representation space? Discussing this would be very interesting.\n(14) Learning curves for the control tasks?\n\nComments about typos\/possible points of confusion:\n(1) The last para in Section 6.1 -- discusses \"open\" labyrinth heat map, then what do we mean by learning the dynamics of the wall? There is no wall in open, right?\n(2) In Section 4 -- I think x_{t+1} is an estimate from the unrolled model -- \\hat{x}_{t+1}? Further, it would be helpful to mention that it is an estimate based on the learned model.\n(3) n_freq is used in the pseudocode in the main paper -- but no mention of it to explain it is made in the main.\n(4) Contrasting the work to existing literature would be useful (in the Related Work section; as opposed to summarizing existing work).\n(5) buffered Q network --> target networks?\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"ryYjvicxM","text":"The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real\/generated data, compared to training on real data only. Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution. \n\nIn addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for. If the aim is to have realistic samples, a visual turing test is probably the best metric. If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option.\n\nPROS:\nThe idea is interesting. \n\nCONS:\n1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction). It would be interesting to understand how the different metrics relate. Moreover, the new metric is introduced with the following motivation ‚Äú[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution‚Äù. The mode collapse issue is never discussed elsewhere in the paper. \n\n2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays. Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. \n\n3. The authors should clarify if the method is specifically designed for GANs and VAEs. If not, section 2.1 should contain several other works (as in Table 1). \n\n4. One of the main statements of the paper ‚ÄúOur approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)‚Äù is never proved, nor discussed.\n\n5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive. Further, how many tau‚Äôs one should evaluate? In order to evaluate a generative model one should test on the generated data only (tau=1) I believe. In the worst case, the generator experiences mode collapse and performs badly. Differently, it can memorize the training data and performs as good as the baseline model. If it does actual data augmentation, it should perform better.\n\n6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models. In fact, the limit of training with a fixed dataset is that the model ‚Äòsees‚Äô the data multiple times across epochs with the risk of memorizing. In the proposed protocol, the model ‚Äòsees‚Äô the generated data D_gen (which is fixed before training) multiple time across epochs. This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability.\n\n\nMinor: \nSection 2.2 might be more readable it divided in two (exploitation and evaluation).   \n","cats":["1"],"entities":[],"Comments":[]}
{"id":"SJlghKO937","text":"Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task  as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. \n\nNovelty: This texter is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. \n\n","cats":["1"],"entities":[[534,699,"toxic"]],"Comments":[]}
{"id":"GRm39Fj2E_","text":"This paper presents, in a principled way, a novel and interesting way of applying Nesterov-alike Acceleration to adaptive DL optimizers. When applied on a number of optimizers, architectures and vision classification tasks, the accelerated version shows substantial improvements across the board. The authors claim that, thanks to the principled formulation, they are able to prove advantageous convergence properties in Theorem 1, appendix B, but I wasn't able to find said appendix nor the theorem on the paper (also, some of the provided references are unused, which may be a related issue).\n\nI think the idea of adaptive Nesterov is good and well presented, and experiments show clear advantages, so I propose to accept the paper as I consider it an interesting addition to the DL optimizer literature.\n\nComments\n* It is true that Adam is very popular and it generally helps a lot with convergence, but it has been theoretically shown to not converge, and even fail to generalize in some cases (https:\/\/opentext.net\/forum?id=ryQu7f-RZ https:\/\/arxiv.org\/abs\/1509.01240)\n* While results look very compelling and the effort to is welcome, authors may want to consider extending to established optimizer benchmarks to enhance comparability (https:\/\/github.com\/fsschneider\/DeepOBS https:\/\/arxiv.org\/abs\/2007.01547)\n","cats":["0"],"entities":[[1075,1313,"positive"]],"Comments":[]}
{"id":"Nko_4qPt-o5","text":"Through four studies, this paper proposes to lift a theoretical limitation in the application range of the Dual Gaussian Distribution Model, namely that it could also work when touch acquisition occurs from a touchscreen  to that same touchscreen.\n\nThis paper is well written and shows good experiment design and consistent analyses.  \nHowever I found the theoretical argument to use the DGDM in screen-to-screen pointing quite hard to follow, even though it is the main point of this article. I also have a number of concerns that I would like to see addressed in a revision.\n\n\n# BLAMING AGE\n\nHonestly, I found it quite a weak argument to put the lack of generalization of the approach on age (p. 10). Age difference is one among many possible explanations, but one in which this paper rushes in nevertheless, at the expense of any other. \nThe paper doesn't even acknowledge that this lack of success could simply be due to a lower external validity than the authors hoped for. As the authors state themselves p. 9, \"A common way to check external validity is to apply obtained parameters to data from different participants.\" Checking can also come up negative, and that is ok. These results remain valid, even if the proposed approach is not as context-independent as hoped.\nPerhaps worse, the paper immediately jumps from this patched-together explanation, straight to calling it a \"novel finding\", and then to suggesting design guidelines from it, as if it was now a proven fact.\nI think this part needs to be drastically shortened or even removed, in favor of a more realistic discussion about generalization---and possible lack thereof.\n\n\n# \"UNLIMITING\"\n\nI found it quite hard to understand the point of Bi et al. for rejecting screen-to-screen pointing, at least the way it is explained in this paper. That, in turn, makes it quite difficult to understand the counter-argument developed in this paper---and especially since \"The evidence comes from a study by Bi et al.\" (p. 4), which makes one wonder why Bi et al. put that \"limitation\" up in the first place.\n\nOne example, in the last paragraph before EXPERIMENTS (p. 4), a point is made that goes like this:\n- a lack of effect might be due to A values that are too close to each other, \n- even if A should in fact have an effect according to some model (Eq. 12), - and for some reason that makes it ok to consider that screen-to-screen pointing is compatible with Bi et al.'s model (which does not consider A).\n\n\n# DESIGN APPLICATIONS\n\nI am not sure that the possible applications of this model are well described or argued for in this paper. The described examples feel rather artificial.\n\n- In the example given in p. 1 (choosing between 5 or 7-mm circular icons), it is unclear why the designer would need a model, or to know by how much a 7-mm icon would improve accuracy. It seems that this sort of design issues can be solved using threshold values under which users simply cannot accurately acquire a target. I assume that strong design guidelines already exist for this?\n\n- Similar argument about the second and third paragraphs in p. 9. The level of detail argued here seems quite artificial, e.g. \"If designers want a hyperlink to have a 77% success rate\". I doubt many designers would consider a clickable, 2.4-mm high font or icon on a touch screen in any case. I might be wrong.\n\n- \"by reducing the time and cost of conducting user studies, our model will let them focus on other important tasks such as visual design and backend system development, which will indirectly contribute to implementing better, novel UIs.\" (p. 2)\nThat seems quite a stretched \"contribution\", at least in the absence of actual data about how long designers do spend on testing width values today.\n\n\n# AMOUNT OF ERROR\n\nThroughout the paper, prediction errors (additive) up to 10% are described as small, and that is surprising (5% in Exp 1, 10% in Exp 2, 7% in Exp 3, 10% in Exp 4).\n\nTo the best of my understanding, these are not percentages of prediction error (e.g. going from 50 to 55 is a 10% increase), which would be more ok. These are differences between values that are already expressed in percents. \nIn my experience, many pointing studies have error rates ranging from 0 to, say, 15%, perhaps more when the tasks or input devices make it particularly difficult. 2-mm targets on a touch device could definitely count as difficult. However, that still makes a 10% prediction error quite high in my book, and worthy of contextualization. Perhaps I misunderstood something.\n\n>> \"the error rate difference was |29 ‚àí 38| = 9%. Similarly, their 2D tasks showed only small differences in error rate, up to 2% at most.\"\n-\nFirst, for a metric that can often be between 0 and 15%, 2 and 9% are not \"similar\" values.\nSecond, 29% and 38% error seems alarmingly high.\n\n\n# CLARITY\n\nRemoving tap points that are further than a fixed distance away from the target center will likely affect W levels differently. I imagine that more of these errors occurred in the W=10mm condition. This would be good to report, either way, even though only a small number of trials was removed overall.\n\nFig. 12 should also show the actual success rates measured in these studies.","cats":["2"],"entities":[[1278,1484,"toxic"]],"Comments":[]}
{"id":"SklR0NgUpQ","text":"---\nUpdate: I think the experiments are interesting and worthy of publication, but the exposition could be significantly improved. For example:\n\n- Not sure if Figure 1 is needed given the context.\n- Ablation study over the proposed method without sparse reward and hyperarameter \\alpha\n- Move section 7.3 into the main text and maybe cut some in the introduction\n- More detailed comparison with closely related work (second to last paragraph in related work section), and maybe reduce exposition on behavior cloning.\n\nI like the work, but I would keep the score as is.\n---\n\n\nThe paper proposes to use a \"minimal adversary\" in generative adversarial imitation learning under high-dimensional visual spaces. While the experiments are interesting, and some parts of the method has not been proposed (using CPC features \/ random projection features etc.), I fear that some of the contributions presented in the paper have appeared in recent literature, such as InfoGAIL (Li et al.).\n\n- Use of image features to facilitate training: InfoGAIL used pretrained ResNet features to deal with high-dimensional inputs, only training a small neural network at the end.\n- Tracking and warm restarts: InfoGAIL does not seem to require tracking a single expert trajectory, since it only classifies (s, a) pairs and is agnostic to the sequence.\n- Reward augmentation: also used in InfoGAIL, although they did not use sparse rewards for augmentation.\n\nAnother contribution claimed by this paper is that we could do GAIL without action information. Since we can shape the rewards for most of our environments that do not depend on actions, it is unsurprising that this could work when D only takes in state information. However, it is interesting that behavior cloning pretraining is not required in the high-dimensional cases; I am interested to see a comparison between with or w\/o behavior cloning in terms of sample complexity. \n\nOne setting that could potentially be useful is where the expert and policy learner do not operate within the same environment dynamics (so actions could not be same) but we would still want to imitate the behavior visually (same state space). \n\nThe paper could also benefit from clearer descriptions, such as pointers to which part of the paper discusses \"special initialization, tracking, or warm starting\", etc., from the introduction.","cats":["1"],"entities":[],"Comments":[]}
{"id":"Clvpch4dzB2","text":"This submission studies how the choice of activation function impacts the reproducibility of experiments involving deep networks. It proposes a new activation function with the goal of designing a smoothed ReLU, and provide experiments comparing it against other activations in terms of irreproducibility (measured via PD) and performance.\n\nThe problem of understanding how model design choices can have negative impacts on experimental reproducibility is interesting and timely, but I believe the paper does not provide a strong enough case for their approach and contributions.\n\nFirst, the adopted metric to measure irreproducibility, 'Prediction Difference (PD)', is never evaluated in terms of how sensible of a metric it is to capture reproducibility -- this also seems to be lacking in [1]. Actually , one can argue that it is not a sensible metric at all (except for its Hamming form), as it is not invariant to how the models are calibrated, as discussed below.\n\nFor example, take any binary classifier and consider two copies of it with different calibrations (i.e. scaling the output layer weights by positive scalars, one for each model): even though the models always agree on their predicted labels regardless of their calibrations, the PD can be made arbitrarily close to 0.5 by calibrating the models appropriately. Even more worrying is that the same can be done by taking a binary classifier and a copy of it with flipped predictions: the PD between the two can be made arbitrarily close to 0 by scaling their weights down. Note that this problem also happens with the relative PD.\n\nTo see how this is connected to the choice of activation functions (especially ReLU x SmeLU), note that for normally-distributed inputs (centered around the origin), gradient's variance of ReLU is 1\/4 while for SmeLU it is approximately sigma^2 \/ (4 beta^2) for large enough beta (sigma^2 being the variance of the input distribution): this discrepancy can have a non-trivial impact on the model's calibration and cause differences in PD to be artifacts.\n\nSince this doesn't happen with the Hamming form of PD I believe Figure 15 in the Appendix to be the most informative one. However, it seems that different activations result in less than 1% prediction discrepancy across models, which is fairly insignificant and hence it is hard to argue that activations actually matter for reproducibility (at least from the presented experiments).\n\nLastly, it is hard to draw any conclusions from the presented experiments: the CTR results are based on a private dataset while the MNIST ones are extremely small-scale, with both the dataset and the model being arguably toy problems. There are numerous tasks where reproducibility is a prominent issue e.g. deep reinforcement learning, generative modelling (especially GANs), making training a 2-layer network on MNIST a poor choice to evaluate reproducibility problems.\n\nAs an additional note, the authors seem to rely heavily on the work of Shamir & Coviello '20 [1] which introduced the PD metric, even though the paper was only made publicly available on arXiv a week --after-- the texting period for this submission started. When citing papers which are yet to be made available it would be helpful to introduce and discuss the relevant content in a self-contained way -- while the authors avoided much of my confusion by presenting the full definition of the PD metric, the referred paper has useful information which was not discussed (such as which summand is normalized in its relative form and how the different variants compare).\n\nSince I have major concerns with the paper -- particularly on the reliability of PD as a metric and the unconvincing empirical results -- I am voting for rejection.\n\n[1] Shamir & Coviello, Anti-Distillation: Improving reproducibility of deep networks,\n\n\n------------\n\n\nUpdate after rebuttal:\n\n\"It appears that the comment made by the texter may stem from an assumption that two models which are compared for PD can be different in the operations they perform to generate the predictions.\"\n\nThis is incorrect, my text does not mention such assumption and my statements hold without it. As stated in my text, I consider models with different weight magnitudes, making no assumptions on the underlying cause.\n\n\"PD, as we defined in Section 2, is aimed explicitly at measuring differences between predictions of a set of models that are supposed to be identical in all their components\"\n\nIndeed, and my point is that comparing the PD of two sets of models that are not identical is also problematic **even if all models within each set are identical**, except for the PD in its Hamming form. More details below.\n\n\"Changing calibration between such models violates this assumption.\"\n\nPlease check the celebrated work of Guo et al., \"On Calibration of Modern Neural Networks\": calibration does not necessarily consist of an explicit, additional component that modifies the model, and the same model trained in different ways can present distinct calibrations. More specifically, two sets of models can have not only the same accuracy, but the exact same predictions (i.e. there is a 1-1 mapping from each model in one set to a model in the other set that has the exact same predictions for all data points) but vastly different internal calibrations, which will result in vastly different PDs (to be overly specific, the scalar PD of a set will be different from the scalar PD of the other set) even though the two sets agree \"point-wise\" in terms of predictions.\n\n\"If one changes something about one of the models (including how calibration is done), one would expect them to predict differently, and have different accuracies.\"\n\nThis is incorrect. First, I'm not assuming models are explicitly calibrated, only that they have distinct internal calibrations (confidences in terms of predicted probabilities, which depend mostly on the parameters' magnitudes). Second, \"scaling the output layer weights by positive scalars\" (quoting from my text) will not change a model's accuracy: while it changes the class-wise predicted probabilities, the rank of the logits is preserved. If the authors remain skeptical of this fact, let $\\phi(x)$ denote the activations of the previous to last layer of a model, and let $\\langle w_i, \\phi(x) \\rangle > \\langle w_j, \\phi(x) \\rangle$, where $w_i$ and $w_j$ are the weight vectors of output units respective to classes $i$ and $j$ (i.e. $p(y_i | x) > p(y_j | x)$ for probabilities produced by a softmax over logits). Then for any $\\alpha \\in \\mathbb R_+$, we have trivially that $\\langle \\alpha w_i, \\phi(x) \\rangle > \\langle \\alpha w_j, \\phi(x) \\rangle$ (hence $p'(y_i | x) > p'(y_j | x)$, for probabilities $p'$ computed from the new logits). Again, note that it is --not-- necessary for an external, explicit calibration factor $\\alpha$ to be employed: training the network differently, or even adopting a different activation function -- just consider $\\max(0, 10x)$ for clarity, which will scale $\\phi(x)$ by a positive factor and yield the same observation as above.\n\n\"Specifically, if one flips the predictions of a binary classifier, the flipped model will have much worse accuracy from the actual model of interest, and measuring PD at this point is irrelevant.\"\n\nThe fact that two classifiers with vastly different accuracies can have zero PD is worrying and shows that PD is not a trustworthy metric: claiming that such evaluation is 'irrelevant' and should not be done does not address the issue.\n\n\nSince the authors remained unconvinced that the PD is sensible to positive scalings of a model's parameters, and hence comparing the PDs of two sets of models with different activations (one activation per set) is not sensible, here is a more detailed explanation of this fact.\n\nAssume a fairly trivial example for clarity: two 1-d data points, $x_1 = +1, x_2 = -1$, and binary classification models $f_1, f_2$, where $f_1(x) = \\sigma(w_1 \\cdot \\phi(x))$ and $f_2(x) = \\sigma(w_2 \\cdot \\phi(x))$ are the assigned probabilities for the positive label, and $\\phi: \\mathbb R \\to \\mathbb R$ captures some notion of activation function and\/or scale of weights before the final classification layer. For simplicity, let $\\phi(x) = \\alpha x$, for some $\\alpha \\in \\mathbb R_+$, and feel free to think of $\\alpha$ as a 'magnitude' of an activation function instead of some notion of internal calibration.\n\nThen, we have $P_{1,1} = (\\sigma(\\alpha w_1), \\sigma(-\\alpha w_1))$, $P_{1,2} = (\\sigma(\\alpha w_2), \\sigma(-\\alpha w_2))$, $P_{2,1} = (\\sigma(-\\alpha w_1), \\sigma(\\alpha w_1))$, and $P_{2,2} = (\\sigma(-\\alpha w_2), \\sigma(\\alpha w_2))$. The PD of the set consisting of the two defined models, after simplifying the 8 relevant terms, ends up being simply $\\Delta_1 = |\\sigma(\\alpha w_1) - \\sigma(\\alpha w_2)|$. Let's pick some numbers to make this crystal clear: let $\\alpha = 1, w_1 = 1.0, w_2 = 0.1$, so we get $\\Delta_1 = \\sigma(1) - \\sigma(0.1) \\approx 0.2$ (note that w.l.o.g. we can assume that $y_1 = +1, y_2 = -1$ so that for these weights both models achieve 100% accuracy).\n\nNow, take ANOTHER set, consisting of models $g_1, g_2$, defined similarly to $f_1, f_2$, but with $g_1(x) = \\sigma(w'_1 \\cdot \\phi'(x)), g_2(x) = \\sigma(w'_2 \\cdot \\phi'(x))$, where $\\phi'$ (not the derivative of $\\phi$) captures the the activation function and\/or weight magnitude of layers preceding the classification head. Let $\\phi'(x) = \\beta x$ for simplicity. Consider the case where $\\beta = 0.1, w_1 = 1.0, w_2 = 0.1$, i.e. the weights of $g_1, g_2$ are *exactly the same* as the weights of $f_1, f_2$, but $\\phi'$ is a 'scaled-down' $\\phi$ (e.g. a different activation function): in this case (note that both $g_1$ and $g_2$ achieve 100% accuracy as well), **for this new set of models, consisting of the pair $g_1, g_2$**, we get $\\Delta_1 = \\sigma(0.1) - \\sigma(0.01) \\approx 0.02$, a value around 10 times smaller than the PD of the first set of models, **even though the second set predicts the exact same labels for each data point**, and claiming that the set $\\{g_1, g_2\\}$ is 'more robust' than the set $\\{f_1, f_2\\}$ in terms of reproducibility is simply factually wrong. If the idea of having $\\beta \\neq \\alpha$ sounds a bit of a stretch since the proposed activations are not simply 'scaled down' ReLUs, consider instead the case $\\beta = 1.0, w_1 = 0.1, w_2 = 0.01$ and note that we again get $\\Delta_1 \\approx 0.02$ for this second set of models: the discrepancy in terms of magnitude of weights can be caused by different optimizers, different strength of $\\ell_2$ regularization, or, as my original text already mentioned, smaller variance of gradients w.r.t. activation function.\n\nTo reiterate, in the above example we did **not**, at any point, compute the PD of a set of models that had different components: both $\\{f_1, f_2\\}$ (the first set) had the same 'activation function' $\\phi$, while $\\{g_1, g_2\\}$ had $\\phi'$.\n\nGoing a step further, which shows how problematic the PD is as a metric, consider an arbitrary set of binary classifiers $S_1 = \\{f_1, f_2, \\dots, f_M\\}$, where $f_i(x) = \\sigma( \\langle w_i, \\phi(x) \\rangle)$ is the probability assigned by the $i$'th model of $x$ belonging to the positive class. Now, take *another* set of binary classifiers $S_2 = \\{g_1, g_2, \\dots, g_M\\}$, with $g_i(x) = \\sigma(\\langle w_i, \\phi'(x) \\rangle)$, where $w_i$ is the **same** weight vector that model $f_i$ has (i.e. except for $\\phi'$, the set $S_2$ is 'point-wise' identical to the set $S_1$). Finally, let $\\phi'(x) = \\beta \\phi(x)$, where $\\beta \\in \\mathbb R_+$, and feel free to check that for any $\\beta$, every model $g_i$ from $S_2$ will agree with the model $f_i$ from $S_1$ in terms of predicted class (i.e. although the class probabilities will change, the rank is be preserved for any $\\beta$). This means that $S_2$ produces the **exact same** predictions as $S_1$ for **any possible data point**. Taking $\\beta \\to 0$ yields in $g_i(x) \\to 0.5$ for any $i \\in [M]$ and possible $x$, hence **the PD of $S_2$ will go to zero, even though the PD of $S_1$ can be arbitrarily large and the two model sets $S_1, S_2$ agree point-wise in terms of predicted classes**. In other words, taking an arbitrary set of models with ReLU activations, copying its weights and replacing the ReLU by $\\phi(x) = \\max(0, \\frac{x}{10^{10}})$, will yield a second model set with PD close to zero. Hopefully the authors agree with me that this trivial replacement of activation functions does not 'solve' any reproducibility problem in machine learning.\n\nWith the above in mind, I urge the authors to re-evaluate PD as a metric. As mentioned in my text, the Hamming form does not suffer from this issue, but the reported numbers in this case seem to indicate that there is little to no reproducibility challenge for the adopted tasks.","cats":["1"],"entities":[],"Comments":[]}
{"id":"_3U37vs8xfz","text":"The paper is well structured and easy to follow. I also agree with the authors that prediction calibration is crucial in the M-open world, which is arguable always the case. The prospered conformal Bayesian computation method sheds some light on this direction. \n\nHowever, I am concerned about whether the novelty of the proposed method can be distinguishable from its existing building blocks. Conformal inference\/distribution-free Predictive Inference‚Äîand thereby the main algorithm considered in this paper‚Äîare not new ideas. It is also not a new idea to use (regularized) importance sampling to avoid model refitting and obtain posterior integrals in the context of either the leave-data-out (LOO) or the add-one-data-in (AOI, e.g. B√ºrkner et al. 2020 used this importance-sampling-AOI idea in time series).  It appears the only methodology contribution this paper is making is to apply the importance sampling strategy to the model refitting step in conformal Bayesian inference. I am not convinced that such incremental contribution would grant a publication. \n\nIn terms of its practical usefulness, I have two concerns. First, throughout the paper, the authors seem to emphasize the situation in which x_{n+1} is fixed. But this is not a typical perdition task in which we have a known x_{n+1}. In the words of Lei  (2018): ‚ÄúIn some applications, where Xn+1 is not necessarily observed, prediction intervals are build by evaluating 1{y ‚àà Cconf (x)} over all pairs of (x,y) on a fine grid‚Äú. Perhaps x_{n+1} can also be evaluated on a grid, but then that method is rather related to jackknife-conformal-inference, which itself can be computed by importance sampling.  \n\nSecond, the ‚Äúcorrect frequentist coverage‚Äù is a probabilistic statement averaged over all observed data and x_{n+1}. This does not mean the interval estimate derived from conformal Bayes is exact for y_{n+1} conditional on a fixed x_{n+1}, nor necessarily optimal for interval predictions (optimal in the sense of interval scoring rules). Think about the Jackknife case in which the bandwidth is a constant across x_i. I suggest the author clearly state the definition of frequentist coverage as not all audience in the machine learning community can immediately tell the subtle difference.\n\nMinor:\n* L-82: why is the rank normalized by n+1? I think the rank statistic should the integer. \n\n* How is the grid of y chosen? Could you use Bayes prediction to design the grid?\n\n* What is the advantage of AOI against jackknife used in conformal Bayes? Is the only difference that IS-AOI is computationally easier than IS-LOO?\n\n* ‚ÄúThe CB and Bayes methods have comparable run-times‚Äù I think it is misleading. CB can have an arbitrarily larger running time if x_{n+1} is evaluated at many values.\n\n* Overall, the experiment section is relatively weak. The sample sizes in all three examples are too small to reflect a modern big data challenge. It is not common to see all MCMC running time < 1 second from ML conferences.\n\n* L-336: I think ‚Äúleverage‚Äù is the wrong concept here. The leverage only depends on x. Here you have the influence of y too.\n\n* \"If only approximate posterior samples are available, e.g. through variational Bayes (VB), then an AOI scheme may still be feasible, where one includes an additional correction term in the IS weights for the VB approximation\". I disagree. Note that Magnusson et al. (2019) are on the LOO of exact Bayes when there are only VB samples. In your context, you need not only adjust for the VB approximation for sigma_{n+1}, but also all sigma_{1}, ... sigma_{n} for lack of exact posteriors. Such a task is generally not feasible.\n","cats":["2"],"entities":[[818,990,"toxic"]],"Comments":[]}
{"id":"H1e2XwOit4","text":"This paper presents the DRS, a class of random functions. In contrast to GPs, DRS uses splines to define the function; in particular it defines a piecewise function using a spline on each interval, making sure that the overall function is continuous (and so are its derivatives). The parameters of each spline are modeled using a deep neural network that takes Gaussian noise as input. Importantly, the parameters of each spline can be chosen so that the modeled function satisfies certain desired shape properties, such as non-negativity, monotonicity, or convexity. As a use case, the paper shows an example of application that uses non-negative splines to model the rate of a Poisson process, analogous to the log-Gaussian Cox process. Inference is carried out with amortized variational inference.\n\nI found this is a strong paper and therefore recommend acceptance. It is well explained and the method is of interest to the community.\n\nOne question that I had is how this model performs\/scales with the dimensionality of the output space (i.e., the analogous to the multi-output GP).\n\nAnother interesting point would be to compare against the log-Gaussian Cox process in the experiments.\n\nAs a minor comment, from page 3 I didn't understand why the Q matrices are of size 2x2 when d=3. On page 2, it says that each matrix Q if of size k-by-k when d=2k+1, so for d=3 shouldn't we obtain k=1?\n","cats":["0"],"entities":[],"Comments":[]}
{"id":"rylBeSoS3X","text":"The paper discusses clustering sparse sequences using some mixture model. It discusses results about clustering data obtained from a restaurant loyalty program.\n\nIt is not clear to me what the research contribution of the paper is. What I see is that some known techniques were used to cluster the loyalty program data and some properties of the experiments conducted noted down. No comparisons are made. I am not sure what to evaluate in this paper. ","cats":["1"],"entities":[],"Comments":[]}
{"id":"HygW6Tz7ar","text":"[Additional text]\nThis paper proposes a technique to incorporate document-level topic model information into language models. \n\nWhile the underlying idea is interesting, my biggest issue is with the misleading assertions at the very beginning of the paper. In the second paragraph of Section 1, the paper claims that RNN-based LMs often make independence assumptions between sentences, hence why they develop a topic modelling approach to model document-level information. Some issues with this claim:\n\n1. Pretty much every LM paper that evaluates on language modelling benchmark (PTB, WT-103, Wikitext-2) uses LSTMs\/Transformers incorporate cross-sentential, document-level information as context, through a very simple approach of just concatenating all the sentences and adding a unique token to mark sentence boundaries.\n\n2. Prior work has shown that LSTMs\/Transformers with cross-sentential context can, and in fact do, make use of information from previous sentences.\n\na. Evidence 1: Khandelwal et al. (2018) showed that LSTMs memorise word orders from the past ~50 tokens, and retain semantic information from the past ~200 tokens; both of which extend far beyond the length of an average sentence, suggesting that information from the previous sentences is used in the predictions of the current sentence.\n\nb. Evidence 2: Language models that operate on single sentences typically do worse than language models that take into account cross-sentential context, e.g. the language model of Kim et al. (2019) that operates on single sentences gets ~90 ppl. on PTB test set, while LSTMs that condition on multiple sentences get a much better ~50-something ppl. on Mikolov PTB.\n\nCrucially, these prior works defeat the paper‚Äôs motivation of why it claims to need topic models in the first place (i.e. to model cross-sentential context), while just concatenating multiple sentences as context would do, and in fact has been done many times.\n\n2. Prior work (mostly in Transformer-land) has come up with ways to make use of very long-range context, from Transformer-XL to the more recent compressive Transformer (https:\/\/opentext.net\/forum?id=SylKikSYDH) that can condition on entire books. While these are done for Transformers, in principle one can also apply similar techniques to LSTMs.\n\n3. While Transformer-XL has the potential to make use of word orders in the preceding sentences, it seems that this paper‚Äôs approach cannot do that, since they only take the bag-of-words from the preceding sentences. It thus seems that their bag-of-word approach is less expressive, and hence less powerful, than the simpler alternative of concatenating sentences.\n\n4. The perplexity results (Table 1) are not done on very standard datasets (no PTB evaluation for instance). It is thus hard to evaluate the strength of the baseline models. In the paper's defense, it seems that they were following the experimental setup of Wang et al. (2019), but the paper should elaborate more on the choice of evaluation datasets.\n\n5. The inference part is not particularly self-contained. The paper simply refers the TLASGR-MCMC method (which is an important part to make inference scalable) to prior work (Cong et al., 2017; Zhang et al., 2018), yet does not explain (even briefly) how the approach works, and how it can be combined with their recurrent topic model formulation.  \n\n6. Evaluation of the induced topic hierarchy (Figure 4) is only done through qualitative samples, and the paper does not really explain how to pick the samples (i.e. possible cherry-picking). I am not very familiar with the topic modelling literature, but it would be nice if the induced hierarchy can be evaluated quantitatively.\n\nReferences:\n1. Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away. In Proc. of ACL 2018.\n2. Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gabor Melis. Unsupervised recurrent neural network grammars. In Proc. of NAACL 2019.\n3. Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen, and Lawrence Carin. Topic-guided variational autoencoders for text generation. In Proc. of NAACL 2019.\n4. Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou. Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC. In Proc. of ICML 2017\n5. Hao Zhang, Bo Chen, Dandan Guo, and Mingyuan Zhou. WHAI: Weibull hybrid autoencoding inference for deep topic modeling. In Proc. of ICLR 2018","cats":["1"],"entities":[],"Comments":[]}
{"id":"ryxG7NUmcS","text":"#rebuttal responses\n \nThe authors' reply does not convince me, and I still think the paper has some problems:\n(1) I do not believe that the cumulative model-error can not be learned efficiently;\n(2) Experimental results are weak as some baselines do not converge! \n\nThus I keep my rating as reject.\n\n#text \nThis paper proposes a new adaptive model-based value-expansion method, AdaMVE, that decides the planning horizon of the learned model by learning the model-error. The model-error is learned by temporal difference methods.\nExperimental results show that AdaMVE beats MVE, STEVE, and DDPG in several environments.  \n\nOverall the paper is well written.  The paper proposes an interesting question: how to adaptively change the planning horizon based on the state-dependent model-error? Firstly, The authors upper bound the cumulative target error by the cumulative model-error.  Then the cumulative model-error is learned by the temporal difference method. With the learned cumulative model-error function over different rollout steps, the planning horizon is decided by a softmax policy.  \n\nHowever, I do not think that learning an upper bound of the target error helps to determine the value of H, as there is no justification that the gap between the target error and the model error is small theoretically.  I also doubt that the cumulative model-error can be learned without large loss, as there are no plots of W in this paper. \n\nThe authors claim that it is expensive to retrain the model error for the current policy at every step, thus they use some reference policy. I think it is ok, but I want to see the results of AdaMVE using the updated current policy, or updating the Q function before improving the policy. Adding a figure showing the change of H in the training helps to motivate this paper.\n\nFinally, AdaMVE is only compared in two MuJoCo environments. Baselines in other environments are only trained in 1e5 steps, thus the experimental results are not convincing.\n\nI am happy to change my opinion on this paper if authors give better motivation and the detail of the learning.","cats":["1"],"entities":[],"Comments":[]}
{"id":"QdKxMcoxfmA","text":"This paper uses the intrinsic dimension of layers (estimated by [1]) to show that the intrinsic dimension of the last layer correlates with the regularization applied. This metric can be used to measure a model's generalization capability. Further, they show that good generalization requires a trade-off between decreasing the intrinsic dimension of the last layer and keeping a high intrinsic dimension for the layer with the highest one. \n\nThe paper is clearly written and understandable throughout. \nThe topic fits well with this workshop, and the results might be of high interest to the whole community. Considering the introduced metric might pave the way to a better understanding of and new regularization methods.\n\nWhat should be changed\/added:\n- There are no error bars\/uncertainty intervals reported. Please repeat your experiments with multiple seeds to prove that your results are significant.\n- Add a detailed description of the ID estimator you are using. One does not want to read an other full paper to understand what you are measuring.\n\nI like that you added hyperlinks to almost all of your references! (Should become a new standard) \n\nTypos, grammar, and formatting:\n- l 13 to 16: hard to understand sentence\n- Figure 3: Please add a description of the red curve to the figure. Took me a while to find it in the caption.\n- l 348: Typo in 060\n\n[1] E. Facco, M. d‚ÄôErrico, A. Rodriguez, and A. Laio. Estimating the intrinsic dimension of\ndatasets by a minimal neighborhood information. Scientific Reports, 7(1):12140, Sep 2017.\nISSN 2045-2322. doi: 10.1038\/s41598-017-11873-y. URL https:\/\/doi.org\/10.1038\/\ns41598-017-11873-y.","cats":["0"],"entities":[],"Comments":[]}
{"id":"HylaQFF8cH","text":"This paper develops a new few-shot image classification algorithm. It has two main contributions. The first one is to use a metric-softmax loss used to train on the meta-training dataset without episodic updates. The second is that the features learnt thereby are further modified using a linear transformation to fit the few-shot training data and the metric soft-max loss is again used for classifying the query samples. The authors provide experimental results for 5-way-1-shot and 5-way-5-shot testing on mini-Imagenet and CUB-200-2011 datasets.\n\nI think this paper is below the acceptance threshold. The reasons are:\n\n1. The contributions of this paper are marginal: both learning centroids for each meta-training class and projecting the few-shot features have been used before in published work (https:\/\/arxiv.org\/abs\/1905.04398). The empirical results are weaker than existing work (see for instance, https:\/\/arxiv.org\/abs\/1904.03758, https:\/\/arxiv.org\/abs\/1909.02729 etc.); also see #3 below.\n2. The authors should provide experimental results on other few-shot learning datasets like tiered-Imagenet.\n3. The image-size used here for Resnet-12 is 224x224, the authors should report results using 84x84 image size so that one can compare against existing literature fairly. Are the results for Resnet-12 so good because of the larger image size?\n4. The training procedure is task-agnostic, why do you train a different model for the 1-shot and the 5-shot case?\n\nI will consider increasing my score if some of the concerns above are addressed. I am listing some more comments below which I would like the authors to consider.\n\n\n1. Contributions: ‚Äúconsistency between training and inference‚Äù, do you instead mean consistency between meta-training and few-shot training? There are no weight updates at inference time.\n2. How essential is the metric-softmax loss? Training on the meta-training dataset without episodic updates has also been done in https:\/\/arxiv.org\/abs\/1909.02729. These authors seem to use standard soft-max training and perform standard fine-tuning, they report empirical performance that is significantly better than that in Table 4 and Figure 2. I am very skeptical as to why the accuracy of fine-tuning is only 21% in Figure 2.\n3. Section 3.2 does not motivate or explain the metric-softmax loss. Why should one have the network learn the centroids of the meta-training dataset? Can you draw a TSNE of the centroids learnt during meta-training? The features of the support samples (or their transformations) can be the centroids of the few-shot classes in the prototypical loss so inference phase does not need these centroids.\n4. I am not sure whether the matrix M is changed non-trivially during few-shot training. The weights W are already initialized to be the centroid of the features (eqn. 9). So the metric-softmax loss in eqn. 10 is expected to be small for the support samples after initialization. Why should the additional expression power afforded by M matter? There is no incentive for the network to change the matrix M. Can you show results on how much M changes from the identity?\n5. I believe the reported numerical results for LEO (Rusu et al. 2019) are for a WRN-28-10 architecture, not ResNet-12.\n6. The accuracy using Resnet-12 seem extremely high. I believe this is because the results reported in the literature, e.g., https:\/\/arxiv.org\/abs\/1904.03758, use images of size 84x84, not 224x224 as the authors here have used. Can you report results using 84x84 sized images?\n7. I don‚Äôt understand the explanation at the end of Section 5. Since the prototypical loss is being used to classify the query datum, it should not matter whether the cluster is shrunk in the 5-shot case, or whether simply the distances between the clusters are increased as in the 1-shot case.\n8. Table 1 is quite incomplete, the authors should mention other existing few-shot classification results are similar to the performance of this paper, e.g., https:\/\/arxiv.org\/abs\/1805.10123, among the ones listed above.\n9. The entries in Table 1 and 2 are not made bold appropriately. All entries with overlapping standard error should be bold.","cats":["1"],"entities":[],"Comments":[]}
{"id":"r7bhZSitrVI","text":"\nSummary: Employing several different models, this paper demonstrates how aggregated wastewater data from across the US can be used to forecast COVID-19 cases. This paper also evaluates the optimal horizon for forecasting COVID-19 case data from wastewater signals. \n\nClarity:  This paper was well written and the paper‚Äôs objectives are clear. There are also clear descriptions for why given models were chosen for this evaluation.\n\nTo improve upon the clarity, I would suggest the following: \n\n--Further explain why case counts were used instead of hospitalization counts as the COVID-19 outcome metric. The given explanation in the paper is that, ‚Äú...case count data becomes an effective indicator of the strain on the healthcare system and the potential long-term effects of SARS-CoV-2 infection‚Äù. However, this same logic applies to COVID-19 hospitalization data, which did not suffer from the same notorious underreporting as case data did. This is not to say case data shouldn‚Äôt be used, just it is not clear why this was the outcome metric chosen. \n\n\n-- it is unclear at what time stamp ‚Äúground truth‚Äù data was being pulled. Did the authors use case data as-of the date of model evaluation (i.e., potentially revised case data)? Or only case data available the week of wastewater data collection? \n\n\n--Based on figures 5a - 5c, the error (measured in NRMSE) does not look much worse at 9 days vs. 6 days. It would help if the authors could clarify more objectively how the cutoff for 6-8 days was determined as the optimal horizon period. \n\n\nMinor comments on clarity: \n\n--A short sentence or two about what is being measured in the wastewater would be beneficial. What gene is being targeted \/ measured to determine COVID-19 concentrations?\n\n--Because there is so much variability in wastewater data, it might be helpful to mention how the prediction intervals of models are impacted by the changes in the wastewater data.\n\n--In section 4.1.1, definitions are needed for variables in equation 1. It is not immediately apparent what each different ‚Äúx‚Äù represents. Additionally, in section 4.1.1, the authors mention an evaluation of data delays in section 5.5, however section 5.5 is about horizons, and not data delays. \n\n--A note that the hyperlinks for footnotes 1 and 2 are broken. \n\n\nOriginality: There are similar articles that have compared modeling approaches on their ability to predict COVID-19 cases from wastewater data, however, this paper adds to the growing body of literature by using a segmentation approach on publicly available data, as well as determining the ideal forecast based on prediction accuracy and maximizing a longer forecasting horizon.\n\nSignificance: The significance of this paper is that it demonstrates that numerous modeling approaches provide similar results when using wastewater data to predict COVID-19 cases at a national level. This paper could be improved by adding discussion of the biases and limitations of using data aggregated at a national level, and demonstrating how well these models perform at a smaller geographic scale. Discussion of the confidence levels of these models would also be beneficial, as would using multiple metrics to evaluate model performance.\n\nPros:\n\n--Well written paper with many clear discussions about decisions made in the experimental process. \n\n-- Use of publicly available data makes methods replicable. \n\n-- Project opens the door for additional analyses that can be done using wastewater data, as well as additional variables that can be added to the analysis. \n\nCons:\n\n--As noted in sections above, the paper could expand on the limitations of using wastewater and case data at the national level, such as heterogeneity in COVID-19 across the county, non-standardized collection approaches across counties \/ states, variation in sampling sites, etc. \n\n\n--The determination of why 6-8 days is the optimal horizon is not clear from the figures presented. \n","cats":["1"],"entities":[],"Comments":[]}
{"id":"rNxxIICCaZ9","text":"This paper is relevant for the workshop.\nThe authors explore which transforms need to be applied to a source object to match a target object.\nThe \"Interventional Framework\" section is described well.\nIt would be nice to see the qualitative results on the synthetic data.\nIn Figure 4: It is not clear how the MSE is computed? Also, to improve the paper for future submissions it would be worth comparing to some of the methods mentioned in the related work.\n\nWhy is the model better at scale y than scale x? Is this to do with bias in the data? Similar for translation x and translation y?\nHow do you disentangle depth from scale? Do you assume a fixed depth? This would be worth discussing.","cats":["1"],"entities":[],"Comments":[]}
{"id":"fnD4GR-s73","text":"===== Strength ======\n1. The paper points out an interesting fact and problem: most of the parameters in a neural net are in the last fully connected (FC) layer, while in a large-number-of-class and non-IID setting, each client may only have data from a small portion of classes. The authors thus accordingly proposed the federated sampled softmax to save the communication cost by local training with only a small number of \"negative\" classes in addition to the \"positive\" classes.\n\n2. The authors conduct experiments on three large-scale datasets. The authors conduct the ablation study on how many negative classes are needed. \n\n==== Weakness =====\n1. The technical contribution is not sufficient. There have been many methods like (Bengio & Sen¬¥ecal,2008) and [a-c] that subsample the negative classes in the softmax objective. Specifically, in [c], the proposed method did include the positive classes in a minibatch, together with a set of subsampled classes, to compute the softmax. While the authors compare different combinations of positive and negative classes in 3.4, to me, it seems pretty obvious that we should combine the clients' positive classes with subsampled negative classes. Besides, while the authors discussed several advanced methods in 3.3, they ended up choosing uniform sampling. I was wondering if any of these advanced approaches can be approximately implemented in a federated setting to further improve the performance.\n\n[a] Joulin et al., Learning Visual Features from Large Weakly Supervised Data, ECCV 2016\n\n[b] Mikolov et al., Distributed representations of words and phrases and their compositionality, NeurIPS 2013\n\n[c] Hu et al., Learning answer embeddings for visual question answering, CVPR 2018\n\n2. One potential direction to improve the novelty or technical contributions of the paper is to develop a series of methods for different kinds of representation learning in a federated setting, including learning a fully connected layer and metric learning. If we do not consider the computational cost at the client end, will metric learning (without learning the FC layer) outperform the proposed method? In [a], the authors proposed to learn multiple one-vs-all classifiers rather than a softmax classifier. Will it be effective in a federated setting?\n\n3. The experimental setup can be improved. There is no clear description of how many classes, images, local epochs for each dataset. I also have concerns about using the pre-trained features for 4.2 and 4.3, which makes 4.2 and 4.3 like downstream tasks rather than representation learning. I would suggest that the authors include a fixed feature baseline in Table 1 and Table 2 to demonstrate that fine-tuning the feature extractor in a federated setting could outperform the pre-trained features. \n\n4. The authors only apply the proposed method to the FedAvg baseline. Will the proposed method be applicable\/effective to more advanced federated learning algorithms like FedProx [d], Scaffold [e], and FedDyn [f]?\n\n[d] Li et al., Federated optimization in heterogeneous networks. In MLSys, 2020\n\n[e] Karimireddy et al., Scaffold: Stochastic controlled averaging for federated learning. In ICML, 2020\n\n[f] Acar et al., Federated learning based on dynamic regularization. In ICLR, 2021\n\n==== Other comments ====\n1. I would suggest that the authors add \"supervised\" into their title, to contrast to many recent works on \"unsupervised\" representation learning.\n\n2. I would suggest that the authors cite some more papers on federated learning.","cats":["2"],"entities":[[991,1198,"toxic"]],"Comments":[]}
{"id":"kUETMc3Fq6f","text":"The paper presents an approach for learning policies for long-horizon tasks (e.g., peg insertion) that relies on predicting \"key states\" along the trajectory. The main idea is that key states are a) easily predictable and b) useful for conditioning action predictions. The results show improvements over baseline methods on four tasks: pick cube, turn faucet, stack cube, and peg insertion. The approach uses ChatGPT to describe the steps for solving each task and then the authors design heuristic rules for each step to identify \"key states\" in expert demonstrations, which are used for learning (via an auxiliary training objective). The paper is clearly written.\n\nThe paper does not use RL and the use of prior knowledge (from ChatGPT) is limited. However, the approach should still foster interesting conversations at the workshop. Specifically, the high-level idea of using an LLM to decompose tasks and then using that decomposition for learning seems highly relevant.\n\nThe authors could consider other ways in which LLMs could be used in the propose approach. For example, can the LLM produce the heuristics rules for identifying the keys states after the steps for solving the task have been listed? Or can an LLM be used to propose methods for identifying key states without the need for privileged simulator information? Such techniques may help scale the proposed approach to more tasks and more domains.","cats":["1"],"entities":[],"Comments":[]}
{"id":"BJx5KwRTiN","text":"The paper presents 3 approaches for on-board scheduling of activities in a planetary rover under reservoir resource constraints.\n\nTwo of them, the Max Duration and Probe algorithms, are sound, but incomplete, while the third, the Linear algorithm, is sound and complete for linear rate resource consumption (but more computationally expensive).\n\nAn empirical evaluation shows that the Probe algorithm (the one currently baselined for use in the onboard scheduler for NASA‚Äôs next planetary rover, the Mars 2020 rover) performs competitively and compares the runtime differences between the three algorithms.\n\nThe topic is interesting, definitely up-to-date given the current efforts in planning and scheduling for the Mars 2020 mission, and it fits the SPARK workshop.\n\nWeakness of this paper in my opinion are in the presentation and in the justification given for the proposed approaches.\n\nRegarding the presentation, I found the paper not easy to follow: some definitions on Timeline Representations (like the \"impact\" for instance) are not standard in timeline based planning (meant as the systems like EUROPA, ASPEN, APSI and others surveyed in Chien et al, SpaceOps 2012) and would have maybe deserved better explanation besides the citation to Rabideau and Benowitz 2017. Also the Problem Definition presents some issues: what are exactly the \"zones\" for instance?\n\nMoreover, I would suggest to add an example, or figure, about the Probe Algorithm (at the end, since this is the chosen baseline, I think it would deserve better presentation).\n\nOn the empirical evaluation, Figure 4b, it is said that \"In Figure 4b, we see that the Linear algorithm strictly outperforms the Probe method while the Max Duration method strictly under-performs\". It does not look like honestly that Linear strictly outperform Probe, they look like almost the same... \n\nPlease check also the bibliography style, many citation appears incomplete.\n\nAbout the justification, it is understood that the system has to operate on-board under strict computational constraints (and in this case the Probe algorithm makes sense), but some words to justify such an ad-hoc approach would have been probably appropriate. This looks like an integrated P&S problem, broken into a sequence of steps aimed at satisfying a sub-set of the problem constraints. How do you control the impact of this specific step (the calculation of awaken states) on the whole plan? There is no side-effect that could violate other logical or resource constraints?\n\nTo conclude a curiosity: in alternative to the Linear Algorithm, would have been computationally too expensive to try a proper calculation of the resource envelope (min and max subject to flexible allocation of consumptions) and then calculating a minimal\/maximal flexible duration for the Asleep activities with a Max-Flow algorithm for instance?\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"Byg4aCzE5S","text":"Thank the authors for the response. I agree with R2 that the paper lacks comparisons with previous works. I will stick to my previous decision.\n----------------------------------------\nSummary\nThis paper presents a new approach for single-objective reinforcement learning by preferencing multi-objective reinforcement learning. The general idea is to first figure out a few important objectives, add some helper-objectives to the original problem, and learn the weights for each individual objective by trying to keep the same order as Pareto dominance. This paper has potential, but I lean to vote for rejecting this paper now, since it is still not ready. I might change my score based on the texts from other texters.\nStrengths\n- The idea is novel. Learning weights for each objective by keeping the order as Pareto dominance is an interesting idea to me.\nWeaknesses\n- The lack of experiments. The authors tested their method in only one scenario, which makes me feel unsafe. Only testing on one simple scenario does not demonstrate the effectiveness. The authors are supposed to test their method on more (complex) scenarios to show the effectiveness of their method.\nPossible Improvements\nAs mentioned before, the proposed method can be tested on more scenarios (e.g., Deep Sea Treasure, SuperMario, etc.).","cats":["1"],"entities":[],"Comments":[]}
{"id":"HyxtTsDjKH","text":"Pros:\nThis paper proposed a new method for zero-shot transfer learning under the reinforcement learning setting. The use of attention weights to regularize the latent states was fairly interesting.\n\nCons:\nLimited applicability of the proposed methods\n- The paper was restricted in a setting where rewards, actions, and true states were identical between source and target environments, and only the observed states differed due to differing renderers. Working under such a restricted setting was interesting in its own right, but it might also lead to limited applicability of the proposed method in the real-world setting.\n- The proposed method focused on solving a very specific problem: learning a dis-entangled latent representation for images. As a result, the potential impact of the proposed methods could be minimal.\n\nLimited technical novelty\n- The proposed method, SADALA, was built on top of Higgins et al., 2017 (DARLA). The only difference was an added attention layer to the learning of latent states. As a result, the novelty of the proposed method was very incremental and limited from a technology perspective.\n- Even with additional attention layer, the paper could have performed a more thorough study to help the readers understand and appreciate the idea. For example, this paper didn‚Äôt discuss the tradeoff between training SADALA over separate stages, versus training it from end to end. For example, why the weights of the pre-trained beta-VAE had to be frozen and used as weights in the state representation stage.\n\nInsufficient experiments\n-More thorough discussion of the qualitative results should be helpful to understand whether the attention weights helped the model to focus on the right thing. For example, this paper did study the quality of reconstruction in Figure 3-5 of the proposed method. When comparing Figure 3 and Figure 5, it appeared to me that the reconstructed the angle of the pole was different from the original one. And it seemed like attention weights did successfully ignored the color of the cart and pole, but it ignored the angle of the pole, which should be important to the learning task. Unfortunately, the paper didn't further explain the implication of such misrepresentation.\n\n-Quantitative results \n* It would be interesting to all compare the proposed methods against model-agonistic methods like MAML\n* It would be useful to include confidence intervals over different tasks.\n* It would be useful to compare different methods with different parameter settings\n* The authors mentioned ‚ÄúVisual Pendulum tasks‚Äù but didn‚Äôt include them in the paper\n\n\nReproducibility\n- It's unclear to me how reproducible the research conducted in this paper was, and it would be useful to open source the code used to conduct the experiments.","cats":["1"],"entities":[[1569,1727,"positive"]],"Comments":[]}
{"id":"RqQ76OXx9l","text":"The authors present a method to assess the similarity between pairs of images for large datasets that relies on the scale invariant feature transform. Both the method and the results described are interesting and of high quality. However, this work does not seem to be in line with the topic of the MIDL conference as no deep learning seems to be used in the analysis.","cats":["1"],"entities":[],"Comments":[]}
{"id":"MvmApzelGe","text":"In the paper, the authors apply geometric deep learning methods to predict the functional organization of the human visual cortex from MRI data. Curvature data, myeling values, and connectivity of vertices on the cortical surface are passed through spline-based convolution layeres to predict the retinotopic map. The network is tested on data from the HCP dataset.\n\nThe paper is clearly written, presents an application of geometric deep learning methodology on a medically relevant dataset, with network architecture that seems well-suited for the task. The methodology and application is relevant for the MIDL audience. I believe the paper fits very well the intended focus and scope of a MIDL short paper.","cats":["0"],"entities":[],"Comments":[]}
{"id":"pIWDwAVUjoy","text":"In this paper, the authors studied the problem of identifying meaningful clinical patterns among patients who had been prescribed opioids and subsequently had the doses reduced over different lengths of time. Overall the paper has several strong aspects\n- It was able to identify several dosage patterns that maybe of interest towards clinical determination\n- The initial analysis seems to point towards differing health outcomes for patients with slow vs rapid tapering (see more below)\n- The paper covered sufficient details about cohort characteristics to let the texters judge the impact of the findings\n\nHowever, from a health economic outcome research aspect, the paper is currently at an early stage and may need further followups to support the validity of the identified patterns. The authors have acknowledged the limitation of not considering other factors that may capture the intent to reduce\/increase dosing. However, this is a key aspect that may need to be validated, perhaps with certain assumptions such as IPW, to satisfy the significance of the findings.  Further, the authors may want to considering survival analysis methods, especially with the possibility of right censored events, to further analyze the clinical outcomes of the identified cohorts. \n\nOverall, this papers has certain promises but may be improved upon from a modeling and analysis aspect.","cats":["0"],"entities":[[609,790,"positive"]],"Comments":[]}
{"id":"SJemyeYW5r","text":"The paper describes a new dual method for graph convolutional networks that combines the features from the graph and it's dual, in two pipelines. The paper builds on the architecture as in GCN and in addition to the dual pipelines, one from the graph and other it's dual, employs KL divergence to achieve the final prediction.\n\nThe paper leaves inadequate explanation on the results, where the proposed TwinGCN comes short in 2 of 3 methods compared to other methods in Table 1, which cannot be ignored considering the slow convergence and marginal improvements, compared to GCN with double pipeline in Table 3. This leaves the premise of the authors on adding improvements to the learning ability by bringing in features from dual graph on shaky grounds.","cats":["1"],"entities":[],"Comments":[]}
{"id":"XBnlTjsPGa","text":"This work presents a new approach for efficiently training multi-modal (image-text) models by aligning existing pre-trained unimodal encoders. The paper is very clear and the results are impressive, showing improvements over state-of-the-art models such as LiT (let alone traditional multi-modal training methods such as CLIP) on the challenging zero-shot ImageNet classification task.","cats":["0"],"entities":[],"Comments":[]}
{"id":"ptzNzYVnTKS","text":"The paper explores the question of whether a successful idea in vision and language - of learning representations with masking can also extend to the reinforcement learning setting. To test this they combine masking with the transformers to predict masked trajectory sequences from the rest of the trajectory. They utilize (state-action-returns) as each element of the trajectory. This model allows them to solve a number of problems in RL in a unified way - Forward dynamics, inverse dynamics, BC, and offline RL.\n\nStrengths:\n1. With a variety of experiments authors successfully show that the method if able to achieve improved performance across all tasks using a single model - showing the importance of features learned when training using masking.\n2. The authors present an ablation of important components of their method which is a good addition.\n\nQuestions:\n1. In the purely offline setting with downstream representations, should an offline RL algorithm have been compared instead of TD3 since it is not clear if the vanilla method is failing due to lack of good representations or the overestimation issue.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"IOUzN8U1RM","text":"The main claim of the paper is that benign overfitting occurs in the adversarial case as well--that is, even though the training data are overfit perfectly, one can still attain reasonable generalization error. However, in practice it is well-documented that overfitting is indeed worse for adversarially-trained models, so I think an adequate analysis in this case would explain this phenomenon. I was not able to get such an explanation from the paper, although I may have missed it.\n\nAside from explaining empirical results, another potential contribution of theory papers is introducing new proof techniques. The paper does not include a discussion of what new technical contributions or proof techniques it contributes, so it was difficult to assess this aspect. While there is a proof outline in Section 5, it is not clear what is novel and what follows the previous proofs.","cats":["1"],"entities":[],"Comments":[]}
{"id":"rGfgm6tlhZq","text":"**Summary**\nThis work proposes a novel factored architecture that aims to leverage the reward factorization structure. The main idea is to learn a mixture of state encoders to compute factor representations. The computed factor representations are used to compute factor value which is aggregated to compute the state value, and also used to compute the policy via attention mechanism. The experiment was conducted on Procgen and Minigrid domains where the architecture of the baseline model is replaced with the proposed architecture.The result shows that the proposed architecture improves the sample efficiency of the baseline method.\n\n**Pros**\n* The proposed idea is interesting and well-motivated\n* The paper reads well\n* The experiment was conducted on diverse tasks\n\n**Cons**\n* The performance improvement is marginal\n* It is unclear whether it is a fair comparison in terms of the architecture capacity. (see below)\n* The presentation can be improved (see below)\n\n**Major comments**\n* Is it a fair comparison in terms of the capacity of model architecture? It seems the proposed architecture multiplies the capacity of the model by a number of factors. It should be made sure that both architectures of the baseline and baseline+AFaR should have the same capacity for fair comparison.\n* Why disconnect gradients from policy for learning representation? The gradient coming from the policy loss is a major source of learning representation. It would be better if authors provide more intuitive reasoning behind this design choice.\n* Although it is a design choice, it may be more natural to use summation instead of mean for computing state-value following the factored reward MDP definition. So it would be better to present a more intuitive justification for such a design choice.\n* It would be more interesting if authors can analyze the different ‚Äúmode‚Äù of policy learned for each factor.\n\n\n**Minor comments**\n* The experiment result for ‚Äúsparse‚Äù is missing in Figure 2, but ‚Äúsparse‚Äù is mentioned in the comment of Figure 2 and section 4.2.","cats":["1"],"entities":[],"Comments":[]}
{"id":"mwf_pCynhc9","text":"Summary:\n\nThe authors proposed fractional variational autoencoder (FVAE) for the learning of disentangled representation where the action sequences can be extracted step-by-step. Experiments are shown to illustrate how the algorithm works.\n\n#################\n\n\n. The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. Is it the same as the objective of \\beta-VAE?\n\n. Fig.3: 1) What's the KL divergence here? Is it between the posterior and the prior? 2) It's claimed that the trend of KL divergence is consistent with that of entropy. But it is hard to see from Fig. 3. 3) It is claimed that the significance of action is related to the capacity of learned latent information. Based on Fig. 3, this conclusion is not convincing. Also, Fig. 3 is obtained based on a toy dataset. To claim it as a main contribution, the conclusion needs to be verified on other datasets as well.  \n\n. Section 4.1: What's definition of the label here? It's not clear. Is it like the types of shapes on dSprites?\n\n. Section 4.1: The training on dSprites includes two phases: find thresholds and then train different stages. 1) The authors arranged  three stages for dSprites. This seems arbitrary. Why not four or five stages? 2) What's the training objective function of each stage? 3) How are the curves in Fig. 5 derived? More explanation is required.\n\n. Section 4.2: It is claimed that ``One can recognize three points where the latent information suddenly increases: 60, 20, 4.'' This is hard to see from Fig. 5b) as all curves look smooth. Thus, the following three-stage training process is questionable. The training for unlabeled task needs more study.\n\n. The experiments are limited. There are a lot of papers regarding disentangled representation, and the authors only compared with \\beta-VAE. \n\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"HzfxxRV0pZ5","text":"Given a video of a real-world system, the aim of this paper is to set up a corresponding system in simulation. To that end, the paper proposes a combination of methods that detect individual objects and infer their physical parameters and mutual joint types. The inference of articulation as a tree of bodies and joints is the main contribution.\n\nStrengths:\n* The proposed pipeline can correctly infer the individual bodies and joints of a real-world pendulum system with three bodies and three joints. This is done given only a video and prior knowledge of what objects we can encounter.\n* While the method assumes access to the geometries of individual objects, the actual object recognition and pose detection from images is done without taking any shortcuts.\n* The method is shown to enable control of a simulated cartpole.\n\nWeaknesses:\n* The evaluation of the individual components of the proposed system is lacking. It is unclear what the limitations are, except for only modeling simple objects.\n\nOverall, this paper fits the theme of the workshop well and it reports interesting results.\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"_PRjdxzjmq","text":"The paper aims to improve the training of ViT models under a fixed computational budget (one GPU, 24 hours)---much smaller than what is usually used. Progress on this problem would enable shorter development cycles of new models, and make them more widely accessible to researchers with less compute hardware.\n\nThe work proposes modifications of the architecture (locality) and order in which data is presented to the model (curriculum learning) that speed up training. These modifications are evaluated, both jointly and independently, on a new benchmark derived from ImageNet1k by addition of constraints on the computational budget. Results indicate that both mechanisms help achieve faster training within the given budget.\n\n(I am not very familiar with the architectures used in the paper, hence the low confidence score.)\n\n---\n\nDetailed comments:\n\nSome of the architecture modifications are motivated, but exchanging the GeLU activations seems arbitrary. It would be good if the authors could provide a motivation in section 2 how this modification helps to speed up training.\n\nIn section 3.1 it would be helpful to add a remark about how the training hyperparameters were chosen.\n\nA satisfying extension of Figure 2 (left) would be to extend these plots to times longer than 24 hours. This should be doable with little effort. If the proposed model reaches the final performance of DeiT-S in less time, this would further strengthen the paper's contribution.\n\nI also think that the manuscript could benefit from a short \"Conclusion\" section at the end where the most important findings are restated. This will make the paper easier to skim.\n\n---\n\nMiscellaneous comments:\n\n- L104: \"excepts\" ‚Üí \"except\"\n- L110: Missing article before \"well-known\"\n- L115: \"is\" ‚Üí \"are\"\n","cats":["0"],"entities":[[961,1081,"positive"]],"Comments":[]}
{"id":"HyxRBRMZ94","text":"This paper proposes a new lower bound for variational inference based on q-deformed logarithms, a generalization of the logarithm that augments it with a q parameter that controls its concavity. The authors train VAEs with this bound and report performance improvements over the ELBO, but not the IWAE bound.\n\nThe paper has issues with clarity, and rigor, but is overall a reasonable contribution.\n\nSpecific Feedback:\n\n1. The derivation of the q-deformed lower bounds is lacking in rigor. The new bounds are just stated by swapping the q-logarithm for the standard logarithm without discussing whether that is possible. A proof that the new bound is a valid lower bound would be useful.\n\n2. Similarly, it is not clear if swapping in the q-logarithm gives a lower bound on the log likelihood of the data (q=1) or the q-deformed log likelihood of the data for a specific value of q. The latter seems more likely. If that is the case, a discussion of the benefits and drawbacks of optimizing a lower bound on the q-deformed log likelihood of the data would be helpful to the reader.\n\n3. In the actual training procedure the authors optimize bounds with different values of q for each batch. An argument should be made that this procedure is still optimizing a valid lower bound on the (possibly q-deformed) log likelihood.\n\n4. The optimization procedure for q is not clearly stated. It seems like q* is set to make the qELBO evaluated with q=q^* match qELBO* as closely as possible. So perhaps the optimization procedure attempts to minimize (qELBO* - qELBO(q=q*))^2 w.r.t. q*. This should be stated clearly, and the optimization procedure should be clearly motivated.\n\n5. In evaluation, what method is used to estimate log \\hat{p}_x?\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"HyeOL_lGcB","text":"Summary:\nThe paper proposes two new regularizers for adversarial robustness inspired by literature on verification of ReLU neural networks for resilience to epsilon perturbations using convex relaxations. The paper shows empirically that the proposed method leads to better robustness than previous works.\n\nStrengths:\n+ The paper seems to have an interesting perspective (with the proposed looser relaxation) of the convex relaxation of an adversary adding noise at every layer in the network\n\nWeaknesses:\n\n*Sec. 4.1: Eqn. (O) does not have a convex relaxation, it is the exact problem which is intractable. Why are we comparing the optimal values of p*(O) and p*(C)? The paper from Salman et.al. already shows that there is a convex relaxation barrier, which essentially corresponds to this difference. In general, in Sec. 4, it is often unclear whether when we talk about p(O) if we are referring to the unrelaxed original problem or the tightest convex relaxation. For example, at the start of Sec. 4.1, it seems like we are talking about the convex relaxation and then in Sec. 4.3 it seems like we are talking about the unrelaxed problem.\n\n*It is not clear how\/ why the proposed method of relaxing (which by the way seems identical to Fast-Lin (Weng et.al.) is better than the optimal convex relaxation. Would this not lead to looser bounds? Is that the thing we are looking to investigate? Making that more clear would be useful. Perhaps it would be good to argue the proposed regularizer in this work cannot be constructed with the optimal convex relaxation. Is that true? A discussion on this would be helpful.\n\n* The crux of the contribution seems to rest on the premise that identifying the optimal perturbation in the input space with the relaxed model, and then computing the activations with respect to that and forcing the forward pass to saturate near the margins of the relu polytope (relaxation) is a good idea. In general, it seems very unclear why this should work based on the evidence presented in the paper. Specifically with the relaxation, it might not even be guaranteed (as far as I understand) that the value of \\delta_0^* that is found from problem C is even going to lie inside the L\\inf norm ball around the point x, for example. Thus it is not clear to me if this is an approach for verification or a regularizer based on verification.\n\n* Ultimately, the value of the approach in this context (as per my understanding) comes from the experiments and the results which show that there is increased robustness. It would be great to clarify a couple of details in the experiments:\n1. Is the method of Wong et.al. using the looser convex relaxation (used here) or the tight convex relaxation when reporting the numbers in Table. 1? \n2. If the optimal convex relaxation can be used to construct the same regularizer as the one proposed here, it would be good to evaluate how well that does.\n\nOverall, I am not an expert in the area but a lot of details from the writing (such as point 1 under weakness) and the theoretical justification of the regularizer are unclear to me. Thus given these (perceived) weaknesses I would lean towards weak rejection. Clarifications on these points would help me revise my score.","cats":["0"],"entities":[[320,492,"positive"]],"Comments":[]}
{"id":"rJxJvdtzcE","text":"The paper proposes a framework for human evaluation of generative models of images. It is based on samples, so it is compatible with any flavour of generative model (likelihood-based, adversarial or otherwise). Two different evaluation strategies are proposed: one based on the time it takes for humans to distinguish generated images from real images, and another which simply measures the percentage of images that are wrongly classified.\n\nThe implementation of the human evaluation setup is described in appropriate detail, and attention to cost is also given. The results are comprehensive and statistical tests are used to show their significance. The approach is also compared to FID, a computational evaluation metric that is currently popular.\n\nOverall, this is work is timely and it is well presented, so I am in favour of acceptance. Nevertheless I have a few more comments and suggested improvements below:\n\n* It is demonstrated that the correlation of HYPE and FID is relatively poor, and it is implied that this demonstrates that FID is a poor metric. However, as the authors state earlier on in the paper, HYPE can only measure realism, not diversity. FID is explicitly constructed to also be affected by sample diversity, so in that light it is not surprising that the two do not correlate very well, and that higher truncation leads to improved HYPE but worse FID scores -- it is well known that truncation reduces diversity of the samples, in favour of improved fidelity. (I do not wish to imply that FID is actually a good metric -- I do believe it is a poor metric, but not for this particular reason.)\n\n* While the authors state clearly that HYPE does not measure diversity, I think it would be worth discussing in more detail how one could use human evaluation to measure diversity, as it is arguably a more interesting challenge. As it stands, the HYPE metric could probably be fooled by a \"model\" which simply stores a few training examples and randomly selects them with equal probability. Also measuring the diversity of the samples in some way would prevent this kind of cheating.\n\n* A common issue with human evaluation is ambiguity in the task specification: the raters are instructed to determine which images are real, but they may be prone to misinterpreting the task in a way that biases the results. While rater training and immediate feedback undoubtedly help to limit this effect, it is still worth considering this carefully, and I think a few diagrams or screenshots of the rater interface would be useful additions to the manuscript in this respect.\n\n* In the introduction, it is implied that likelihood (measured in the input space) would be the ideal metric for generative models if it were always easy to compute (which it often isn't). Theis et al. (2015), cited there, also call this into question. I find the juxtaposition of this citation and the sentence before it a bit misleading.","cats":["0"],"entities":[[1625,1851,"positive"]],"Comments":[]}
{"id":"p5rtDzJ7Fdr","text":"While this submission presents an internally valid research effort I have major doubts on how the work scales to real world usability scenarios, and thus on how valid and interesting the findings will be to the GI audience and readership. \n\nThe paper's main strength is the introduction of a Bayesian approach to gaze target selection. There is a body of past work on Bayesian frameworks for eye tracking and eye control but, as far as I know, not in the HCI context of target selection on a phone\/tablet.\n\nThe paper has a lot going for it: it is well written, the related work section is thorough, and the researchers seem to follow an overall solid methodology (though I was quite confused by the WOz  approach to study 1, please see more below).\n\nThe main reason I am not supportive of acceptance is what I believe to be a lack of attention to external validity in the current manuscript. Several aspects of the work seem to be narrowing down the scope of the research to lab settings with little insight on whether the findings will have much meaning in the real world, and with little attention to how the findings might scale to real task scenarios and real settings. This is a major flaw in my eyes and I hope the authors will at least attempt to provide more insight and discuss these aspects and possible implications of their work in case the paper is accepted to GI'21.\n\nMore specifically, the 1D target selection tasks, which are core to the paper and its two studies, are very limiting and questionable. I was not sure if the authors suggest that a 1D target selection task is valid in some settings? And if yes, which ones? If the 1D target selection task is just a precursor to 2D target selection, why are the authors convinced that their findings will scale? \n\nWhile the paper's motivating Figure 1 seems to suggest a phone form factor, the study experiments are using an ipad which can maybe qualify as a large screen phablet, placed on a desktop stand. Again, the experimental settings are not invalid, but are very constrained, and arguably do not map to real world phone usage settings. I was hoping the authors could provide more insight on how their work could possibly scale to actual usage scenarios, where a a much smaller handheld phone is being handled and addressing various head poses (and yes, mostly 2D target selection tasks...) and maybe thinking of hybrid approaches that do not rely only on gaze for target selection. To clarify, I am not necessary suggesting an expansion of the studies, rather a much richer discussion of the limited scope studies presented in the paper in a wider context, allowing the reader to think of ways of applying the paper's findings in real world settings. \n\nI was confused by the authors suggesting that they used a Wizard of Oz approach for their first study (only?). The details are lacking and I am uncertain how a wizard could operate such a study, and whether the introduction of a wizard would not compound the findings. In case of acceptance I suggest the authors please elaborate on these points.\n\nFinally, a video figure would have helped clarify aspects of the studies and the findings.\n\n\n  \n\n","cats":["1"],"entities":[[2725,2834,"toxic"]],"Comments":[]}
{"id":"HEnxep5kRWq","text":"### Summary\nThis paper proposes an object-centric approach to \"imagination\" where a compositional inductive bias in the architecture is leveraged to generate new data points that are novel combinations of known concepts. Training models of such imagined scenarios helps them generalize more systematically. \n\nThe setting explored here is a simplified version of the Abstraction and Reasoning Corpus (ARC), named Sort-of-ARC, that uses the same input space, but considers a simpler (more restricted) set of operations to map inputs to outputs. In particular, the model is given access to a support set of correct input\/output pairs, where the outputs are the result of applying a transformation (eg. spatial translation) to the set of objects contained in the inputs that satisfy some condition (eg. all red objects). To succeed at this task in manner that leads to systematic generalization, the model first has to infer the underlying program that generated the support set and then apply it to the query input to generate the right output. This is analogous to other abstract visual reasoning settings, such as Raven's progressive matrices.\n\nThe proposed model consists of a controller and an executor. The controller encodes the input\/output images in the support set to produce a set of latents, called the instruction embedding, to give to the executor. The executor decompose the query input to obtain a set of entity-centric latent representations and updates them based on a neural program, after which they are decoded. The neural program is obtained by using the instruction embedding to query from a set of learned condition and transformation embeddings. The program is applied separately to each entity embedding to update them. Here the condition embedding gates the proposed update produced by the transformation to select only relevant entities to which the program applies. \n\nCompositional imagination can now acts as a learning paradigm, through applying arbitrary condition-transformation pairs to the inferred entities of the inputs from the support set to produce new outputs (this amounts to sampling indices for these embeddings, and applying the executor), The controller can then be trained on this new support set to yield program instructions that select condition and transformation embeddings that were used. This imagination loss is combined with the regular query output prediction loss.\n\nThe experiments indicate that (1) it is important that the mechanisms that operate on the object representations are modular themselves, and (2) that training with the imagination loss improves out of distribution generalization. \n\n### text\n\nThis is a nice workshop paper that explores compositional imagination as a framework for out of distribution generalization. The setting considered requires novel compositions to obtained through applying (condition, transformation) pairs, where it is further demonstrated that modularity at the level of mechanisms is also important. The proposed architecture for solving these tasks is intuitive, although perhaps more development could go into the design of the controller to produce the instructions. The novelty of the proposed mechanisms is limited, but this is okay. The paper is quite clear and easy to read. Some pro's and con's:\n\npro's\n\n* interesting results on novel benchmark, promising step towards ARC\n* compositional imagination is an interesting training paradigm adapted from prior work to the object-centric setting\n\ncon's\n\n* experiments indicate benefit of modular mechanism application, but the OOD results are not fully convincing yet. A potential reason for this is given by not explored\n* additional exploration regarding why selection hurts within distribution generalization or further exploration of imagination-based training (when it works, when it doesn't) would be helpful.\n\nWith regards to further developing this work I would encourage the authors to consider different modes of OOD generalization, eg. as considered in [2]. \n \n ","cats":["0"],"entities":[[2997,3166,"positive"]],"Comments":[]}
{"id":"B1xyY2gFhQ","text":"This paper proposes to use a  stochastically quantized network combined with adversarial training to improve the robustness of models against adversarial examples. The main finding is that, compared to a full precision network, the quantized network can generalize to unseen adversarial attacks better while training only on FGSM-perturbed input. This provides a modest speedup over traditional adversarial training.\n\nWhile the findings are certainly interesting, the method lacks experimental validation in certain aspects. The comparison with other adversarial training methods is not standardized across networks, making the efficiency claims questionable. Furthermore, I am uncertain whether the authors implemented expectation over transformations (EoT) for the C&W attack.  Since the network produces randomized output, vanilla gradient descent against an adversarial loss is likely to fail. It is conceivable that by taking an average over gradients from different quantizations, the C&W adversary would be able to circumvent the defense better. I would be willing to reconsider my text if the authors can address the above weaknesses.\n\nPros:\n- Surprising result showing that quantization leads to improved generalization to unseen attack methods.\n\nCons:\n- Invalid comparison to other adversarial training techniques since the evaluated models are very different.\n- Lack of evaluation against EoT adversary.\n- Algorithm 1 is poorly presented. I'm sure there are better ways of expressing such a simple quantization scheme.\n- Figures 2 and 3 are uninteresting. The fact that the model is robust against adversaries implies that the activations remain unchanged when presented with perturbed input.","cats":["1"],"entities":[],"Comments":[]}
{"id":"BJg5O9zd2Q","text":"The paper is easy to read and the presentation is clear, and I really appreciate this.\n\nThe authors address the very important topic of feature extraction and state representation learning. New results in this area are always valuable and welcome. However, my feeling is that the paper falls short in terms of making sufficient new contributions for an ICLR paper. \n\n1. The authors propose to learn a state representation by either training using a combined loss function, or training several representations using multiple loss functions followed by stacking. These are standard and well-known techniques in machine learning. The key contribution one looks for is in terms of new insights on why and when each approach works. The paper fails to provide much insight in this regard. Take this simple scenario: Suppose my input image is actually generated by a linear map plus gaussian noise on the true states. Then I can simply use a PCA as my \"auto encoder\" and happily learn a high quality state representation close to the ground truth. We know why this works. In the real task, the image is a complex non-linear transformation of the true states. What insights do I gain from this work in terms of how I should tackle this?\n\n2. Section 3 states some desirable characteristics in constructing a state representation. These are well-known and fundamental aspects of machine learning -- applicable to almost all models that we want to learn. In this sense, I do not find the section very informative.\n\n3. The empirical results (say, Table 1) seem too noisy to interpret (other than that using the ground truth provides the best performance). It almost seems to suggest that one should simply use random features (as done in the \"extreme learning machine\" approach). Again, not much insight to draw from this.\n\n4. Last comment. Suppose I have a new robotic goal-directed task and my inputs are camera images. Does this work tell me something that I don't already know in terms of learning new feature representation that is highly suitable for my task?\n\n\n\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"0DFsJu3urT","text":"Interesting parallelism setup for small batch sizes. Some nits:\n\n- \"In general, distributed training requires a minimum batch size per GPU (Œ≤min), which is generally close to one.\" why is this the case? many pipelines train with >>1 example per GPU.\n- \"On the other hand, increasing the batch size hurts the effectiveness of stochastic gradient descent\" the citation you include for that does not say that larger batches always hurt the effectiveness. in fact, many papers have shown that large batches train just fine with standard optimizers https:\/\/arxiv.org\/abs\/2102.06356 https:\/\/arxiv.org\/abs\/1811.03600\n","cats":["1"],"entities":[],"Comments":[]}
{"id":"HyjN-YPlz","text":"The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples. The two objective functions are based on assigning low confidence values to points that are near or off the underlying (learned) data manifold while assigning high confidence values to points lying on the data manifold. In particular, for an adversarial example that is distinguishable from the points on the manifold and assigned a low confidence by the model, is projected back onto the designated manifold such that the model assigns it a high confidence value. The authors claim that the two objective functions proposed in this manuscript provide such a projection onto the desired manifold and assign high confidence for these adversarial points. These mechanisms, together with the so-called shell wrapper around the model (a deep learning model in this case) will provide the desired defense mechanism against adversarial examples.\n\nThe manuscript at the current stage seems to be a preliminary work that is not well matured yet. The manuscript is overly verbose and the arguments seem to be weak and not fully developed yet. More importantly, the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments.","cats":["1"],"entities":[[1166,1307,"toxic"]],"Comments":[]}
{"id":"HfMgcgv6Xf5","text":"# Summary\n\nThis paper proposes task imagination mechanism at the top of object-centric representation learning while also considering the scenario of out-of-distribution adaptation. The whole scheme consists of several steps:\n\n- A conditional model (controller) that generates a task embedding based on the given datasets.\n- A object-centric encoder where the k-slot representations will be updated according to the task embedding given by the controller.\n- An imagination mechanism to generate unseen tasks based on the combination of learned `condition c` and `transform p`.\n\nIn my opinion, the methodology is very promising. Also the application (ARC) is quite special since the image pairs represent some underlying transformation, and it is more difficult than image reconstruction task in some recent object-centric learning works. I vote for acceptance. \n\n# Some Questions\nProbably I miss something. There are also some unclear parts in the experiments that may need some explanation. \n1. How the hyperparams are set (e.g. $\\alpha_{rec}$, $\\alpha_{im}$ in the loss, the total number $c$ and $p$.)\n2. What's the difference between query output loss and support set output loss.\n3. Do we really need both `condition c` and `transform p` to modulate the object-centric representations? A ablation comparison against using only one (i.e. $h_k^{new} = h_k + \\tilde{h}_k$) will be convincing.\n4. More explanation is need for the metrics in Table 1:\n    - Although accuracy is improved for OOD tasks, the performance of *With Imagination* has $~3$% drop for in-domain tasks. Is it caused by the suboptimal setting of the regularization coefficient ($\\alpha_{im}$)? If so, a hyperparameter search could be helpful to provide more insights.\n    - The performance of *With Imagination* and *No Imagination* in OOD tasks is not convincing for me. Considering the super large variance of  *With Imagination*, it is hard to say it is better. Nevertheless, the advantage of the other components is obvious.","cats":["0"],"entities":[],"Comments":[]}
{"id":"HfY19gtnbL","text":"Originality: I believe the theoretical results on distributed saddle point problem with $\\delta$-related assumptions are new.\n\nClarity: The structure and writing of the paper look fine. However, in the section 4, the authors provide little explanation for their algorithms. Although the algorithms are similar to some existing ones, it is easier for the readers to understand if there are some intuitions put in the paper. Same for the lower bound part, there is no overview for the hard instance in the main text. \n\nComments and questions:\n1. For the lower bounds in Theorem 1 and 2, on the right hand sides of the equations include only $\\|y^\\star\\|$. First, I think the authors did not define $\\|y^\\star\\|$ in the paper and I assume it is the optimal value for $y$. Second, the right hand side is not in terms of $\\|x_t-x_0\\|^2 + \\|y_t-y_0\\|^2$. Here if $\\|y^*\\|=0$, the lower bounds are not informative. \n\n2. The paper is based on many existing work, for example the lower bounds for smooth strongly-convex-strongly-concave minmax optimization and lower bounds for distributed minimization problem with $\\delta$-related assumption. Can the authors explain what is the technical novelty in this paper?\n\nminor:\n1. There is a typo under line 154 in the equation. \n2.  In line 17, the authors assume $X, Y$ to be compact, but in lower bound the case has unbounded domain. I think in the main text, the paper only discusses strongly-convex-strongly-concave setting and does not need to assume boundedness. It seems it is only needed in the convex-concave setting in the appendix. \n","cats":["1"],"entities":[],"Comments":[]}
{"id":"BJg2jRSKhm","text":" This paper proposed a so-called ISS-GAN framework for data hiding in images, which  integrates steganography and steganalysis processes in GAN. The discriminative model simulate the steganalysis process, and the steganography generative model is to generate stego image, and confuse steganalysis discriminative model. \n\nOverall the application seems interesting. My concern is its use in real secure information transmission systems: it can fool human eyes but what is its capacity against decoding algorithms; if the intent is to transmit some hidden information, how the receiver is supposed to decode it; is there something similar to the public key in encryption systems? These basic questions\/concepts should be made clear to the reader to avoid confusion. \n\nThe evaluation protocol should be clarified and especially on how the PSNR is calculated (i.e., using the reconstructed secret image and real one?) ","cats":["1"],"entities":[],"Comments":[]}
{"id":"QxkRd5ozRRy","text":"This paper tries to interpret PER from the lens of replaying experience that has the most \"surprise\" and shows how it connects to some notions of value such as the expected value of the backup and policy improvement value and evaluation improvement value and argue. The authors also derive a max-ent version of this and show that this can improve performance on some Atari games (though this is not that convincing).\n\nMy score for this paper is based on these points:\n\n Motivation: I do not see the motivation for introducing these metrics and why that explains PER in the first place. Agreed that PER is a reasonable choice, and it can upper bound the EIB and EVB metrics (i have issues with this too, more on this next), it just seems to me that the paper doesn't make any convincing claim for why this helps us understand why PER works. If the focus of the paper is on understanding PER, then the paper does not do a good job of it. If it is to introduce these prioritization based on these metrics -- and the paper focuses entirely on them -- then I then have several concerns next.\n\n- Definition of the value metrics: The cited definition of these metrics requires using the true Q-function or the true value function of the resulting policy. If we end up approximating this using the learned model, what is the guarantee that these metrics are indeed useful? Also theorem 1 should be restated to say that they care about the \"empirical\" EIB and EVB, that is computed using the learned Q-function, else it doesn't make sense to me. Moreover, if the TD error is a bound (which I think isn't with neural networks as I discuss in the next point) on the empirical EVB, can't I just drastically overestimate Q-values and get a larger empirical EVB value to be super high and prioritize on those examples? Why is that good? Why won't that promote overestimation? \n\n- Why is the update on the Q-value assumed to be tabular if the experiments are with a deep network on Atari? In a non-tabular setting Theorem 1 does not hold so either that should be rederived for the case of DQN or the experiments should be adjusted to do it on tabular settings.   In any case, now it is not clear to me why the method works with DQN, since the update in this setting isn't equal to $Q(s, a) \\leftarrow Q(s, a) + \\alpha TD(s, a)$. In general, the solution isn't known with neural networks, so the upper bound story doesn't hold there. With the NTK (Jacot et al.) assumption, I can obtain a somewhat similar update but pre-conditioned with the kernel Gram matrix (see Achiam et al. Towards characterizing divergence in deep Q-learning). However, Theorem 1 doesn't hold anymore now. So, it is unclear why the method works.\n\n- Even if I were to look at the experiments only, the results are not that impressive. The method is generally close to PER, and maybe a little better, but no comparison is made on a more efficient method such as Rainbow, and there are only 9 Atari games, which is too little. So, that is not super convincing yet.\n\nI would suggest the authors make some of the changes above.","cats":["2"],"entities":[[2707,2791,"toxic"]],"Comments":[]}
{"id":"GNDflfA73P","text":"The paper proposes an extension to the DARTS method for neural architecture search, by augmenting it with constraints, in particular constraints on the model size and memory consumption. The DARTS method continuously relaxes the combinatorial NAS problem and is hence able to use gradient-based optimizers to find a good architecture. The proposed modification of DARTS simply uses a different loss function that penalizes high model complexity whenever the constraints are not fulfilled. The experiments presented are very limited, even for a workshop paper, and it is hard to draw any conclusion from them. Nevertheless, the problem that the paper aims to address is certainly relevant and the proposed method seems promising enough. ","cats":["1"],"entities":[],"Comments":[]}
