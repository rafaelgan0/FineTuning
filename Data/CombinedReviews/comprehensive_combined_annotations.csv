id,invitation,review,reviewer_rating,annotation_round,r1,r2,r3,r4,r5,s1,s2,s3,s4,s5,proportional_agreement,average,majority
0Lpxs4k-mgd,ICLR.cc/2021/Conference/Paper841/-/Official_Review,"This paper empirically demonstrated the effectiveness of neural networks for learning to predict different mathematical properties of dynamical systems, which achieve high accuracy on synthetic datasets generated by the authors.


Pros:
+ the paper is clearly written and easy to follow
+ the dataset seems to be carefully generated and is large.

Cons:
- The authors do not clearly state their methodology, including the concrete architecture of the transformer-based model, and the training loss for optimizing the model. As shown in the experiments, the accuracy of the proposed model is much higher than the baseline, but it is not clear what the major reason is, and why.
- As the dataset contains about 50 million samples, and only 10000 of them are held out for test and validation, which means the training dataset contains sufficient data and the result could just be overfitting. Besides, the authors do not describe clearly how they generate the dataset and what is the problem distribution. For some distribution (e.g., those with smaller variance), maybe 50 million is large enough and there won't be a generalization issue. As a well-known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data. It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.

Overall this paper does not have enough technical contributions, so I vote for a clear reject.",3: Clear rejection,2,0,1,1,0,0,"1-+ the paper is clearly written and easy to follow
 + the dataset seems to be carefully generated and is large.~~~2,3-- The authors do not clearly state their methodology, including the concrete architecture of the transformer-based model, and the training loss for optimizing the model. As shown in the experiments, the accuracy of the proposed model is much higher than the baseline, but it is not clear what the major reason is, and why.
 - As the dataset contains about 50 million samples, and only 10000 of them are held out for test and validation, which means the training dataset contains sufficient data and the result could just be overfitting. Besides, the authors do not describe clearly how they generate the dataset and what is the problem distribution. For some distribution (e.g., those with smaller variance), maybe 50 million is large enough and there won't be a generalization issue. As a well-known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data. It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.",,,"1-the paper is clearly written and easy to follow~~~1-the dataset seems to be carefully generated and is large.~~~2,3-As the dataset contains about 50 million samples, and only 10000 of them are held out for test and validation, which means the training dataset contains sufficient data and the result could just be overfitting. Besides, the authors do not describe clearly how they generate the dataset and what is the problem distribution. For some distribution (e.g., those with smaller variance), maybe 50 million is large enough and there won't be a generalization issue. As a well-known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data. It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.","1-<the paper is clearly written and easy to follow>~~~2,3< It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.>",0.4,0.4,0
170KJ-xfkY,KDD.org/2023/Workshop/epiDAMIK/Paper5/-/Official_Review,"The paper proposes and implements a framework for fine-grained estimation of vaccination rates across geographical locations, vaccine holdouts, and the behavior of vaccine holdouts over time. The authors leverage a combination of search engine query data, aggregate vaccination rates, census data, and news reliability ratings (i.e., Newsguard) for their method. This is a particularly challenging problem due lags in vaccination reporting and self-reporting biases, especially among holdouts. The authors demonstrate that their vaccine intent classifier performs well and correlates with CDC vaccination rates, and conduct a fine-grained analysis of concerns among vaccine holdouts over time. 

The real-world impact and applicability of this paper is obvious to me. The authors select a very topical and compelling area (COVID-19 vaccination hesitancy) as well. Although my experience is primarily computational, the results seem grounded in vaccine policymaking objectives/priorities as well. This work further provides a template that could be potentially adapted to other policy rollouts both retrospectively (e.g., ACA rollout) and the future, provided that the requisite data sources are available.

The comparison of query data between different sources (Bing vs. Google) also addressed my biggest concern — i.e., how representative is the population studied. I also found the breakdowns of vaccine intent by demographic to be very compelling (Fig. 6c, and A5).

A few questions about the method:
* Since there are so many steps, the pipeline for generating vaccine intent labels seems susceptible to error propagation (i.e., if there is a systematic bias in the human annotators, or earlier in the pipeline) since it depends on the quality of data collected— what checks, in addition to those mentioned in the paper (some human evaluation & comparison of Google vs. Bing query data), were done for systematic biases/other pitfalls at each stage of the pipeline? 
* It is slightly unclear to me how negative vaccine intent examples were labeled. Is this based on the human annotation method in Sec. 3.2 (i.e., <3 positive annotations), followed by GNN-based label-propagation + spies? What if we label vaccine intent using a simple majority vote method (i.e., 2-1 is sufficient) at the human annotator phase? Are queries that have nothing to do with COVID-19 or vaccinations ever included as negative examples?

""¨Some further questions about the results:
* In Fig. 6a, some counties are shown in white. Is this because the sample size is too small to generate an estimate of vaccine intent?
The authors choose Newsguard as a provider of news reliability ratings; however, such ratings are inherently dependent on the rating provider's specific methodology (i.e., who decides who is more reliable in an increasingly polarized news environment). Are there alternate providers of trust ratings, and are the results robust to such changes?
* How were the URL clusters validated? How was model selection (i.e., Louvain over LDA) performed? What is the definition of a ""remarkably coherent cluster?"" While all of the results look believable, I would have liked to see some measurement of cluster quality here (although this is difficult to do objectively) in addition to the qualitative analysis. Or, is there a human-annotator based way to partially validate these clusters?
* I don't know that ""Holdouts appear like early adopters"" is the correct framing towards the end of Sec. 5 — I would expect 7d to look much flatter (vertically) if that were the case, which is true for a few of the bars, but instead I mostly notice the reversal. So it seems like the correct conclusion is that some holdouts' concerns dramatically shift w.r.t. early adopters at some point, while others converge towards early adopters' concerns. The reversal trend is probably the most interesting piece in my opinion. 

Additional breakdowns of the results that I would find interesting:
* Stratification by area deprivation index, tribal vs. non-tribal, rural vs. urban (Pop/sq. m. is a proxy), access to healthcare (e.g., # of pharmacies offering the vaccine per capita/within 1h)  

I also wanted to raise a potential ethical consideration for future work — due to the cross-platform aggregation of data required, the potential for privacy violations due to invasive behavioral interventions or discrimination should be considered in my opinion — for example, targeting specific users for misinformation, vaccine providers/pharmacies engaging in implicit adverse selection by targeting specific segments, or discriminatory labor practices based on vaccine status. One could replace the word ""vaccine"" with ""health"" for similar studies on health policy as well. 

Since this study largely consists of retrospective data analysis, the risk to users' privacy is very small at this stage. While I think the authors exercised due diligence in data ethics via IRB approval, anonymization, dissociation from specific user accounts/profiles, ZIP-level granularity, and ensuring no linkage to other products is possible, I am wondering about the potential for actors that do not exercise the same standards of diligence as the authors to harm users' privacy.  I.e., could a bad actor copy this code and engage in behavioral interventions/discriminatory practices, and what safeguards, computational, legal, or otherwise, exist to mitigate any such threats? 

Overall, I think the authors did develop a rigorous and well-motivated method for classifying vaccine intent via a multi-stage pipeline featuring regex queries, URL identification via a combination of PPR, human annotation, a GNN, and the Spy technique from PU learning. The fine-grained analysis of the model's predictions then provide insights into vaccine hesitancy rates, and how concerns of vaccine holdouts change over time. I find that this is already a well-motivated, clear, and well-written computational study of vaccination policy, and addressing the above would simply strengthen the work further in my opinion.", how representative is the population studied. I also found the breakdowns of vaccine intent by demographic to be very compelling (Fig. 6c,2,0,1,0,0,0,"1,2,3-Overall, I think the authors did develop a rigorous and well-motivated method for classifying vaccine intent via a multi-stage pipeline featuring regex queries, URL identification via a combination of PPR, human annotation, a GNN, and the Spy technique from PU learning. The fine-grained analysis of the model's predictions then provide insights into vaccine hesitancy rates, and how concerns of vaccine holdouts change over time. I find that this is already a well-motivated, clear, and well-written computational study of vaccination policy, and addressing the above would simply strengthen the work further in my opinion.",,,"[[Very thorough, appropriate encouragement and acknowledgement, and plent of specfic and constructive feedback. A clear 0, by our definitions.]]","1-<Overall, I think the authors did develop a rigorous and well-motivated method for classifying vaccine intent>",0.6,0.2,0
1WxF-dvBPw,MIDL.io/2020/Conference/Paper161/-/Official_Review,"Pros:
* The segmentation output from the DeepMedic framework was corrected with post-processing, such as connected component analysis and hole-filling.

* The median dice of the reported method show improved performance over two manual graders. The correlation of lesion volumes between the human graders and the CNN-based segmentation was statistically non-significant.

*Further, the authors have performed extensive statistical analysis to justify the significance of the method.
  
Minor comments:
* Do NCCT datasets contain 272 samples (not dataset)? Are 204 samples (not datasets) used for training?

* In section 3, No significant difference ''' ""were found"" is repeated twice.",3: Weak accept,1,1,1,0,0,0,,,,,,0.4,0.4,0
2DD8_PadF,MIDL.io/2020/Conference/Paper277/-/Official_Review,"In the proposed approach, the authors used a fully convolutional network to localize intervertebral discs in MR images. The proposed approach has a number of serious drawbacks. 
(1) The proposed approached uses 2D images as the input, that is generated by the average of the six middle slices. This limits the scope of the method, due to the assumption that all centres of intervertebral discs are located in close proximity around the middle slice. It is also not clear what the slice thickness is and thus how the image generated by averaging middle slices looks like.
(2) In the preprocessing step, the 2D image is straightened according to the spinal cord centreline. How is the centreline extracted? If the centreline has been already extracted finding the centres of intervertebral discs and vertebral bodies are greatly simplified and can be done with a variety of approaches that do not require the use of ML-based method.
(3) The term ""labelling"" of the intervertebral disc might not be correctly used in this work. The proposed network does not distinguish intervertebral disks, but only generates the centres of visible intervertebral disks in the images. The term ""localization"" should be better.
(4) There is at least one workshop each year at MICCAI that is combined with a challenge dedicated to labelling and segmentation of spine structures. Vertebral body or intervertebral disk localization and labelling is usually part of each completion. Authors should compare their methods with the SOTA method presented in these challenges. Moreover, the CSI workshop at MICCAI 2015 was dedicated to localization and segmentation of intervertebral disks from 3D T2 MRI data. "," but only generates the centres of visible intervertebral disks in the images. The term ""localization"" should be better.",1,1,2,1,1,0,,The proposed approach has a number of serious drawbacks.,,,,0.3,1,1
3F3SczScdhI,NeurIPS.cc/2022/Workshop/nCSI/Paper4/-/Official_Review,"### Summary
The paper proposes an object-centric approach to tackle the tasks in the Abstraction and Reasoning Corpus. Their approach consists of two stages, parsing the task's images to graph representations and a constraint-guided search to construct operations to solve the task.   In the experimental evaluation, the introduced approach is compared to a corresponding Kaggle Challenge's first-place solution.  ""¨


### Strength
The introduced object-centric approach is well-described and easy to understand.
The experimental evaluation is designed to analyze the object-centric tasks. To this end, a subset of the ARC was extracted and separated into different categories.


### Weaknesses
- However, I'm missing a deeper discussion on the benefit of an object-centric solution. The authors mention that ARGA is performing better than the baseline on object movement tasks but then focus on why it performs worse on the others.

The authors highlight the efficiency of their approach and state that their methods solution space may not be expressive enough. 
Could a more expressive DSL lead to decreased efficiency?

Unfortunately, the evaluation lacks appropriate cross-validation.

Furthermore, related object-centric work is missing. Especially I would be interested in a discussion of other object-centric approaches which could be used to extend the introduced approach beyond the ARC.


### Minor
Typo Line 9: demonstrate
Typo Line 141: For each node, we
Typo Line 174: To further analyze the performance of ARGA, we ","1: Insufficient workshop paper, Reject",2,1,1,1,0,0,,,,"1-The introduced object-centric approach is well-described and easy to understand.~~~2,3-However, IÃ¢â‚¬â„¢m missing a deeper discussion on the benefit of an object-centric solution. The authors mention that ARGA is performing better than the baseline on object movement tasks but then focus on why it performs worse on the others.~~~2,3-The authors highlight the efficiency of their approach and state that their methods solution space may not be expressive enough. 
 Could a more expressive DSL lead to decreased efficiency?~~~2,3-Unfortunately, the evaluation lacks appropriate cross-validation.~~~2,3-Furthermore, related object-centric work is missing. Especially I would be interested in a discussion of other object-centric approaches which could be used to extend the introduced approach beyond the ARC.~~~2,3-Typo Line 9: demonstrate~~~2,3-Typo Line 141: For each node, we~~~2,3-Typo Line 174: To further analyze the performance of ARGA, we","1-<The introduced object-centric approach is well-described and easy to understand.> ~~~ 2,3-< Especially I would be interested in a discussion of other object-centric approaches which could be used to extend the introduced approach beyond the ARC.>",0.4,0.6,1
3If4ranM5Cb,ICLR.cc/2021/Conference/Paper830/-/Official_Review,"**Summary**
The paper suggests improving the training run-time of neural ODEs when using the adjoint sensitivity method by relaxing the computational steps of $a_{\\theta}$ and $a_{t}$. 

**Significance**
There are a couple of major issues about the content and the significance of the results presented in this paper:
- While the paper is beautifully organized and easy to follow, I consider the scientific contributions of the work to be limited.
- In the CNF setting, Table 2, I do not see a major improvement in NFE. Why is this the case? 
- All neural ODEs have been tested with a single type of ODE solver. I strongly suggest the authors investigate other solvers to see how a model's performance changes when applying the proposed method? 
- Deriving a theoretical ground for the reduction in time complexity and its effect on high- and low-frequency time series modeling performance. I suspect the proposed method to speed up the training of neural ODEs would have a significant effect on the performance of time series modeling with high-frequency fluctuations. Could the authors comment on this please?

This paper is a 10 out of 10 as a blog post for the users of the adjoint method for training neural ODEs. I believe that the contribution of this paper is limited to a software hack that would benefit the community best if presented as a blog post. 

**Clarity**
An extremely clear submission. I greatly appreciate the authors' effort in preparing such a clear presentation. 

**Originality**
The paper is an original contribution. 


",5: Marginally below acceptance threshold,1,0,0,0,1,0,"While the paper is beautifully organized and easy to follow, I consider the scientific contributions of the work to be limited. ~~~ An extremely clear submission. I greatly appreciate the authors' effort in preparing such a clear presentation.",An extremely clear submission. I greatly appreciate the authors' effort in preparing such a clear presentation.,,,,0.6,0.2,0
3zjB9r17w_B,graphicsinterface.org/Graphics_Interface/2021/Conference/Second_Cycle/Paper38/-/Official_Review,"### Summary

The paper proposes careful enhancements to an imaging matching pipeline consisting of:
1) applying noise removal to get a cleaner patch dataset by taking the Phototourism dataset and removing images with low confidence and sampling points that has been track more than 5 times 
2) improvements to the network: improved feature extraction using HardNet by using a quadratic hinge triplet (QHT), and improved constraints and matching to OANet 
3) Set of experiments and ablation study on the Phototourism Image Matching challenge at CVPR 2020


### Strengths
- The proposed pipeline is able to achieve strong performance (1st place in the CVPR 2020 Image Matching Challenges Workshop Track 1) 
- The strong performance results suggests that the improvements proposed by the work (while minor), was carefully considered and would be of potential interest and value to the community

### Weaknesses

- The main weakness of the work is the extremely poor writing quality.  There is spelling, grammar, and wording issues throughout (see Comments for some examples).  The writing is sufficiently poor that it makes the specifics of the proposed method difficult to understand, and aspects of the experiments hard to interpret.  

- Due to the poor writing, some details are also not made clear.  For instance, the Phototourism benchmark has leaderboards for variations of unlimited/restricted keyboards and different sized descriptors.  It is not specified that the work is number one on the leaderboard for the unlimited keypoints and standard descriptors.  In addition, the work fails to discuss and reference the other works that appears in Table 3 (Luca et al, Jiahui et al, Ximin et al are not included in the reference).  It's unclear what works these refers to (Luca et al, should be Cavalli et al, AdaLAM: Revisiting Handcrafted Outlier Detection, 2020)

- It's not clear what the FRP95 metric is that is show in Table 2, Figure 6.  Note that several metrics described in section 4.1 (%Rep., %M.S., %mATE, %FPT) does not appear to be reported on in the paper.


### Comments

Some examples of misspellings, grammar and other wording issues.  A full editing pass is recommended.

Abstract
- ""Traditional local features"" => ""traditional local features""
- ""suffer the noise"" => ""suffer from noise""
- ""less training dataset"" => ""less training data""

Section 1: Introduction
- ""Phtotour"" => ""Phototour""

Section 2: Related work
- ""overpass"" => ""surpass""
- ""pointNet"" => ""PointNet""
- ""irrelevant transformation problem"" =>  ??
- ""we consider improving the pipeline with respect to the following aspects"" => ""we propose improving the pipeline as follows""

Section 3: Method
- Figure3: ""Ourlier"" => ""Outlier""
- ""hypermeters"" => ""hyperparameters""
- Section 3.1.2 is poorly written. I can't really understand what is meant by: ""in which 25% with the least 3d points"" or ""we sample the 3d points that have been tracked more than 5 times, and only keep the 3d points which are tracked 5 times"".  Also, what is NCC? 

- Section 3, 3.2, 3.3 should include citations to the prior work (HardNet[8], [OANet[9], and CNe [12])
- ""on the basis of CNe"" => ""on top of the CNE network [12]""
- ""with D-dimensional"" => ""with D-dimensional features""
- ""rigorous to learn"" => unclear what this means

Section 4 (Experiments)
- Table 2: ""HartNet"" => ""HardNet""
- Section 4.3: ""pret rained"" => ""pretrained""

References should be proofread
- Not all authors are included (et, al. used after 3rd author)
- Poorly formatted, with [J]. or [C]// before publication venues
",5: Marginally below acceptance threshold,1,1,2,2,2,0,,"The main weakness of the work is the extremely poor writing quality.~Due to the poor writing, some details are also not made clear.~The writing is sufficiently poor that it makes the specifics of the proposed method difficult to understand, and aspects of the experiments hard to interpret.","The main weakness of the work is the extremely poor writing quality. ~~~ The writing is sufficiently poor that it makes the specifics of the proposed method difficult to understand, and aspects of the experiments hard to interpret. ~~~ Due to the poor writing, some details are also not made clear.",The main weakness of the work is the extremely poor writing quality.,,0.3,1.4,2
4tAeSCEgvc,MIDL.io/2020/Conference/Paper41/-/Official_Review,"Although I disagree with the claim of novelty of this method, I find the paper interesting and appropriate for MIDL presentation. I think this approach has the merit of showing a practical application where the use of LSTMs to incorporate spatial consistency for volumetric problems is appropriate. This idea has been previously used for segmentation and, as far as I know, pre-trained models were not leveraged in that work. Here, the authors propose to re-use imagenet pre-trained models, trained with slice-wise labels, and enforce consistency over volumes through a bidirectional LSTM. This has a series of advantages such as reduced compute resources needed for training and the ability of using pre-trained models from 2D applications, which are very common in computer vision. 

The description of datasets and methods is pretty accurate. The method is trained on the RSNA dataset published on Kaggle and tested on the GQ500 dataset. The results are convincing and they reveal the merits of the model. The model can in fact compete with methods making use of ensemble prediction and obtain comparable if not superior results. 

Table 1 could be omitted, so that Table 2 would fit within 3 pages. 

I would like to accept this paper. My rating is between weak accept (because of the limited novelty) and strong accept. ",4: Strong accept,1,0,0,0,1,0,"Although I disagree with the claim of novelty of this method, I find the paper interesting and appropriate for MIDL presentation",,,,,0.6,0.2,0
5Q8B6epMMG,ICLR.cc/2023/Workshop/RRL/Paper24/-/Official_Review,"# Summary

This paper presents AIME, which is an imitation learning method that learns a world model from past experience, then uses that model to infer actions from an observation-only demonstration.

--- 

## Comments / Questions
-  AIME can be used to leverage a world model which is trained on data from multiple tasks
- It is not clear to me what benefit AIME has over BCO and model-based reinforcement learning approaches.  I think the authors could take some time in Section 3 to lay out the differences explicitly.
- The task used for evaluation is relatively simple, so it would be nice to see how AIME compares to other algorithms in more complex settings where it is extremely difficult to learn a world model
- Could mathematical notation in Section 3 be condensed?
","2: Marginally above acceptance threshold, paper has no serious flaws but may have limited scope or significance",2,1,1,1,1,1,,,,[[Not particularly encouraging.]],,1,1,1
6lXHQTqX-Cp,NeurIPS.cc/2022/Workshop/HITY/Paper12/-/Official_Review,"This paper introduces a simple algorithms to form trajectory ensembles from multiple checkpoints. To this, end multiple checkpoints are chosen from which the ensemble is computed. Interestingly, the most important checkpoints are not necessarily the best performing ones in terms of accuracy. Also, the gain from several independent trajectories is limited. 
The writing is reasonably clear and fits the workshop. Accept.",1: accept,2,1,0,0,1,0,,,,[[Very similar to the above (r1gDTrGIKE) in that it is way too short (fails to be specific) but is generally encouragingâ€¦or maybe it's just not discouragingâ€¦ I wouldn't think of this as an ideal review by any means.]],1-<The writing is reasonably clear and fits the workshop. >,0.4,0.4,0
a95_PE9aJG0,ICLR.cc/2021/Conference/Paper1891/-/Official_Review,"The paper is well written, and seems to solve the proposed problem well. I am very confused however about the point of the problem you are trying to solve. From what I understand you want to take a voxelized version of an MRI scan and distort it such that the observed face is no longer identifiable with the original patient, but the useful information in the MRI is preserved. Part of your method requires or extracts the brain from the MRI. In either case you claim that that is information is available and sufficient for downstream tasks, and everything can be removed. Would a superior method here not just be to take this brain information directly.  The De-Identification Quality metric which you use would obviously get the optimal 20% accuracy here as the we have no way of associating images of brains with images of faces, and even if we do, you have not demonstrated that you method would perform well on it either. I see no point in altering the face of the scans as the face information will be useless for any downstream task, and all other information is apparently preserved, so why not just take this preserved data as the privacy preserving MRI information? While this may fail to perform well under SIENAX, I would say this is a problem with using this software as a metric, as the brain would clearly be preserved. 

If I am mistaken here, I apologies, and am happy to change by rating and comments given sufficient reasoning. However, from the knowledge I posses in the field I do not think the approach has merit due to superior, trivial baselines. 

I also have an issue with the experiment for assessing privacy preservation. I would say that the mechanical turk experiment is valuable, but only if you also demonstrate that deep learning based approaches cannot identify the associate the faces as well. Without this I have no way of knowing if your method fails to a simple CNN. 

",3: Clear rejection,1,1,1,1,2,0,,,,I am very confused however about the point of the problem you are trying to solve.~~~I see no point in altering the face of the scans as the face information will be useless for any downstream task,,0.3,1,1
AmZL-Ix4lms,ICLR.cc/2023/Workshop/RRL/Paper18/-/Official_Review,"## Summary

In this work the authors present a two stage learning scheme, entitled ""PIRLNav"", on the *ObjectGoal Navigation* benchmark. This algorithm has two stages: first learning with pre-training on human demonstrations, and second with RL-finetuning. They also test various other pre-training methods along with RL-finetuning, but find that their proposed configuration, with human demonstrations, is superior.

## Review

This paper presents some interesting ideas around imitation learning and RL-finetuning. However, I feel that the paper is poorly structured, lacks depth, and ultimately, is difficult to follow. It appears this *may* be because the authors have misunderstood the workshop brief —  they seemingly restricted themselves to just 4 pages for a technical paper, which was allowed to have 8 pages. After surveying their appendix, where the bulk of their work resides, I was more impressed, and their arguments were somewhat more coherent. Based purely on the main text, however, I would have to recommend *reject.* As mentioned, the quality of the paper (in 4 pages) is quite poor and, more importantly, the authors lack any clear discussion on how their work is directly relevant to RRL.

## Overall Score: 1

Reject.

## Marking Rubric

Below I provide a breakdown of how I rate different aspects of the paper. Each category I rate out of 5.

### Relevance - 2/5

There was no direct discussion about RRL, however, the imitation learning to RL transfer can be seen as relevant to RRL. It is unfortunate that the reader must infer this link themselves. I also found the discussion on how the authors transitioned from imitation learning to RL fine tunings (including figure 6) potentially interesting for the RRL workshop, therefore it was disappointing that the section was left to the appendix.

### Novelty - 2/5

I am not convinced the work is very novel, as it primarily stitches together two extant methods.

### Significance / Importance - 2/5

I think that some of the results could be valuable to the RRL community, for example that human demonstrations were more useful than synthetic offline datasets. However, the authors lacked any kind of discussion that motivated the for the significance of their work to RRL.

### Soundness - 1/5

The soundness of the result in the paper was one of my main concerns. Most noticeably, the results reported in the tables and figures do not include any kind of uncertainty estimates.

### Scholarship - 2/5 

The authors did not cover much of the related work in the main text. Most of the related works were moved to the appendix. This is an regrettable omission.

### Clarity - 1/5 â­

The main text is very short and poorly structured. To name just a handful of concerns:

- In Table 1, the acronym ""SPL"" is never explicitly explained. What does it mean?
- Figure 1 is given on Page 1, but is only referenced on a later page — what should the reader infer from this figure?
- Moreover, I find Figure 1 itself highly confusing: what are the axis labels?; why are there two scales for the x-axis, and how are they connected? etc.
- ""PIRLNav"", as the core contribution of the paper, is only mentioned in the Abstract, Intro & Conclusion — it would be helpful to have clear, dedicated discussion on it.

The clarity of this paper is improved if you read the appendix, where they seem to have put the bulk of the text. But a reader should not be *expected* to access the appendix for cogency, I feel.

### Reproducibility - 2/5

The authors provide information on the hyper-parameters used, which is appreciated. However, they do not mention how many independent runs (if any) they used for each experiment. Another way to improve the *reproducibility* score would be to share the code used in the experiments.", on the *ObjectGoal Navigation* benchmark. This algorithm has two stages: first learning with pre-training on human demonstrations,1,2,2,2,2,1,It appears this *may* be because the authors have misunderstood the workshop brief,"However, I feel that the paper is poorly structured, lacks depth, and ultimately, is difficult to follow.","However, I feel that the paper is poorly structured, lacks depth, and ultimately, is difficult to follow. ~~~ As mentioned, the quality of the paper (in 4 pages) is quite poor and, more importantly, the authors lack any clear discussion on how their work is directly relevant to RRL. ~~~ The main text is very short and poorly structured. ~~~ I am not convinced the work is very novel, as it primarily stitches together two extant methods. ~~~ I also found the discussion on how the authors transitioned from imitation learning to RL fine tunings (including figure 6) potentially interesting for the RRL workshop, therefore it was disappointing that the section was left to the appendix. ~~~ This is an regrettable omission.","the paper is poorly structured, lacks depth, and ultimately, is difficult to follow. It appears this *may* be because the authors have misunderstood the workshop brief~~~ As mentioned, the quality of the paper (in 4 pages) is quite poor and, more importantly, the authors lack any clear discussion on how their work is directly relevant to RRL.~~~However, the authors lacked any kind of discussion that motivated the for the significance of their work to RRL.~~~The main text is very short and poorly structured. To name just a handful of concerns:
 

 - In Table 1, the acronym â€œSPLâ€ is never explicitly explained. What does it mean?
 - Figure 1 is given on Page 1, but is only referenced on a later page â€” what should the reader infer from this figure?
 - Moreover, I find Figure 1 itself highly confusing: what are the axis labels?; why are there two scales for the x-axis, and how are they connected? etc.~~~a reader should not be *expected* to access the appendix for cogency",,0.6,1.8,2
aNeCIM5eszA,ICLR.cc/2021/Conference/Paper3351/-/Official_Review,"
1.	In the introduction, the author separately pointed out the issues of DUQ and DKL. However, these issues are not convincing as no citations or theoretical proof is provided in this paper. The notations in the intro are also not well-defined. X, x, x* are used without difference, which however should be clearly defined as vectors or matrices.
2.	The technical contribution is very incremental. The proposed vDUQ is simply applying the inducing point GP in the DUQ to mitigate the uncertainty collapse in DKL. The ''inducing point variational approximation of the GP predictive distribution'' referred as inducing point GP is not clear for me. What exactly does 'inducing point GP' refer to? Why the so-called inducing point GP can speed up inference in GP model? What does ''decouple it from dataset size'' mean? All these important points are not clarified in the introduction.
3.	The theoretical contributions are also not well-organized. The author fails to prove that the spectral normalization as a regularization scheme can be uses to mitigate uncertainty collapse. Moreover, how the spectral normalization guarantees the effectiveness of inducing point GP in vDUQ?
4.	I also have some concerns on the experimental results of causal inference. Why the treatment effect estimation has uncertainty settings. The authors should fully explain the uncertainty settings in causal inference, as most of the causal baselines are not proposed for uncertainty settings.
",,2,1,2,1,1,1,,"3,5-However, these issues are not convincing as no citations or theoretical proof is provided in this paper.~~~2-The theoretical contributions are also not well-organized.",,"[[Not inappropriately emotionalâ€¦but not remotely encouragingâ€¦ Still, rather blunt. I don't think it quite fits under our toxicity guidelines, but it's maybe borderlineâ€¦]]",,0.6,1.2,1
ATeEg9cbgcH,NeurIPS.cc/2022/Workshop/HITY/Paper23/-/Official_Review,"This paper proposes to an efficient method for optimizing non-decomposable metrics (those which depend on multiple training points) by optimizing the NN feature extractor (the first $L-1$ layers) and the last layer separately, and assuming that the non-decomposable metric is a function of only the last layer.

I think the method is interesting and can be useful as an alternative to prior (more expensive) methods.",1: accept,1,1,0,0,1,0,,,,,,0.4,0.4,0
B1BsWE9lM,ICLR.cc/2018/Conference/-/Paper8/Official_Review,"
In this paper, the authors propose doubly stochastic adversarial autoencoder, which is essentially applying the doubly stochastic gradient for the variational form of maximum mean discrepancy. 

The most severe issue is lacking novelty. It is a straightforward combination of existing work, therefore, the contribution of this work is rare. 

Moreover, some of the claims in the paper are not appropriate. For example, using random features to approximate the kernel function does not bring extra stochasticity. The random features are fixed once sampled from the base measure of the corresponding kernel. Basically, you can view the random feature approximation as a linear combination of fixed nonlinear basis which are sampled from some distribution. 

Finally, the experiments are promising. However, to be more convincing, more benchmarks, e.g., cifar10/100 and CelebA, are needed. ",3: Clear rejection,2,1,2,1,0,1,,"2,3,5-The most severe issue is lacking novelty. It is a straightforward combination of existing work, therefore, the contribution of this work is rare.",,"2,3-Moreover, some of the claims in the paper are not appropriate. For example, using random features to approximate the kernel function does not bring extra stochasticity. The random features are fixed once sampled from the base measure of the corresponding kernel. Basically, you can view the random feature approximation as a linear combination of fixed nonlinear basis which are sampled from some distribution.~~~1,2,3-Finally, the experiments are promising. However, to be more convincing, more benchmarks, e.g., cifar10/100 and CelebA, are needed.[[This is another interesting instance where a review does not have an overall positive vibe, but still seems to meet our positivity criteria]]",,0.3,1,1
B1eJCEuUYV,ICLR.cc/2019/Workshop/LLD/-/Paper20/Official_Review,"The authors here present an interesting analysis on how skip-connections in residual networks and batch-normalization affect data separation. Their analysis included observing the transformation of the input vectors through hidden layers of the neural networks, like a standard multilayer perceptron, a resnet and a resnet with batch normalization. They did that by studying the angle and cosine similarity through the layers. This property is critical to decide if the model is able to separate points in different classes.

The paper is well written and easy to read. The settings and configurations are carefully explained and carry the detail needed to understand the analysis. The study of angles and cosine similarities between the layers is very interesting, although its relation to the data separation property (section 3.3) could be written in a more clear way. 

Pros:
- connecting the dynamics of angles with data separation
- extensive analysis

Cons: 
- more settings could be explored

Some questions that the authors could address as well: why are the specific resnet models selected by the authors (Yang, Hardt)? what are the effects of using different initialization methods for the internal weights? how much important is the specific initialization chosen by the authors? what happens when the number of training examples increases or decreases? ","4: Top 50% of accepted papers, clear accept",1,0,0,0,0,0,The paper is well written and easy to read. ~~~ The settings and configurations are carefully explained and carry the detail needed to understand the analysis.,The paper is well written and easy to read.,,,,1,0,0
BJzWfyTlf,ICLR.cc/2018/Conference/-/Paper1137/Official_Review,"Summary: The authors use RL to learn a visual arithmetic task, and are able to do this with a relatively small number of examples, presumably not including the number of examples that were used to pre-train the classifiers that pre-process the images. This appears to be a very straightforward application of existing techniques and networks.

Quality: Given the task that the authors are trying to solve, the approach seems reasonable.
Clarity: The paper appears quite clearly written for the most part.
Originality & Significance: Unless I am missing something important, or misunderstanding something, I do not really understand what is significant about this work, and I don't see it as having originality.

Nitpick 1: top of Page 5, it says ""Figure ??"" 
Nitpick 2: Section 2.3 says ""M means take the product, A means take the sum, etc"". Why choose exactly those terms that obscure the pattern, and then write ""etc""? In Figure 1, ""X"" could mean multiply, or take the maximum, but by elimination, it means take the maximum. It would have only added a few characters to the paper to specify the notation here, e.g. ""Addition(A), Max (X), Min (N), Multiply (M)"". If the authors insist on making the reader figure this out by deduction, I recommend they just write ""We leave the symbols-operation mapping as a small puzzle for the reader.""

The authors might find the paper ""Visual Learning of Arithmetic Operations"" by  Yedid Goshen and Shmuel Peleg to also be somewhat relevant, although it's different from what they are doing.

Section 3. The story from the figures seems to be that the authors' system works beats a CNN when there are very few examples. But significance of this is not really discussed in any depth other than being mentioned in corresponding places in the text, i.e. it's not really the focal story of the text. 

Pros: Seems to work OK. Seems like a reasonable application of pre-trained nets to allow solving a different visual problem for which less data might be available.

Cons: Unless I am missing an important point, the results are unsurprising, and I am not clear what is novel or significant about them.", although it's different from what they are doing.,1,1,1,0,1,0,"Unless I am missing an important point, the results are unsurprising, and I am not clear what is novel or significant about them",,,,,0.4,0.6,1
BkgF8SV53X,ICLR.cc/2019/Conference/-/Paper571/Official_Review,"This paper proposes learning a transition model that takes an action sequence as an input (instead of a single action), and performing model-based planning by using the cross-entropy method.

One obvious concern with this is that this produces a sequence of open-loop plans, rather than a closed-loop policies, with all the inherent limitations. I could see this working well in practice in problems where anticipating how future decisions will react to state changes is not that important, however the authors should discuss the trade-offs more.

A larger concern for me revolves around learning the transition model. Taking the action sequence as an input (which is one of the main novelties in the paper) is likely to require a lot of data, and maybe this is fine on relatively simple Mujoco tasks but I see it as a potential issue when trying to expand this to more realistic problems.

Finally, I suggest that the authors change the title to something more descriptive of the paper's contents, as there is no analysis of asymptotic performance in the paper (as I would have thought from the title). I also recommend that they look to see if there is any model-based work in the semi-MDP literature, which could be relevant here.
",4: Ok but not good enough - rejection,1,1,1,1,1,0,,,,,,0.6,0.8,1
BkggwRbF3E,icaps-conference.org/ICAPS/2019/Workshop/XAIP/-/Paper1/Official_Review,"A main idea of the paper, that a contrastive explanation should be answered 'universally' rather than 'existentially' is extremely interesting and novel. 

The capability of this approach to explain how different goal subsets exclude each other is powerful because in any reasonably complex domain, this is could be unknown to the user. Such an understanding could also be helpful for other goal reasoning components if the planner is part of a larger cognitive system. 

The evaluation is quite strong, and the experimental setup is described in detail. For Figure 2, it seems that the only analysis of the results is the last sentence before the conclusion, that ""Smaller numbers of goal facts tend to be quite easy, larger ones mostly infeasible, with variance depending on the domain and algorithm."" Is this the only interesting analysis of this figure? Even though space is tight, I would have appreciated more analysis here.

The paper is highly relevant to the workshop.

minor comments:
	* Section numbers throughout the document are missing

","5: Top 15% of accepted papers, strong accept",1,0,1,0,0,0,,,,,,0.6,0.2,0
BN2eZyiS2Wq,ICLR.cc/2022/Workshop/OSC/Paper17/-/Official_Review,"Summary: The paper proposes a way to bind action inputs with object vectors in object-centric dynamics models. While previous models simply broadcast the action input to all the object vectors -- thus all objects receive the action input equally, the proposed approach seeks to use an attention mechanism to decide which object vector should receive the action input.

Pros: The paper highlights the importance of finding the right architecture to make the action input interact with the state representations in dynamics models. The performance improvement reported with and without the attention mechanism is significant.

Cons: There seems to be a dilemma between soft and hard attention -- the soft attention is more expressive, however,  it also under-performs hard attention in some experiments. 

Summary: I vote in favor of accept.","2: Good workshop paper, accept",2,1,0,0,1,0,,,,[[I feel like it's not affirmatively positiveâ€¦just encouraging at bestâ€¦]],1-<he paper highlights the importance of finding the right architecture to make the action input interact with the state representations in dynamics models>~~~2-<There seems to be a dilemma between soft and hard attention>,0.4,0.4,0
BOWgkwSLw-q,ICLR.cc/2022/Workshop/OSC/Paper22/-/Official_Review,"This paper proposes to use an alignment network output (from Rocco 2018) and MSE to quantify changes between two different images such that they ""explain"" the difference between the images via a ""pose"" change, a ""shape"" change and an ""appearance"" change. By using the outputs of the alignment network the authors propose a way to quantify pose changes (by magnitude of affine transform between images), shape changes (by magnitude of TPS deformation between images) and appearance changes (by a pixel wise L2 error).

My main issue, and the authors themselves admit it, is that this is a very simplistic way of explaining differences between images - it completely ignores the 3D aspect of natural images, and, even more crucially, the appearance model used is really too simple - other pre-trained features (maybe a self-supervised representation trained on ImageNet or something along these lines) could have been much more informative. 

I like the core idea in this work and presenting at the workshop to get more feedback is probably a good idea, but I would encourage the authors to enrich and make the model a bit more sophisticated. The technology exists and would benefit this nice core idea.","2: Good workshop paper, accept",1,1,1,0,1,0,,,,,,0.4,0.6,1
BPbzCeCc4L,NeurIPS.cc/2022/Workshop/HITY/Paper28/-/Official_Review,"This work proposes a new method for speeding up neural network training by importance sampling. The paper is clearly written, and the empirical results demonstrate superiority of the method over relevant baselines.",1: accept,2,0,0,0,0,0,"1-The paper is clearly written, and the empirical results demonstrate superiority of the method over relevant baselines.",,,[[The reviewer must've done this at 11:59 but it has a strong positive vibe for me.]],"1-< The paper is clearly written, and the empirical results demonstrate superiority of the method over relevant baselines.>",1,0,0
BRzgpZTzAbc,ICLR.cc/2022/Workshop/OSC/Paper12/-/Official_Review,"This paper proposes a framework to tackle the MI EEG classification task and mainly focus on the challenge of subject distribution shift. Specifically, the core module of the framework is the concept of dynamic convolutions which contain parallel trainable convolutional kernels to increase model's complexity without increasing the network's depth or width. 

However, it seems that the authours just apply this already proposed method to the task of MI EEG classification and the whole method lacks novelty. Besides,  causal reasoning is not well descrbed in the paper except Eq.(3) which is not enough for self-contained and Z is not stated in your pipeline (Fig. 2)","2: Good workshop paper, accept",2,2,2,2,2,2,"1,5-However, it seems that the authours just apply this already proposed method to the task of MI EEG classification and the whole method lacks novelty. Besides, causal reasoning is not well descrbed in the paper except Eq.(3) which is not enough for self-contained and Z is not stated in your pipeline (Fig. 2)","2-However, it seems that the authours just apply this already proposed method to the task of MI EEG classification and the whole method lacks novelty.","2- However, it seems that the authours just apply this already proposed method to the task of MI EEG classification and the whole method lacks novelty. Besides, causal reasoning is not well descrbed in the paper except Eq.(3) which is not enough for self-contained and Z is not stated in your pipeline (Fig. 2)",5-the whole method lacks novelty,5-<the whole method lacks novelty>,1,2,2
bS5LWCHwRY,MIDL.io/2020/Conference/Paper23/-/Official_Review,"This paper presents a multi-label classification framework based on deep convolutional neural networks (CNNs) for diagnosing the presence of 14 common thoracic diseases and observations in X-rays images. The novelty of the proposed framework is to take the label structure into account and to learn label dependencies, based on the idea of conditional learning in (Chen et al., 2019) and the lung disease hierarchy of the CheXpert dataset (Irvin and al., 2019). The method is then shown to significantly outperform the state-of-the-art methods of (Irvin and al., 2019; Allaouzi and Ahmed, 2019).

The paper reads well and the methodology seems to be interesting. I only regret the fact that this is a short paper, and there is therefore not enough space for a more formal description and discussion of the methodology.",3: Weak accept,1,1,1,0,0,0,,,,,,0.4,0.4,0
BylxXI3M9N,ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper33/Official_Review,"Positives:
- The results are straightforward, and compare to the standard baselines (SIF, infersent etc). I think these are not SoTA numbers but the relative comparisons seem fine, especially for a workshop paper. The addition of supplementary ablation experiments is also much appreciated.
- The approach is fairly clear, and there's no real big holes in terms of how the problem is set up and solved. 

Conceptual:
- I'm a bit unhappy with how the entailment experiments were set up. The narrative up to the last paragraph (""For this purpose, .."") is fairly clear that one wants to find distributions that are 'contained in the support' of another. You could directly do this on the PPMI matrix you have and define a measure of 'containment' in terms of distributions. Instead of this, you end up plugging in a (heurisitic) ground metric and computing embeddings. This seems haphazard, and I'm not terribly convinced this makes sense. Are you doing consistency checks like making sure you dont have negative-cost-cycles?

Clarity:
- Throughout the paper, there are all sorts of minor unsupported side-remarks and claims that should be stripped. The paper has a fairly straightforward main story and positive results; the addition of these remarks just detract from the rest of the paper. The authors should ask themselves - would I be willing to write a supplemental proof to make this statement precise? or is it just a side-remark I can remove. 
- The explanation surrounding equation 4 is fairly confusing. I guess what's happening is that you're taking each context, embedding it to some metric space, and weighting it by its bin count. This is quite the sudden leap, since until this point it wasn't really clear that you were going to embed the contexts with a base embedding. Is this an approximation to the actual thing you want to compute (OT over contexts?) or a way to induce the distances? I think its a little bit of both, but the explanation here needs to be more carefully thought out and laid out. I think it would help to make the inputs and outputs to your method precise. 
- 'We also consider adding the information from point estimate into the distributional estimate to get best of both the worlds.' - you should motivate why you want to do this ('best of both worlds' is extremely unclear.. what problem are you solving exactly?). You should then precisely state what you're doing. Your setup Eq (4) is already a bit confusing, so you need to be a bit careful when building on it. I guess the point estimate being added here is the original Glove embedding?
- 'Since the contexts are dense embeddins' - you really need to explain this. A context is a context, not an embedding - I think the more precise statement is that a context can be mapped to a dense embedding. I assume it's something like, you start with point estimates (i.e. glove vectors) of contexts, so you can treat each context as being assigned an associated vector. You then cluster this, and sum over the clusters. I'm also not sure if summing normalized SPPMI values makes sense as an object. Shouldn't you merge the contexts and then compute the SPPMI again over the 'combined' context? Either way, this part can be made much more clear.
- I'm not sure why barycenters is obviously better.. if you have polysemy, you'd want to select the meaning that's implied by all the words, and reject any others. The barycenter does not do this, because you're still incurring costs from the word sense that's not being used in the sentence. I guess it's better than the alternatives?
- In the paragraph connecting SIF and CoMB - i have no idea what the precise connection is. You should write down propositions and equations for any precise statements like this one (or remove it).

Minor comment:
- 'Also, KL-divergence isn't defined when the supports of distributions under comparison don't fully overlap' - is false, you only need absolute continuity, meaning you don't need full overlap - just that the support of the distribution inside the log must be a subset of the support of the distribution outside the log. 
- 'Hence, one potential application could be in checking for the implicit bias in point estimates (Bolukbasi et al., 2016) and then correcting it via the ground cost.' - this is a pretty vacuous statement - you could have compared pairwise distances for word embeddings to correct it, for example. I honestly think the throwaway comments like this one and the one above should just be stripped from the paper. 
- The section 'Relation between the histogram and point estimates.' is similarly vacuous. Yes, count based methods use histograms and neural networks use vectors. Yes, your paper kind of uses both. I really do not think pointing this out adds much insight to your paper. It may be that you had something more profound to say, but it doesn't quite come though.
- 'A practical take-home message of this work thus is to not throw away the co-occurrence information e.g. when using GloVe, but to instead pass it on to our method.' - should be moved to the discussion. 
- You may also want to put the timings in the experiment - runtimes are somewhat useless without matching accuracy numbers for approximate algorithms such as sinkhorn. What's the relative (percent) error on your wasserstein distance estimates?
- ' In fact, Figure 3 says it all,' - in fact, figure 3 does not say it all because it's a particular example projected into 2d without very much explanation. In fact i'd argue that it's not a terribly enlightening - what is the ground truth supposed to look like? why is euclidean averaging bad? 
",3: Marginally above acceptance threshold,2,1,2,1,2,1,,"3-This seems haphazard, and I'm not terribly convinced this makes sense.",,3-this is a pretty vacuous statement,"1-<The results are straightforward, and compare to the standard baselines (SIF, infersent etc). I think these are not SoTA numbers but the relative comparisons seem fine, especially for a workshop paper. The addition of supplementary ablation experiments is also much appreciated.> ~~~ 2,3-< I think the more precise statement is that a context can be mapped to a dense embedding. I assume it's something like, you start with point estimates (i.e. glove vectors) of contexts, so you can treat each context as being assigned an associated vector>",0.4,1.4,1
ByxqW0LIK4,ICLR.cc/2019/Workshop/drlStructPred/-/Paper5/Official_Review,This paper is well written and describes their goals and reasons for their decision making clearly. They clearly state their objectives and use of variational inference in as a reinforcement learning problem to improve trajectories. They provide comparisons to Random Monte Carlo methods as well as ISSoft demonstrating improvements of MC methods and in some instances similar performance to ISSoft. The pros are their clear evidence of being able to use variational inference as a means of matching hand crafted baselines as well as a clear reinforcement learning objective to improve the quality of trajectories. The cons are that their proposed method can learn proposals only for a simple stochastic process and do note that they recieve poor results on rare instances from their X distribution as well as their approach not being able to improve the quality  of pre-trained proposals.,3: Marginally above acceptance threshold,2,0,0,0,0,0,1-This paper is well written and describes their goals and reasons for their decision making clearly. They clearly state their objectives and use of variational inference in as a reinforcement learning problem to improve trajectories.,,,"1,2-This paper is well written and describes their goals and reasons for their decision making clearly. They clearly state their objectives and use of variational inference in as a reinforcement learning problem to improve trajectories.~~~1,2-The pros are their clear evidence of being able to use variational inference as a means of matching hand crafted baselines as well as a clear reinforcement learning objective to improve the quality of trajectories.",1-<The pros are their clear evidence of being able to use variational inference as a means of matching hand crafted baselines as well as a clear reinforcement learning objective to improve the quality of trajectories>~~~1<This paper is well written and describes their goals and reasons for their decision making clearly>,1,0,0
ceAr32aMFm,MIDL.io/2020/Conference/Paper94/-/Official_Review,"- Quality:
Interesting problem of predicting malignancy from longitudinal scans. 
High-quality cohort of images, though small-sized. 
Propose a ""two-stream 3D convolutional neural network (TS-3DCNN)"". Authors call it ""sibling networks"" rather than ""siamese"" architectures while sounding very similar.
Strong results on F-score improvement: ""This model outperformed by 9% and 12% the F1-score obtained by the best models using single time-point datasets"".
A confusion matrix would have been really informative, along with discussion on cases where prediction failed. 


- Clarity:
""pair of patches of 32x32x32, cropped around the center of the annotated nodules at both time-points"" & ""the radiologists detected and matched the most relevant nodule and annotated its malignancy"": while this is a quite impressive cohort and annotation work, sampling a single nodule per scan remains limited. Any lower threshold on size of nodule, as in [1]?

""The nodules were labelled as malignant if they had a positive cancer biopsy, and benign if they did not have a significant change in their structure, density or morphology during 2 years or more."": these are not exclusive conditions. What about a significant change and surgery but no confirmation of cancer via biopsy? Or vice versa, no change over 2 years but later on cancer?


- Originality:
This paper is very similar to [1] (some figures are identical) while the final task differs (matching distance versus nodule malignancy).  

[1] Rafael-Palou X, Aubanell A, Bonavita I, Ceresa M, Piella G, Ribas V, Ballester MÃ. Re-Identification and Growth Detection of Pulmonary Nodules without Image Registration Using 3D Siamese Neural Networks. arXiv preprint arXiv:1912.10525. 2019 Dec 22.)   ",3: Weak accept,1,1,1,1,1,0,,,This paper is very similar to [1] (some figures are identical) while the final task differs (matching distance versus nodule malignancy).,,,0.6,0.8,1
cFbPW_fKraQ,ICLR.cc/2021/Conference/Paper2215/-/Official_Review,"Summary: this paper claims to show that the ""mathematical formulation"" of SDEs is ""directly comparable"" with the formulation of GANS. 

I found this paper to be poorly premised. At the outset, the authors state ""An SDE is a map from a noise distribution...to the solution of the SDE which is some other distribution on path space."" This statement is incorrect. First of all an SDE is not a map on measure space. It defines the evolution of sample paths of stochastic processes that induce measures. This suggests to me that the authors conflate the measures on path space with paths themselves. I'm also confused by the analogy between sampling SDEs and GANs — one might as well draw analogies with sampling Gaussian distributions. This is entirely confusing.

There are further fundamental issues that crop up throughout the paper. For instance, in Section 2.2, the authors state that $Y\\stackrel{d}{\\approx} Z$; but what do they mean by this? That  the finite dimensional distributions are approximately equal?

It appears that the point of the paper is that Wasserstein GANs can be applied to path measures induced by SDEs. This is not a novel insight, in my opinion.",,2,2,2,2,2,2,"3-I found this paper to be poorly premised.~~~1,3-This statement is incorrect. First of all an SDE is not a map on measure space. It defines the evolution of sample paths of stochastic processes that induce measures. This suggests to me that the authors conflate the measures on path space with paths themselves.~~~3-IÃ¢â‚¬â„¢m also confused by the analogy between sampling SDEs and GANs Ã¢â‚¬â€ one might as well draw analogies with sampling Gaussian distributions. This is entirely confusing.","1-This is not a novel insight, in my opinion.","1,3-At the outset, the authors state Ã¢â‚¬Å“An SDE is a map from a noise distribution...to the solution of the SDE which is some other distribution on path space.Ã¢â‚¬Â This statement is incorrect. First of all an SDE is not a map on measure space. It defines the evolution of sample paths of stochastic processes that induce measures. This suggests to me that the authors conflate the measures on path space with paths themselves.~~~3-This is entirely confusing. ",5-This is entirely confusing.,5-<This is entirely confusing.>,1,2,2
CvENlRiR5Ve,ICLR.cc/2021/Conference/Paper2915/-/Official_Review,"This paper proposes to use a counterattack strategy to attack an input x, and calculate the distance between x and the crafted example x' as the detection metric. There are two main concerns about this paper:

1. The authors claim that their detection method is attack-agnostic, but their motivation in Fig 1 highly depends on the assumption of the attacking mechanism. There is no guarantee on the effectiveness of AttackDist when the attacks do not follow assumed patterns.

2. In experiments, there are only oblivious attacks, where the adversaries do not know the mechanism of AttackDist. For a defense method, it is necessary to carefully design a convinced adaptive attack and demonstrate the effectiveness of the defense under the adaptive attack.

Minors:
In the introduction section, the authors claim that ""Existing adversarial defense techniques could be classified into two main categories: adversarial training and detection"". Actually, there are many other types of defenses including input processing, robust architecture, random smoothing, certified defenses, etc. Besides, existing adversarial training methods like FastAT can easily scale to ImageNet, running for several hours on a single GPU. The authors should be more updated on these related progresses.",3: Clear rejection,1,1,2,1,1,0,,"There is no guarantee on the effectiveness of AttackDist when the attacks do not follow assumed patterns.~In experiments, there are only oblivious attacks, where the adversaries do not know the mechanism of AttackDist.","The authors claim that their detection method is attack-agnostic, but their motivation in Fig 1 highly depends on the assumption of the attacking mechanism.",,,0.3,1,1
dAbTmyq2PD,NeurIPS.cc/2022/Workshop/HITY/Paper7/-/Official_Review,"The paper proposes to combine sharpness-aware minimization with self-distillation to speed up convergence and improve test performance. Since the empirical results indicate a potential improvement in performance, I recommend to accept the paper.

Some simple suggestions to improve the paper:
- Provide results averaged over multiple random seeds.
- Improve the readability of the figures and the table by increasing the font size.
- Improve the clarity of the method section (Section 3).
",1: accept,1,1,1,0,0,0,,,,,,0.4,0.4,0
db9k5NULiNC,MIDL.io/2020/Conference/Paper188/-/Official_Review,"The authors describe a new end to end image analysis pipeline they developed to interpret histo-pathological images of breast and gastric cancers. Specifically, a deep convolutional neural network (CNN) first makes automatic the analysis of fluorescence in situ hybridization (FISH) images that test the Human Epidermal growth factor Receptor 2 (HER2) oncogene amplification status.  The deep learning pipeline mimics the pathological assessment, and localizes plus classifies the fluorescence signals within each nucleus. Then, it classifies the whole image regarding its HER2 amplification status. 
This short paper gives a good overview of the pipeline and reads well. The methodology seems to be solid and very flexible. The results are also promising although the proposed pipeline is not compared with another state-of-the-art method. The source code of the pipeline is finally freely available. It therefore believe that this work will be of significant interest at MIDL. 
",3: Weak accept,2,0,0,0,0,0,,,,"1,2-This short paper gives a good overview of the pipeline and reads well. The methodology seems to be solid and very flexible. The results are also promising although the proposed pipeline is not compared with another state-of-the-art method. The source code of the pipeline is finally freely available. [[Pos. guideline 3 could be met if the reviewer phrased their critique a bit more specifically. As it is, areas of improvement are just heavily implied; not specific or actionable.]]",1-<This short paper gives a good overview of the pipeline and reads well. The methodology seems to be solid and very flexible. >,1,0,0
eSHGR9YWfi,NeurIPS.cc/2022/Workshop/nCSI/Paper13/-/Official_Review,"**Summary** 

This paper proposes to use the abstract reward machine (RM) as an additional context to learn transferable RL policies. The proposed approach learns a graph embedding of the RM, called RM-GECO, and concatenates the node embedding of the current state to the environment observation before learning the policy. The paper hypothesizes that the RM-GECO provides additional information about the context that helps in generalizing the RL policies to different tasks. The preliminary experiments aim to evaluate this hypothesis on grid navigation tasks.


**Strengths**
* The proposed idea of using RM as context is quite novel. 
* The proposed approach is relevant for the nCSI workshop as RM may encode dynamic causal information of the domain.


**Weaknesses** 
* The paper proposes the use of RM-GECO to learn policies and compares the proposed approach to learning without any context. While the selected baseline is important, it is not sufficient. To justify the use of RM-GECO, a baseline policy that uses some other information as context is essential. Perhaps something like policy sketches [1], or the use of RM without embedding. 
* It is not clear how the preliminary experiments demonstrate the benefit of their approach. How is DQN+GECO better than DQN in Figure 3? 
* The paper needs more evaluation and analysis of when would the RM-GECO help? 

**Questions**
* Figure 3 shows evaluations on a 4x4 grid using RM with 6 and 18 states. How do these curves look when the number of RM states is 3 (like in Figure 1c)? A more thorough discussion of the required resolution of the RM would help.


","2: Good workshop paper, Accept",1,1,1,0,1,0,,,,The paper proposes the use of RM-GECO to learn policies and compares the proposed approach to learning without any context.,,0.4,0.6,1
F8k2r_jshnG,KDD.org/2023/Workshop/epiDAMIK/Paper5/-/Official_Review,"The authors study the problem of detecting vaccine intent from Bing search query log data. Briefly (as I understand their method) their goal is to take a query + click graph and label it with whether it represents vaccine intent or not and then use the results of this classification to estimate the number of vaccines that will be administered in a particular zip code tabulation area. To do so, the authors use Mechanical Turk to label an initial set of query-URL click pairs and then apply semi-supervised learning techniques to grow this set of labels. Pretraining in the form of initializing the model to minimize an auxiliary loss is applied to states with less data. The resulting classifier is evaluated to be highly effective at detecting vaccine intent. Then, a bias correction is performed to go from Bing user counts to population counts, as the usage of Bing is not uniform across states. The estimates the authors develop are highly correlated with CDC-reported vaccine counts, but more granular and do not have a reporting delay.

The paper is of high quality, generally clear, makes methodological innovations, and likely to be of wide interest.

Minor comments:
- Section 3 para 1—fairly important to include the precise criteria for inclusion (at least in Appendix)
- Giving some overview of the challenge of detecting intent from queries would be helpful for those who have not worked with this kind of data before. For example, in 3.1, the phrase ""covid vaccine New York"" is mentioned as suggestive but not unambiguous enough. But it is not clear what is missing from this. Is it that the location named is not specific enough? Or is covid vaccine + location always too ambiguous?
- How were URLs presented to the annotators? Did they see just the URL or did they see the page it led to?

Things that came to mind:
- Accuracy of intent classification across time—I believe this is not reported anywhere. This is a pretty important question given the Google Flu Trends experience.
- Connect vaccine intent queries to queries about symptoms, e.g., does experiencing symptoms motivate people to seek vaccine information?",,1,1,0,0,1,0,,"The paper is of high quality, generally clear, makes methodological innovations, and likely to be of wide interest.",,,,0.4,0.4,0
FALrJLKIt,MIDL.io/2020/Conference/Paper5/-/Official_Review,The coil sensitive functions are modeled by linear combinations of spherical function basis. The model coefficients were estimated using an ADMM algorithm. But the underlying motivation is not justified. The notation $R_u$ was not explained. T The provided images are too small and low-resolution to show any useful details. ,3: Weak accept,1,2,1,2,1,1,,,The notation $R_u$ was not explained. ~~~ T The provided images are too small and low-resolution to show any useful details.,The notation $R_u$ was not explained. T The provided images are too small and low-resolution to show any useful details.,,0.4,1.4,1
Fky6K_Wk12d,ICLR.cc/2023/Workshop/RRL/Paper49/-/Official_Review,"Overall, this is a strong workshop paper on an interesting subject. I think it could be a good full conference paper submission in the future and I think the authors could improve some things for that so I've written more detailed comments.

# paper summary

## overview

- finetuning pretrained RL models isn't as performant as NLP, Vision
    - propose that the issue is [catastrophic] forgetting
- create a toy robot task to demonstrate forgetting in finetuning
- show that continual learning methods (rehearsal, regularization) work here as well

## method and experiments summary

regularization (L2 and EWC)
- L2 loss between pretrained and finetuned weights
- either a standard sum (L2), or EWC weighted

replay (behaviour cloning)
- gather buffer B with pretrained model
- minimize KL on action distribution between pretrained and finetuned models
- *note* mitigating forgetting in critic isn't necessary

first experiment is a toy task, 1D grid env with two tasks in sequence
- model pretrained on task 2 will forget it if task 1 is long enough

second experiment is a robotics task, a sequence of 4 smaller tasks from meta-world, 2 easy then 2 hard
- model pretrained on last two tasks (hard), and must learn the first two without fully forgetting its pretraining
- EWC and BC both help the model retain its pretraining skills and outperform naive finetuning and from-scratch training baselines, L2 is worse somehow

# review

The paper is novel in the connections it makes and I believe it makes a strong and bold claim that these continual learning (CL) problems are what is holding back reincarnated RL. The clarity could use some work as a couple experiments and conclusions are unclear, but the overall significance of this makes it a strong workshop paper. With some improvements, I can see this being a strong conference paper in the future.

What this paper does well is try to bring together the CL and RRL fields. I think a longer related work with equivalents between the two would help. The main robotics experiment does a good job of demonstrating skill forgetting in a practical way, and though it is toy-ish it is hard to dispute the main argument. The CL methods are simple but arguably even more convincing because they are simple and effective. 

The main improvements would be to generally be clearer with what each experiment is testing and more strongly justify the conclusions drawn from each one. There are also some easy improvements to make the experiments more convincing that I suggest below. Finally, the paper is a bit light on overall results, which is fine for a workshop paper, but should definitely be improved for a full conference submission. I think just making the connection between CL and reincarnated RL is interesting but the thing that will sell the paper is justified, actionable, takeaways. The toy experiment feels a bit obvious and I would dedicate less time to them. The robotics experiments are great and do a better job of demonstrating the same points in a ""toy-ish"" way since the setup is a bit unrealistic (knowingly pretraining on the last two tasks). My personal view is that the paper needs to lean into either demonstrating this issue for a real reincarnated setup (e.g. Agarwal et al already showed BC works for Atari, demonstrating the EWC works too would be a strong contribution) or creating a benchmark CL+reincarnated task (such as the robotics task) and release models / eval protocols to allow future work to test which CL methods are beneficial for reincarnated RL. 

Below are more specific comments for improvement.  


## 3

## 3.1
confusion formulation for compositional MDP
- initially 
    - $\\tilde M$ is a subset of $M$ 
    - $\\pi_\\theta$ is pretrained on $\\tilde M$ 
    - the goal is to solve $M$
    - this implies that $\\tilde M$ is the easier initial distribution, $M$ is the distribution you want to transfer to
- later
    - getting to $\\tilde S$ is not trivial
    - $\\pi_\\theta$ doesn't manage to reach states $\\tilde S$ despite being pretrained on them
    - 

I am not sure that this is necessarily ``compositionality'' since you are not recomposing a set of tasks/skills you learned but rather extending your set of tasks while remembering previous ones. Compositionality in RL is more in line with the recently proposed Zipfian RL than this. I would recommend starting from the idea of temporal splits in your MDP and defining your setup around that with clear language that relates to your toy task setup e.g.
    - MDP $M$ is made up of multiple tasks $m_1 \\ldots m_n$ e.g. picking, placing, opening box
    - policy $\\theta$ is pretrained on sub-MDP $\\tilde M$ that contains only a subset of those tasks e.g. picking, placing
    - the goal is to maintain learned skills on tasks in $\\tilde M$ when transferring to $M$ e.g. maintaining picking even if opening box comes earlier in the MDP $M$

## 3.2

$c$ is never defined
- I assume it is the current space the agent is on
- why is it a parameter to the environment if the agent starts at $x = 0$
- why do you have a variable $x$ that seems identical to $c$?
- I would expect $c$ to be a constant if it is a parameter to the env but that would make solving the problem impossible since it needs to know state
- Figure 3a says it sets $c = 0.5$, what does this mean given the previous context?

Deceptive X-axis in Figure 3 (right)
- left and center go to 500 steps
- right goes only to 50 steps
- this is confusing and showing the full 500 would be better
- e.g. it seems like M = 35 solves the full environment but only has a 15% chance of getting to Phase 2 at the end of training

I didn't fully understand why magnitude of $s1 / s2$ will make a big difference

why don't you demonstrate your CL methods on this?

nitpick: just use an affine model and make the state $c$, this is clearer and simpler

## 4

Figures 4 and 5 are great, very clear

If you want to do even better, you might consider randomizing orders of envs and taking average to get an overall score for ""best"" CL method


## Related Work

There is a substantial work on RL for language models that deals with this problem of continual learning, usually called ""alignment tax"" or ""robustness"" in that literature. Specifically, you can see the KL divergence to a pretrained model and re-running on pretrain data (replay) as CL methods, and they are standard in RLHF literature (KL introduced by Jaques et al, standard RLHF paper is Ziegler et al). There is also a smaller literature on ""Countering Drift"" in emergent communication that tackles a similar problem in RL (e.g. Lee et al). I don't think these works diminish the novelty of yours at all though, since these scenarios are really bandits and use RL for gradient estimation, essentially, whereas you are dealing with full MDPs and possibly more than two tasks (they do LM pretraining and finetune on a single task).

A comparison of CL and RRL methods, showing which ones are the same or have strong similarities would also be great.


","4: Good paper, strong accept",2,0,1,0,0,0,"1-Overall, this is a strong workshop paper on an interesting subject. I think it could be a good full conference paper submission in the future and I think the authors could improve some things for that so I've written more detailed comments.~~~1,2,3-The paper is novel in the connections it makes and I believe it makes a strong and bold claim that these continual learning (CL) problems are what is holding back reincarnated RL. The clarity could use some work as a couple experiments and conclusions are unclear, but the overall significance of this makes it a strong workshop paper. With some improvements, I can see this being a strong conference paper in the future.~~~1,2,3-What this paper does well is try to bring together the CL and RRL fields. I think a longer related work with equivalents between the two would help. The main robotics experiment does a good job of demonstrating skill forgetting in a practical way, and though it is toy-ish it is hard to dispute the main argument. The CL methods are simple but arguably even more convincing because they are simple and effective.~~~2-The main improvements would be to generally be clearer with what each experiment is testing and more strongly justify the conclusions drawn from each one. There are also some easy improvements to make the experiments more convincing that I suggest below. Finally, the paper is a bit light on overall results, which is fine for a workshop paper, but should definitely be improved for a full conference submission. I think just making the connection between CL and reincarnated RL is interesting but the thing that will sell the paper is justified, actionable, takeaways.",,,"[[Too long to identify all positive sentences, but many can be readily identified for each Positivity Guideline]]","1-<Overall, this is a strong workshop paper on an interesting subject?~~~3-<. I would recommend starting from the idea of temporal splits in your MDP and defining your setup around that with clear language that relates to your toy task setup e.g.>",0.6,0.2,0
FO4GdgU1yRz,NeurIPS.cc/2022/Workshop/HITY/Paper16/-/Official_Review,"This paper provides an acceleration method for adaptive SGD like Adam. The core idea is to improve upon the proximal point method by taking the norm in the regularization term to be wrt. the preconditioner. The resulting method is a Nesterov-like acceleration, combining both conservative and reckless steps in a gradient update step. 

I think the idea is interesting and initial experiments are looking good.",1: accept,2,0,1,1,1,0,1-I think the idea is interesting and initial experiments are looking good.,,,,"1-<I think the idea is interesting and initial experiments are looking good""",0.4,0.6,1
GT0K-M-85rZ,ICLR.cc/2021/Conference/Paper1404/-/Official_Review,"1.	I want to congratulate the authors on producing an attractive narrative, with a very nice flow to it and a very clear heuristic content.  I believe if I were seeing these ideas for the first time, I would definitely find this presentation friendlier and clearer than if I learned it from some other presentations.
2.	The topic is important. The analysis of the spectrum of the Hessian of the loss function gives us significant insight, and it is rather amazing and beautiful that the spectrum has definite mathematical properties which also have mathematical explanation. The narrative excels in making us feel this as something clear and natural.
3.	This is my first time ever reviewing for conferences of this nature; I am a bit unclear about the level of contribution we are looking for. My understanding is that researchers in this field are writing numerous very short papers each year and a paper doesn't need to represent more than an incremental bump over previous work.
4.	One reason I bid to review this Mss. is that the topic is not new to me. In the last few years three separate researchers at my institution have written papers on this general topic; and so I feel more conversant with this topic than some other material I might have been assigned. So '
5.	I feel that this work may not know about or may not have fully assimilated the contributions by other authors. I can mention these papers:

arXiv:1901.10159 
An Investigation into Neural Net Optimization via Hessian Eigenvalue Density
	Authors: Behrooz Ghorbani, Shankar Krishnan, Ying Xiao
Abstract: To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments.


So arXiv:1901.10159 clearly calls out the phenomenon of batch norm changing the hessian. In comparison, the paper under review gives a heuristically very clear explanation of why such an effect should be present, i.e. why batch normalization should be effective at making the objective easier to optimize.  However, the paper under review does not fully discuss the above research contribution of arXiv:1901.10159,  and it would be very easy for a reader of this paper to imagine that the phenomenon being presented here originates with this paper.   I should point out that the authors of arXiv:1901.10159 have made company-wide presentations about that work. Since the company is Google, that means that their work is fairly well known.


arXiv:1811.07062  
The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size
Authors: Vardan Papyan
Abstract: We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits ""spiked"" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually

To my knowledge arXiv:1811.07062  is the first paper to show the empirical deep learning community that one can compute the eigenvalue density spectra of modern deepnet classifiers eg those of the kind that are in use on large datasets like imagenet and in real applications. The result of having the technology in arXiv:1811.07062  is that the author is able to show that some of the key features of eigenvalue spectra that were seen in small scale situations are also present at full scale. The examples shown in the paper under review, in contrast, are of quite limited scale, and to my understanding do not represent the current state of the art. Consequently, although tools re available to test out the authors' ideas on realistic problems, we are left wondering what seen in these small examples might generalize. 

arXiv:1901.08244 Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians
Authors: Vardan Papyan
Abstract: We consider deep classifying neural networks. We expose a structure in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain ""averaging"" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes

To my knowledge arXiv:1901.08244 goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. arXiv:1901.08244  shows that not only do the C class means influence the eigenvalues, but actually there are C(C-1) eigenvalues that have structure deriving from means. The examples shown in the paper under review, in contrast, are of quite limited scale, and in contrast don't seem able to show the full structure which we now know to be present in deepnet spectra across a very wide range of networks and datasets at full scale.  However, the paper under review does not discuss the research contribution of arXiv:1901.08244,  and in consequence a reader could get the misleading impression that this paper's heuristic explanations of the mean structure are the only knowledge we have about this phenomenon, when we have actually dramatically more information. 


arXiv:2008.11865  Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra
Authors: Vardan Papyan
Abstract: Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deepnet spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, ""spikes"", and small but distinct continuous distributions, ""bumps"", often seen beyond the edge of a ""main bulk"". The significance of the cross-class structure is illustrated in three ways: (i) we prove the ratio of outliers to bulk in the spectrum of the Fisher information matrix is predictive of misclassification, in the context of multinomial logistic regression; (ii) we demonstrate how, gradually with depth, a network is able to separate class-distinctive information from class variability, all while orthogonalizing the class-distinctive information; and (iii) we propose a correction to KFAC, a well-known second-order optimization algorithm for training deepnets.

To my knowledge arXiv:2008.11865  again goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. arXiv:2008.11865  in particular discusses the KFAC approximation of Martens and Grosse and in fact claims, with substantial evidence gleaned from many realistic examples, that KFAC is not a good approximation. The paper under review doesn't cite arXiv:2008.11865, but does make heuristic claims about the adequacy of the KFAC approximation.  However, the evidence presented is much weaker and the implications much more sketchy than what is discussed at greater length  in arXiv:2008.11865.
",,2,0,0,0,0,0,"1-I want to congratulate the authors on producing an attractive narrative, with a very nice flow to it and a very clear heuristic content. I believe if I were seeing these ideas for the first time, I would definitely find this presentation friendlier and clearer than if I learned it from some other presentations.",,,"1-I want to congratulate the authors on producing an attractive narrative, with a very nice flow to it and a very clear heuristic content. I believe if I were seeing these ideas for the first time, I would definitely find this presentation friendlier and clearer than if I learned it from some other presentations. The topic is important. The analysis of the spectrum of the Hessian of the loss function gives us significant insight, and it is rather amazing and beautiful that the spectrum has definite mathematical properties which also have mathematical explanation.~~~[[2,3-The remainder of the paper is extremely detailed, giving clear and specific reasoning for critiques.]]","1-< I want to congratulate the authors on producing an attractive narrative, with a very nice flow to it and a very clear heuristic content> ~~~2-<. I can mention these papers:>",1,0,0
GvVHhZXZkx2,ICLR.cc/2021/Conference/Paper3700/-/Official_Review,"The paper focuses on defining a new architecture that allows being reduced without significantly affecting the performance. 

In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art. Experiments are not convincing: Toy experiment and minimum experiments in MNIST without a clear comparison to existing neuron pruning algorithms.  ",2: Strong rejection,1,2,2,2,2,2,"In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art. Experiments are not convincing: Toy experiment and minimum experiments in MNIST without a clear comparison to existing neuron pruning algorithms.","In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art.","In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art.","In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art.",,1,2,2
H1gp9H-xhX,,"This paper formulated the problem of unsupervised one-to-many image translation and addressed the problem by minimizing  the mutual information. A principle formulation of such problem is quite interesting. However, the novelty of this paper is limited. The proposed the method is a simple extension of InfoGAN, applied to image-to-image translation and replacing the mutual information part with MINE.

The experiments, which only include edge to shoes and MNIST to SVHN, are also not comprehensive and convincing. This paper also lacks discussion of several quite important related references for one-to-many image translation.

XOGAN: One-to-Many Unsupervised Image-to-Image Translation
Toward Multimodal Image-to-Image Translation

",,1,1,2,1,1,0,,This paper also lacks discussion of several quite important related references for one-to-many image translation.,"The proposed the method is a simple extension of InfoGAN, applied to image-to-image translation and replacing the mutual information part with MINE. ~~~ The experiments, which only include edge to shoes and MNIST to SVHN, are also not comprehensive and convincing.",,,0.3,1,1
h25PR6dVQy,MIDL.io/2020/Conference/Paper213/-/Official_Review,"Summary: 

A pre-trained organ segmentation network based on CNN model is evaluated with different noise and CT dose settings. These variations are simulated by adding structured noise. It is reported that the DICE accuracy is reasonable even when CT dosage is reduced by 30%

Strengths: 
+ The question of reducing CT dosage is an important one and the set-up used here with the noise models can be useful. 
+ The conclusion that CT dosage can be reduced by upto 30% is an important one.
+ The experiments and the plots look convincing

Weakness:
- Perhaps the authors are not used to submitting to MIDL-like conferences, as the paper lacks some essential components to it. Such as the description of the models/data used, comparison, experimental set-up, citations.

I would encourage the authors to investigate this research question further as it is of value to the community. In terms of presenting the work, perhaps reading papers from previous versions of the conference can be a good starting point to help organize the work in a manner that is accessible to the MIDL community. ",1: Strong reject,1,1,1,0,0,0,,,,,,0.4,0.4,0
H5eT1_2Cbc,ICLR.cc/2022/Workshop/OSC/Paper13/-/Official_Review,"The paper is well-written and the main approach could be understood.

Pros:
* The approach of separating out a policy intro a graph-based classification problem and then rule-mining based approach seems novel.
* The experiment results are promising

Cons: 
* Discussion regarding how such an approach could translate to standard RL benchmarks could have been discussed.
* Discussing how the approach could extend to non-text-based domains could also have been discussed.","2: Good workshop paper, accept",1,0,1,0,0,0,The paper is well-written and the main approach could be understood.,,,,,0.6,0.2,0
Hke184akY4,ICLR.cc/2019/Workshop/RML/-/Paper6/Official_Review,"I enjoyed reading the paper. I wish there are more papers which study how to debug a model which requires lots of computational resources. 

This paper focuses on difficulties involved in models which uses  large datasets for training. This paper makes the claim that in order to understand better what the model is doing, it's important to have monitoring systems in the discovery process.
This paper does a pretty good job in describing the components involved in a system like 
AlphaGo (and its variants). Specifically, paper focuses on reproducibility in self-play algorithms.
Its interesting to know that conclusion of the paper is  to monitor nearly everything one could think of, to make sure that the implementation is bug-free.
","5: Top 15% of accepted papers, strong accept",2,0,0,0,0,0,,,,"[[Specific and encouraging, even if not much constructive criticism]]",1-<I enjoyed reading the paper. I wish there are more papers which study how to debug a model which requires lots of computational resources. >,1,0,0
Hke2Xie-c4,ICLR.cc/2019/Workshop/LLD/-/Paper28/Official_Review,"The authors consider the setting of deep attention learning. It consists in selecting critical unlabelled data in a semi-labelled dataset, so that once labelled they can improve drastically the accuracy of the model. The approach of the authors consist in training a DNN that computes similarity between data, starting with a limited pool of labelled data points. To do so, they augment iteratively the dataset using a greedy approach. 

The paper is well written, and even I am not not at all a specialist of the field I think I understood the main points of the paper.

The numerical experiments seems strong enough to be convinced by their approach.","4: Top 50% of accepted papers, clear accept",1,0,0,0,0,0,"The paper is well written, and even I am not not at all a specialist of the field I think I understood the main points of the paper.","The paper is well written, and even I am not not at all a specialist of the field I think I understood the main points of the paper.",,,,1,0,0
Hkns5PSlM,ICLR.cc/2018/Conference/-/Paper1026/Official_Review,"[Summary]

The paper is overall well written and the literature review fairly up to date.
The main issue is the lack of novelty.
The proposed method is just a straightforward dimensionality reduction based on
convolutional and max pooling layers.
Using CNNs to handle variable length time series is hardly novel.
In addition, as always with metric learning, why learning the metric if you can just learn the classifier?
If the metric is not used in some compelling application, I am not convinced.

[Detailed comments and suggestions]

* Since ""assumptions"" is the only subsection in Section 2, 
I would use \\texbf{Assumptions.} rather than \\subsection{Assumptions}.

* Same remark for Section 4.1 ""Complexity analysis"".

* Some missing relevant citations:

Learning the Metric for Aligning Temporal Sequences.
Damien Garreau, RÃ©mi Lajugie, Sylvain Arlot, Francis Bach.
In Proc. of NIPS 2014.

Deep Convolutional Neural Networks On Multichannel Time Series For Human Activity Recognition.
Jian Bo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, Shonali Krishnaswamy.
In Proc.  of IJCAI 2015.

Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks
Yi ZhengQi LiuEnhong ChenYong GeJ. Leon Zhao.
In Proc. of International Conference on Web-Age Information Management.

Soft-DTW: a Differentiable Loss Function for Time-Series.
Marco Cuturi, Mathieu Blondel.
In Proc. of ICML 2017.",4: Ok but not good enough - rejection,1,1,1,1,2,0,,,Using CNNs to handle variable length time series is hardly novel.,Using CNNs to handle variable length time series is hardly novel.,,0.3,1,1
Hvpa5Y-eZ2,MIDL.io/2020/Conference/Paper98/-/Official_Review,"The authors claim to have developed a method to generate an ADC image from T2Weighted prostate MRI.
The assumption is that T2-weighted imaging contains information to generate ADC images. I find this assumption absurd. Prostate MRI is well researched and the information in t2 and DWI imaging is clearly distinct and both are required. Depending on the zone either T2 or DWI is required to diagnose prostate cancer. (Read PIRADS). Why not claim this for all MRI imaging? Just acquire one sequence and you'll generate all other images of a patient!",1: Strong reject,2,2,2,2,2,2,3-I find this assumption absurd.,4-I find this assumption absurd.,3-I find this assumption is absurd,"3,2-I find this assumption absurd.~~~3,2-Why not claim this for all MRI imaging? Just acquire one sequence and you'll generate all other images of a patient!",3-<I find this assumption absurd>,1,2,2
HWVg9RTyuWq,ICLR.cc/2022/Workshop/OSC/Paper18/-/Official_Review,"The paper proposes to build a linear bayesian network on the output of linear programs with the NOTEARS method. A linear program takes as input a convex polytope of constraints and a linear cost function and outputs a point in the polytope that minimizes the cost. For example, for a fixed polytope in 4 dims, for a distribution of cost functions, the authors generate a dataset of samples of the 4D cost vector and 4D corresponding solution. They then fit a linear Bayes Net to explain the 8D random vector. The authors find  some relationship between the learned Bayes Net coefficients and the polytope.

I think the approach is quite creative and it may shine some light on the inner structure of optimization algorithms. That is why I recommend acceptance of the paper for the workshop.

However, I find two major shortcomings in the method. First: while the authors make reference to causality, it is  unclear what causality means in this context. Why should the learned Bayes Net be a Causal Bayes Net? For example, why would one allow for the solution causing the cost function? Given an algorithm that maps inputs to outputs, can one ever assign in general any causality to that, without first postulating that the workings of the algorithm somehow are corresponding to some real causal/physical process (which the authors do not) or define a notion of intervention? The authors do note that they measure mere correlation, but using the word causality warrants a better explanation.

Secondly, in spite of the name, the solutions of linear programs do not depend  linearly  on the polytope or the cost function. Fitting a linear Bayes Net thus seems  inappropriate. This is  most obvious when the authors attempt to fit a linear classifier on the boundary of a convex polytope in fig 2.","2: Good workshop paper, accept",2,0,1,0,0,0,"1-I think the approach is quite creative and it may shine some light on the inner structure of optimization algorithms. That is why I recommend acceptance of the paper for the workshop.~~~2,3-
 However, I find two major shortcomings in the method. First: while the authors make reference to causality, it is unclear what causality means in this context. Why should the learned Bayes Net be a Causal Bayes Net? For example, why would one allow for the solution causing the cost function? Given an algorithm that maps inputs to outputs, can one ever assign in general any causality to that, without first postulating that the workings of the algorithm somehow are corresponding to some real causal/physical process (which the authors do not) or define a notion of intervention? The authors do note that they measure mere correlation, but using the word causality warrants a better explanation.",,,,1-<I think the approach is quite creative and it may shine some light on the inner structure of optimization algorithms>,0.6,0.2,0
Hye2YPOu37,ICLR.cc/2019/Conference/-/Paper259/Official_Review,"
The title is misleading, since only two particular manifolds are studied in this work. In addition, the proposed methods cannot be applied to a larger or a general class of manifolds. Therefore, you should update the title.

There are multiple problem definitions proposed in the paper. They are not compatible with each other and also with the proposed methods. In addition, some of the proposed problem definitions are incorrect, as explained below:

You should be more precise about the definition of the manifold you consider in this paper. For example, in equation (1), please define your manifold of interest more precisely checking some standard textbooks.

Please define intersection of manifolds, what do you mean by which intersection of which type of manifolds?

In the contribution (1); the paper does not introduce an algorithm to deal with optimization on with multiple manifolds, but for a particular type of individual manifolds.

In the contribution (2): It is not clear why and how the proposed method can be applied to optimization on manifolds with momentum (what do you mean by use of momentum here?), and regularization (what do you mean by regularization?). There are many problems with this claim, but you can simply consider that applying momentum and regularization will affect the geometry of loss landscape.

Definition of retraction is not precise, please fix it.
What is L in equation (2)?

Please define neighborhood in U_x in Lemma 2.1.

What is || ||in Lemma 2.1?

As you also noticed on page 3, an intersection of manifolds may not be a manifold. Then, your proposed first problem (1) fails. Therefore, you should completely change your claims on your problem definitions and contributions.

What do you mean by ""We add a drift which contains information from the other manifold to the original gradient descent on manifold""? What is ""the information from the manifold""? In equations (3) and (4), you just apply optimization on manifolds individually. 

How do you compute/determine a_k^(1) and a_k^(2)? How do they affect the theoretical and experimental results?
In your claim ""From the construction of bk, we can see that the smaller the correlation between gradf(xk) and hk is, the smaller effect the information from M2 brings"", it is not clear how ""the information from M2"" affects? First, again, what is ""the information""? Second, b_k^(1) and b_k^(2) are computed for individual manifolds separately. Then, how ""the information"" make an effect?

 In Theorem 2.2, what do you mean by ""then xk convergence to a local minimizer""?

What is <,> in Theorem 2.2?

What is ^ in Theorem 2.3?

What is v in proof 6?

What is an engine value?

What does P (1) xk gradf(yk) denote in computation of h_k? For example, gradf(yk) is a vector on tangent space of the second manifold at yk. Then, how do you project orthogonally this projected vector to the tangent space of the first manifold at xk? 

They may be completely different geometries, and such an ""orthogonal projection"" may not exist in general. Then, how do you compute and calculate that projection?

All the theoretical results given in the paper are not about convergence of parameters on a manifold at the intersection or product of manifolds but for an individual manifold. For example, x and y belong to manifolds M1 and M2, and convergence results is about x. How are they related to parameters at the intersection or product of manifolds?

The statements regarding batch normalization are confusing and also sound incorrect:

Do you apply batch normalization on weights on BN(w)?

Please explain what you mean by ""BN(w) has same image space on G(1, n) and St(n, 1)"". There are not such results in the papers Cho & Lee (2017); Huang et al. (2017) you cited for these results.

What do you mean by ""applying optimization on manifold to batch normalization problem""?

In your statement ""However, the property of these two manifold implies that we can actually optimize on the intersection of two manifolds"". Please explain how does this property imply this result more precisely?

Please define ""Grassmann manifold G(1, n)"" more precisely. In your notation, together with explanation of the notation for St(n,p), G(1,n) is like a set of 1xn dimensional row vectors, while St(n,1) is an nx1 dimensional column vector, Then, their intersection is an empty set and your proposal for optimization on a vector on their intersection is wrong. 

Notation and definitions used in (9) are wrong and confusing. Please check and revise them.

In the whole paper, the problem, method, solutions, theorems, and contributions are proposed for optimization using parameters which belong to intersection of some manifolds. Then, suddenly, you start considering optimization on product manifolds, and give the results for that;

What does the statement ""Then we apply Algorithm 1 to update parameters, which means we optimize on a product manifold"" mean?

What do ""G(1, k1) Ã— Â· Â· Â· G(1, kn)"" and ""St(k1, 1) Ã— Â· Â· Â· St(kn, 1)"" denote?

Don't you perform optimization on intersection of manifolds? Why do you ignore your original problem and methods, and consider this problem? 

In addition, how do you use your Algorithm 1 for optimization on product manifolds? Optimization on intersection on manifolds and product manifolds are completely different problems. If they are same or related to each in particular cases in your specific definitions, then you should provide these definitions more precisely.

What do you mean by optimization on product manifold of weights of all layers? If you compute a product manifold for spaces of all layers, then you simply perform a shallow optimization on a huge matrix containing millions of dimensions according to this definition. First, how do you do that? Second, how can you train a large network using this approach?

In the experiments, please first give variance of errors. These results are statistically insignificant.

Which problem is solved to perform these experiments is not also clear (see above).

The results reported in the paper are also not good, may be due to the mathematical and algorithmic  problems and errors mentioned above. Please clarify them, and provide additional results, especially using other datasets (small scale mnist and large scale imagenet), and networks (mlp, vgg, resnet etc.)

Related work is also incomplete, such that many traditional and recent work on optimization on multiple manifolds are omitted. ", you just apply optimization on manifolds individually. ,2,1,2,2,2,1,,"2,3,4-Therefore, you should completely change your claims on your problem definitions and contributions.","2,3- There are multiple problem definitions proposed in the paper. They are not compatible with each other and also with the proposed methods. ~~~","3,2-...[[I feel like we really need a guideline that accounts for sentences that just really could've been said better. Excessive Negativity description doesn't quite cover a lot of specific examples in this as well as that sentiment would, even if it does characterize the review well.]]","2,3<In the contribution (1); the paper does not introduce an algorithm to deal with optimization on with multiple manifolds, but for a particular type of individual manifolds.> ~~~2,3-<Please define neighborhood in U_x in Lemma 2.1.>",0.4,1.6,2
Hyevqfi-5V,ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Official_Review,"This paper proposes a way to train image classification models to be resistant to L-infinity perturbation attacks. The idea is to simultaneously learn the classification model and an adversary model that adds L-infinity-bounded perturbations to images, in order to maximally confuse the first model. This adversary model can use not only the image itself, but also gradient information from the classification model. It can even propose a perturbation, get gradient information on that perturbation, and then propose an updated perturbation, similarly to how projected gradient descent (PGD) can take multiple gradient steps to find a perturbation.

The main results are that training in this way improves adversarial accuracy compared to PGD, while improving training speed. On CIFAR-10 with epsilon=0.03, the proposed method gets 51.5% accuracy against a PGD adversary, whereas the PGD-trained model gets 40.7% accuracy. Madry et al. (2017) report better accuracy of 47%, but the proposed method still improves upon this. Moreover, the model-based adversary is faster than PGD, as shown by faster training times (more than 2x faster to get similar accuracy as PGD, and the best model is still about 50% faster).

Overall, I believe the paper is above the acceptance threshold from a quality perspective, and likely in the top 50% of accepted papers. However, it may not be a good fit for the topic of this workshop. Technically you could argue that the adversary is synthesizing a perturbation to an image, so this is some sort of structured generation. Therefore I give an overall rating of 3, and defer to the workshop organizers regarding appropriateness.

Minor note: In Algorithm 2, I think g(x_i, u_i; \\phi) should use the \\mathcal{A} notation used elsewhere.",3: Marginally above acceptance threshold,2,1,1,1,0,0,,,,"1-I believe the paper is above the acceptance threshold from a quality perspective, and likely in the top 50% of accepted papers.~~~3-However, it may not be a good fit for the topic of this workshop. Technically you could argue that the adversary is synthesizing a perturbation to an image, so this is some sort of structured generation.","1-<Overall, I believe the paper is above the acceptance threshold from a quality perspective>",0.4,0.6,1
HylhCmRp2Q,ICLR.cc/2019/Conference/-/Paper1337/Official_Review,"The authors combine graph convolutional neural network and conditional random fields to get a new model, named conditional graph neural fields. The idea of the paper is interesting, but the work is not solid enough. Detailed comments come as follows,

1. In the proposed model, the authors treat the pairwise energy as prior and they do not depend on any features. Unlike the usual graph model in Eq (4), the authors further use the normalized \\hat A as a scale factor in the pairwise term. What is the intuition for this?

2. The loss (10), which the authors claim that they are using, may not work. In fact, the loss cannot be used to use for training most architectures: ``while this loss will push down on the energy of the desired answer, it will not pull up on any other energy.''(LeCun et. al. 2006, A tutorial on energy-based learning). For deep structured model learning, please using piecewise learning, or joint training using some common CRF loss, such as log-loss. In fact, the authors are not using the energy-based loss as they have constraints on unary and pairwise terms. In fact, if we ignore the pairwise term in (11), the loss becomes log-loss for GCN. With the pairwise term, the loss is somehow like the loss for piecewise learning but the constraints on U is wrong (for piecewise learning, U should sum to 1).

3. The inference procedure is too simple that it can hardly find the near-optimal solutions. In fact, there exists an efficient mean-field based inference algorithm (Shuai Zheng et. al., 2015). Why did the authors choose a simple but poor inference procedure?

Comments After rebuttal
==========
Thank you for adress my concerns.

The response and the revision resolved my concern (1). However, the most important part, the possibly problematic loss is not resolved. It is true that sometimes (10) can achieve good results with good regularizers or a good set of hyperparameters. However, theoretically, the loss is ]only pushed down the desired answer, which may make the training procedure quite unstable. Thus I still think that a different loss should be used here.",4: Ok but not good enough - rejection,1,1,1,1,2,0,,,,What is the intuition for this?~~~Why did the authors choose a simple but poor inference procedure?,,0.3,1,1
hYTP2pyQfUb,ICLR.cc/2021/Conference/Paper3132/-/Official_Review,"__Summary__
They authors describe a method to detect structural variation from aligned sequencing reads in a genome browser view. Their model encodes this genome browser view into an RGB image and applies a deep convolutional neural network to classify variant type (or no variant). They make use of curated variant annotations to train and test their model.

__Major comments__
* The RGB encoding is entirely arbitrary, unnecessary, and confusing. The authors should consider the actual range of the various input data and encode with a simple and interpretable strategy. For example, nucleotides are typically one hot encoded.

* The improved accuracy on this task is abstract when only summarized in tables. Depicting an example of a structural variant whose prediction is improved by the authors' method would be very valuable. Ideally, both a false positive turned true negative and a false negative turned true positive could be shown.

* The requirement that this method be provided candidate structural variation start and end points means that it's actually a module that would need to be plugged into a pipeline that also specified how those candidates are obtained. I would encourage the authors to develop that strategy before publishing their method.

* The authors have not clearly described how the predictions of the other methods were used to annotate these curated variants. Doing so involves critical parameters, such as the allowed distance between the method prediction and true specified variant break points. Setting these parameters to strict values would be very unfair given that the authors method is not required to produce such breakpoints de novo. The Zook et al. paper describes this process in detail and discusses several software packages to do it.

__Minor comments__
* Does the authors' method make use of paired end read information?
* How does Table 2 combine the accuracies for duplications and indels?",2: Strong rejection,1,1,2,1,2,1,,"The RGB encoding is entirely arbitrary, unnecessary, and confusing.","The RGB encoding is entirely arbitrary, unnecessary, and confusing. The authors should consider the actual range of the various input data and encode with a simple and interpretable strategy. ~~~ The requirement that this method be provided candidate structural variation start and end points means that it's actually a module that would need to be plugged into a pipeline that also specified how those candidates are obtained. I would encourage the authors to develop that strategy before publishing their method.","The RGB encoding is entirely arbitrary, unnecessary, and confusing.","The RGB encoding is entirely arbitrary, unnecessary, and confusing.",0.4,1.4,1
hZEzZhTqdY,NeurIPS.cc/2022/Workshop/nCSI/Paper22/-/Official_Review,"The paper studies the sample size (of observational and experimental data needed to estimate the probability of causation, given a specified confidence interval. The authors derive the number of ""adequate"" sample sizes and evaluate their theoretical results empirically while estimating the PNS 

$Pros$
-  The paper proposes a theorem describing the margin errors for the bounds of PNS given a confidence interval. 
-  The theoretical results are tested on a synthetical data, confirming that when the sample size reaches the predicated number the estimated bounds appear more stable. 
-  The authors provide also a discussion on the relationship between the sample size of experimental and observational data on the average error of estimates, giving guidelines on when the largest drop in the error occurs and  thus what would be a recommended minimal size of the data. 

$Cons$
-  I would like the authors to comment on the impact of some design choices of the experiments on the obtained results, or the justification for the used models.  In particular, why the authors use only two models? Are those models meant to simulate some true data, or are purely synthetic (if, yes, how have they been generated - i.e. I can see the description in the appendix, but I am not sure what was the method for choosing the parameters of the distributions).  Similarly, why have the authors chosen to use 20 confounders? 

In general I would also like to see how the work can be extended to real-world application and problems (something that authors mention in the Discussion, and which would be nice to see in the future). 
",,1,0,1,0,0,0,,,,,,0.6,0.2,0
Jxc_u-7b8j,ICLR.cc/2021/Conference/Paper1148/-/Official_Review,"The authors argue that there are some safe ""spots"" in the data space that are less prone to adversarial attacks. The authors propose a technique to identify such ""safe spots"". They then leverage them for robust training and observe higher robust accuracy than baseline. Finally, they leverage this observation to identify out of distribution data. 

The application is important and the results look promising. However, I have the following concern:

- The authors propose a new threat model where the adversary may have access to the labeled data. They motivated such a setting with an example of Google image search. However, such a setting is quite limited. There are also existing methods that use supervised learning setting with incorrect labeling. The paper should discuss how they differ from such a line of work. 

- The search algorithm requires that a correct predicted label is available. This setting is not quite realistic. How can we find a safe spot when the label is unknown.

- Some of the findings are not quite surprising. For example, a safe spot is more in a robust model with small epsilon. 





",5: Marginally below acceptance threshold,2,1,1,1,1,0,,,,"[[I just can't get myself to call this positiveâ€¦ It might meet some of the criteria, but it's clearly different than other, distinctly positive reviews to me.]]",1-<The application is important and the results look promising. >,0.6,0.8,1
kR5sr054cH,NeurIPS.cc/2022/Workshop/HITY/Paper29/-/Official_Review,"The paper proposes a novel method for weak-supervised learning where the relation between training targets and unknown labels is described by a symbolic domain constraint. In particular, the example the paper uses is to learn an MNIST classifier, via targets that are sums of MNIST digits. The proposed method essentially consist of 3 steps. First, a latent representation of the MNIST images is learned as an autoencoder in an unsupervised manner. Then, K-means (with K=#number of MNIST classes) is applied to the latent representation yielding K clusters. Then, for batches of images with known sums, the clusters are assigned an MNIST class each. The predicted class per image is then used to train a supervised CNN. The claim of the paper is that their step-wise method scales better in the constraint complexity at the expense of some final classification accuracy which is an interesting trade-off.  The reduction in combinatorial complexity is achieved via the learned latent representation.

Minor comment: I enjoyed the insightful discussion part of the paper. It would still be interesting to report the final classification accuracy that is mentioned between lines 109-114, even it is not on par with the competitor. Further, I would be interested in a comment on the central assumption that clusters can indeed be assigned a label. For MNIST, the assumption might be approximately true, but for more complex datasets, it is unclear if the clusters learned in an unsupervised way will in any way relate to the ground truth classes. This is a generally known issue that seems to be quite relevant for the proposed method. ",1: accept,1,0,1,0,0,0,I enjoyed the insightful discussion part of the paper,,,,,0.6,0.2,0
lewXU7bGuDM,ICLR.cc/2021/Conference/Paper1408/-/Official_Review,"

Update: The authors addressed part of my concerns. For the factor estimation, the proposed method relies on first order approximations while learning the posterior of the factors; however, the approximation error does not enter into the posterior.  The approximation also raises concerns regarding the convergence of the algorithm. Overall, I think the approach is promising, but some justification of the quality of the approximation is needed. Thus, I tend to keep my rating.

##############

The authors propose a streaming approach to tensor factorization with Bayesian neural networks. The problem is to factorize a three-way tensor with Gaussian noise. The proposed approach combines a Bayesian neural network (BNN) whose output predicts the entries of the tensor and the streaming variational Bayes (SVB) for incremental posterior updates. In addition, a spike-and-slab prior is placed on the BNN weights to encourage sparsity as well as prevent overfitting.  The authors performed empirical studies on four real datasets, DBLP, Anime, ACC, and MovieLen1M, where improved prediction accuracy of the tensor entries are reported.

From the empirical evaluation, the objective of the proposed approach is to ensure that the output of the BNN matches the tensor entries. However, this does not necessarily guarantee the correctness of the recovered factor matrices (input to the BNN). I feel that the problem setting of the paper is somehow different from the standard CP factorization setting where the (unique) factors are of primary interest. It would be good to add some discussions about the correctness and/or uniqueness of the uncovered factors.

Strengths:
- The idea of introducing a spike-and-slab prior in the factorization is interesting.
- Using SVB for online posterior updates is computationally practical.

Weaknesses:
- The presentation could be improved — it is not very clear how the factor matrices are estimated.
- Factorization using the proposed method is not unique (up to rescaling and permutation of the factor matrices). 
",,1,1,2,0,1,1,,The approximation also raises concerns regarding the convergence of the algorithm.~I feel that the problem setting of the paper is somehow different from the standard CP factorization setting where the (unique) factors are of primary interest.,,,,0.3,1,1
LExLxHUUQi,NeurIPS.cc/2022/Workshop/HITY/Paper3/-/Official_Review,"This paper proposes to use the peaks and droughts in training with multiplicative cyclic learning rate schedules, to construct approximations to accuracy-training tradeoff curves. While this is an ad hoc approximation, it has the advantage to be done in one training pass. The idea sounds crazy to me, but the experiments are actually pretty encouraging in the sense that the approximations are pretty accurate. Hence, I vote to accept this paper.",1: accept,1,0,0,0,1,0,"The idea sounds crazy to me, but the experiments are actually pretty encouraging in the sense that the approximations are pretty accurate.","The idea sounds crazy to me, but the experiments are actually pretty encouraging in the sense that the approximations are pretty accurate.",,,,0.6,0.2,0
MNwIj2eGPn,,"The work experiments with a flat prior in functional space for multilabel classification, in a Deep Learning setting. The network f_theta predicts, given the input x, the concentration parameters alpha of a Dirichlet prior on the vector of class probabilities. After marginalizing over class probabilities this yields the standard softmax on label probabilities (up to reparametrization $\\alpha$ <-> $\\exp{\\eta}$). 
The functional prior is built from the Dirichlet distribution with $\\alpha=1$; and evaluated on a measurement set in the data space that suitably accounts for in- and out-of-distribution points. 

I think this abstract is suitable to be presented at MIDL.

The approach is not necessarily very novel in practice, but the functional space perspective is still uncommon and interesting (including the soft-constraints induced by a prior that looks uninformative at first glance). Also, the functional viewpoint as a way to incorporate Bayesian priors in neural networks is a promising direction.

There are a few typos that can be corrected. The choice of validation is suitable and reasonably executed given the format. The entropy/ies are mentioned at the very end but not reported? 

There are a few claims that do not necessarily serve the argument:
""uncertainty outputs, which can increase patient safety [...]"" -> maybe not necessary to go there unless you have results? 
""Our method is also significantly less computationally expensive as compared to Bayesian or frequentist approaches"" -> At most it is orthogonal to being Bayesian or frequentist. The work is quite clearly using the Bayesian toolbox, including evidence lower bound computations (as per the title(!), it is an instance of variational inference).",,1,1,1,0,0,0,,,,,,0.4,0.4,0
nAKAovmTOnF,ICLR.cc/2021/Conference/Paper3396/-/Official_Review,"This paper presents a new method which can be combined with graph search algorithms to boost exploration when the uncertainty is high. This new mechanism, called TBV, can override actions given by the model to explore and verify model predictions. It is also shown in the experiments that TBV improves the model performance when combined with MCTS or BestFS. TBV utilizes graph structure of the problem and finds the solution much quicker for both MCTS and BestFS. 

While the presented method is interesting with high performance, I found many editorial errors in the writing. For example, in the second paragraph of section 3.3, 'we concentrated of exploration.', and 'In our experiments, such a version proved to be effective in in discrete', just to name a few. There are so many errors like this and the paper needs serious rewriting. Also, having a conclusion or discussion can help the structure of the paper. 

Figure 3 is unclear if the blue line is without TBV with the legend 'Right corridor visited'. It could be interesting to discuss extension of TBV into continuous environments. Reference format seems to have errors since there are underlines. "," and ""ËœIn our experiments",2,1,2,1,1,0,,"3,5-There are so many errors like this and the paper needs serious rewriting.",,[[pretty solidly 1 for me; it meets the professional language minimum but does not meet the positive requirements]],"1-<While the presented method is interesting with high performance,>~~~2,3<Figure 3 is unclear if the blue line is without TBV with the legend Ã¢â‚¬ËœRight corridor visitedÃ¢â‚¬â„¢. It could be interesting to discuss extension of TBV into continuous environments.>",0.3,1,1
NlWYvX5T-gE,ICLR.cc/2023/BlogPosts/Paper15/-/Official_Review,"This blogpost elucidates the reader on MIDI-DDSP by Wu et al. (ICLR2022), and reads like a summarisation of the original paper. As a result, there doesn't appear to be any additional content or insights above and beyond this original work. For instance, almost all the figures are derived from the original paper, and I actually found the original blogpost from the ICLR2022 paper more informative, as it includes a worked example demonstrating the workflow of MIDI-DDSP with synthesised audio examples and audible comparisons with prior work.

As a secondary note, there are also errors/typos in this blogpost. To list a few:
*  ""I has 3 separately trainable modules (DDSP Inference, Synthesis Generator, Expression Generator)"" -> ""It has 3 separately trainable modules (DDSP Inference, Synthesis Generator, Expression Generator).""
* ""As shown in below Figure 1(left) shows that strided convolution models..."" -> ""As shown below in Figure 3 (left), strided convolution models...""
* ""DDSP model overcomes above challenge and gain an advantage..."" -> ""The DDSP model overcomes the aforementioned challenge, and gains an advantage...""
* ""As Shown in above figure in left there is comparison of Mel spectrogram of synthesis results and on right it shows comparison of synthesis quality from listening test. From figure(right) it is seen that MIDI-DDSP inference is perceived as likely as ground truth compared to other methods such as MIDI2Params, Ableton and FluidSynth."" -> The figures are missing captions and labels entirely, and there were multiple figures here, so it was unclear what is being referred to. Moreover, there are issues with language and errors in capitalization.

Subsequently, I don't think this fits the requirements for the ICLR blogpost track. If the authors had provided additional insights, such as an example based on learning from polyphonic examples, then I would be comfortable recommending this (assuming the formatting issues are addressed). However, in its current state I don't believe this is appropriate.",3: Clear rejection,1,1,2,1,1,0,,"As a result, there doesn't appear to be any additional content or insights above and beyond this original work.~However, in its current state I don't believe this is appropriate.",,,,0.3,1,1
OJrxu0jIyry,ICLR.cc/2021/Conference/Paper2655/-/Official_Review,"This paper provides a theoretical connection between active inference and reinforcement learning and develops a method that can find a prior preference from experts. The new theory is derived from the concept of expected free energy (EFE) based on the free-energy principle.  Simulation experiments were conducted, and the effect of the prior preference learning was demonstrated.

The theoretical contribution of the paper is to find the relationship between EFE and negative value function and proposed a prior preference learning method. The theoretical connection is insightful and interesting. 

However, the originality of the proposed method itself is not clear from the theoretical and practical viewpoints.
In the experiment, they compared their method with a baseline method, i.e., global preference. 
There is no comparison between the pre-existing baseline method.
Though the EFE-based approach is very interesting, the authors did not succeed in providing evidence of the advantage of the proposed method.
It is questionable if this experiment is suitable for evaluating the main argument of this paper.

Also, from the viewpoint of the information-theoretic approach to RL and the relation to the free energy principle, studies related to ""control as inference"" is worth mentioning.

- Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).
- Okada, Masashi, and Tadahiro Taniguchi. ""Variational inference mpc for bayesian model-based reinforcement learning."" Conference on Robot Learning. 2020.
- Hafner, Danijar, et al. ""Action and perception as divergence minimization."" arXiv preprint arXiv:2009.01791 (2020).

<Minor comments>

Capitalized Q is used for representing a variational density function. Q is often used in action-value function in the context of RL. If this is not equivalent to Q-function, it cannot be very clear. I think using q is a better choice.

In 5.1.1, ""We did not run setting 2 in this study, because Acrobat is ambiguous in defining the global
preference of the environment.""
-> This may be ""setting 4.""


The definition of ""global preference"" is not given. To my understanding, the term is not so well-known in the community of imitation and reinforcement learning. That should be defined. Because of this, what the experiment showed is unclear to potential readers.


In conclusion, they describe, ""We also show that active inference can provide insights to solve the inverse RL problems."" However, they did not provide any explicit discussion over ""inverse RL."" This is actually the second time they mention ""inverse RL."" The first one is just at the end of the introduction.
This should be explicitly mentioned if the authors put this statement in conclusion.",5: Marginally below acceptance threshold,2,1,1,1,0,0,,4-It is questionable if this experiment is suitable for evaluating the main argument of this paper.,,"[[I think this meets the positivity guildelines completelyâ€¦even though it is substantially critical, it is constructive, specific, and acknowledges positive aspects]]",1-<The theoretical connection is insightful and interesting. >,0.4,0.6,1
QgIvgxhbwbi,ICLR.cc/2021/Conference/Paper1252/-/Official_Review,"This paper proposes a consistency score (C-score) that measures the expected accuracy of a held-out instance averaged over different training sample sizes, which can be useful for analyzing learning dynamics and generalization performance.

I see some problems with the proposed approach. First of all, the proposed metric is computationally very expensive since 2,000 models need to be trained for each s. Can't we simply use the learning speed of each test sample to achieve similar goals? Measuring learning speed should be computationally much simpler since it does not require training 2,000 models if we measure the learning speed for test or validation samples. Those test samples are automatically held out since they are not in the training dataset. In fact, authors show strong correlations between C-score and other measures based on learning speed in Fig. 9. Are there *real* benefits of using C-score over simpler alternatives such as measures based on learning speed, e.g., better outlier detection performance, better accuracy for detecting mis-labeled samples, better analysis of learning dynamics and generalization performance, etc.? If there's no such *real* benefit, then it's hard to justify the use of C-score considering its high computational cost.

At s=70,80,90%, there will be a lot of overlap of samples among 2,000 subsets and I don't see much value in distinguishing such cases (70%, 80%, 90%) with such fine granularity. In Fig. 4(a), they (70%, 80%, 90%) do not show much different anyway while there's a huge difference in the histograms between 10% and the rest (20% ~ 90%). To solve this problem, I suggest using something like s=1%, 2%, 4%, 8%, ..., 64% (log scale) instead of 10%, 20%, ..., 90% (linear scale). This way, we can also see the effect of very small n, which I believe is an interesting regime to study. Using log scale may also be better for comparing consistency profiles for different datasets such as CIFAR-10 and CIFAR-100. Fig. 2 shows the consistency profile curves start at around 0.2 for CIFAR-10 at s=10% while they are more spread from 0.0 to 0.85 for CIFAR-100 at s=10%. This may be an artifact of having coarse-grained intervals for s. By using log scale, we may be able to see a similar behavior between CIFAR-10 and CIFAR-100 only shifted in log scale, e.g., the consistency profile curves for CIFAR-100 may also start around 0.2 when s=1%.

I am not sure how to justify the definition of C-score that simply averages the consistency profile over uniform s from 10% to 90%. Why uniform instead of non-uniform? Why consider averaging instead of max, min, etc.? There are many other possibilities. Taking an average over uniform s seems ad hoc to me. Can authors provide a good justification? If there's no good justification, it may make sense to define C-score at a particular value of s (what authors call a point estimate) without even defining C-score averaged over s. But, this issue does not seem crucial.

How about using a higher learning rate for samples with low C-scores to facilitate learning from small number of irregular samples? This seems to be the opposite of what authors are suggesting in Figs. 8 and 9. 

It would be great if authors can improve the performance of optimizers based on the observations made in Figs. 8 and 9.",4: Ok but not good enough - rejection,2,1,0,1,1,1,,,,"[[Respectful, specific, and constructive, but not encouraging. (And maybe 'respectful' is a positivitity attribute we should consider?)]]",,0.6,0.8,1
qpdGrT40nIs,NeurIPS.cc/2022/Workshop/HITY/Paper27/-/Official_Review,"**Summary:** This paper empirically investigates the impact of regularization
techniques on the *intrinsic dimension* of model activations across the network
and how this quantity is connected to generalization. 

**Strengths, Weaknesses & Questions:**
- The paper is well-written and nicely structured with a motivating
introduction. The experiments are well thought out and the observations are
explained and discussed comprehensibly. 
- My main point of criticism: The intrinsic dimension is *the* central quantity
of your paper, but the definition you provide in lines 32-35 is quite vague and
therefore not sufficient in my opinion. I'd thus suggest adding a precise
mathematical definition and explanations of how you computed this quantity.
Without that, the reader is left with the information that it is some form of complexity
measure, which strongly limits in-depth interpretation and assessment of your
results. 
- Figures 2, 5, and 6: Almost all weight decay values are shown in gray (I guess
because you use a log-style grid to generate them). This makes it hard to
distinguish different weight decay values. I'd thus suggest using the log scale
also on the color bar.
- Figure 4: I would have expected the relationship between PID and weight decay
to be monotonic. But, this does not seem to be the case: Very small weight decay
values *and* large values lead to small PID with a maximum PID in between. Do
you have an explanation for this behavior? Also, I'd suggest using a log scale
for the weight decay axes to spread out the small values.

**Minor:**
- Line 41: I don't understand what it means for activations to *live on* a
low-dimensional manifold. 
- Figure 3: In my opinion, this Figure could be improved by using a 4x4 grid.
This way, the subplots could be ordered such that the $p$-value determines the
row and the WD-value the column. This would facilitate orientation and would
make it more obvious that the occurrence of red lines results from small $p$ and
small WD. Also, how is the y-label *Validation ID* defined - is it LLID?",1: accept,2,0,1,0,0,0,"1,2,3-- The paper is well-written and nicely structured with a motivating
 introduction. The experiments are well thought out and the observations are
 explained and discussed comprehensibly. 
 - My main point of criticism: The intrinsic dimension is *the* central quantity
 of your paper, but the definition you provide in lines 32-35 is quite vague and
 therefore not sufficient in my opinion. I'd thus suggest adding a precise
 mathematical definition and explanations of how you computed this quantity.
 Without that, the reader is left with the information that it is some form of complexity
 measure, which strongly limits in-depth interpretation and assessment of your
 results.",,,[[Generally constructive with some positive notes of encouragement. Seems professional enough and is certainly detailed.]],"1-<The paper is well-written and nicely structured with a motivating
 introduction>~~~1-<The experiments are well thought out and the observations are
 explained and discussed comprehensibly. >",0.6,0.2,0
qtUtrQ5PaOq,ICLR.cc/2023/Workshop/RRL/Paper51/-/Official_Review,"# Summary
The author present a novel algorithm for encouraging object interactions in an unsupervised exploration setting.
They propose a novel intrinsic reward formulation based on counterfactual reasoning, and show that this method outperforms baselines on multi-objects environments.

# Significance, Novelty, Impact
The paper proposes a novel intrinsic reward formulation, with two variants -- namely, forward model and successor features -- which are well-defined and motivated.
The reward formulation is general and has a great potential in tasks that contain multiple objects, for which previous intrinsic reward formulation could converge to a low hanging fruit behaviour.

# Empirical Evaluation
The method is validated empirically in two different object manipulation tasks.
The experiments contains comparison against unsupervised exploration baselines, as well as a topline with domain knowledge.
The method outperforms baselines on all tasks, and is the only method succeeding in performing object interaction in the harder task.
The author also presented extensive analysis to understand the behaviour of both variants of their method, as well as generalization experiments to unseen objects.

# Writing
The paper is clear and well-written, and succeeds in providing an intuition of both problem and method, and the balance is good between formal description and intuitive explanation.
It provides extensive evaluation, analysis and qualitative visualisation of the method.

# Remark
One interesting ablation would be to compare several types of interventions (random perturbations vs. more object-specific interventions), and study the impact on the downstream task performance.","4: Good paper, strong accept",1,0,0,0,0,0,"The paper is clear and well-written, and succeeds in providing an intuition of both problem and method, and the balance is good between formal description and intuitive explanation.
 ~~~ It provides extensive evaluation, analysis and qualitative visualisation of the method.","The paper is clear and well-written, and succeeds in providing an intuition of both problem and method, and the balance is good between formal description and intuitive explanation.",,,,1,0,0
r1eYTjcW3E,icaps-conference.org/ICAPS/2019/Workshop/XAIP/-/Paper10/Official_Review,"Motivating Example: Interesting, this gets into some Theory of Mind stuff to me, but I'm curious how it gets addressed later in the paper. ""Online Model Reconciliation"" maybe?  

Figure 1: the text in the bubbles is too small to read. I therefore lose the point of seeing it, though it does look nice.

Conclusion: What next?

Minor:

  Definition 1: the gap between the end of this and the start of the next paragraph looks like its somehow formatted oddly? Rearranging that sentence of cost(\\pi,M) so that it's not so close to the above italics would probably fix it.

  Definition 2: Your equations mess w/ the padding of the line. I might break it out similar to your definition of r(f) above.","5: Top 15% of accepted papers, strong accept",1,1,1,0,1,0,,,,,,0.4,0.6,1
r1gDTrGIKE,ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review,"This paper takes a number of widely-used algorithms for sequence generation, including maximum-likelihood, RAML, SPG and data noising and shows that they can all be viewed as optimizing a member of a family of objective functions, which is defined through three hyper-parameters that control parts of the objective - the reward, a maximum entropy term, and a KL term between a variational distribution and the model. 

The authors show the exact values of the hyper-parameters that lead to these various objectives and also shed light onto how these hyper-parameters correspond to a trade-off between the exploration and difficulty of learning, where more exploration results in a more difficult learning problem. 

Because now we have a family of objectives, the authors now naturally propose an algorithm that anneals the values of the hyper-parameters using a curriculum where simple learning without exploration happens at the beginning and more exploration is added later on to avoid local minima.

I found the analysis clear and interesting, the insights on the relation between the algorithms to be informative and the final simple algorithmic contribution to be natural and worthwhile. Overall, a nice paper that I think definitely fits well in this workshop.

It is worth noting that a similar but different attempt has been made in the context of sequence generation when there is no gold sequence given at training time: See Misra et al. 2018
http://www.cs.cornell.edu/~dkm/papers/mchy-emnlp.2018.pdf","5: Top 15% of accepted papers, strong accept",2,0,0,0,0,0,"1-I found the analysis clear and interesting, the insights on the relation between the algorithms to be informative and the final simple algorithmic contribution to be natural and worthwhile. Overall, a nice paper that I think definitely fits well in this workshop.",,,"[[Plenty of encouraging sentences, no critiques though. Positive, but NOT a prime example of an outstanding review. I'm thining it might even be beneficial, if we wanted to be really rigorous, to rate reviews on each metric of our positivity/toxicity definitions, and use some established thresholds to determine the final score...]]","1-< found the analysis clear and interesting, the insights on the relation between the algorithms to be informative and the final simple algorithmic contribution to be natural and worthwhile. Overall, a nice paper that I think definitely fits well in this workshop.>",1,0,0
r1gEDoHi2Q,ICLR.cc/2019/Conference/-/Paper1005/Official_Review,"The stated contribution of the paper is the development of a model to learn continuous representations of k-mers from RNA sequencing experiments in an annotation-free manner. The paper motivates this model by considering analysis challenges faced in cancer genomics. This introduction serves well to frame the paper towards addressing these challenges. In particular, the authors highlight challenges faced in recognizing and quantifying patient/tumor-specific RNASeq based expression estimates involving structural variants and indels, which have and continue to be a challenge for existing tools. 

Despite this, we are not enthusiastic about the paper for the following reasons:
Narrow and incomplete view of commonly used modern RNA-seq tools/pipelines and their application/use in biomedical research.

The proposed computational method is computationally intractable and is unlikely to ever scale to the genome-wide context.

Described experiments are without context to the existing literature of tools designed to address the biological challenge and by construction are not annotation free.

We further describe these reasons in the following subsections. Overall, we do not believe that the described model/experiments demonstrate utility for either the specific problems in cancer genomics that motivate the paper or the biomedical research field in general.

Narrow and incomplete view of RNA-seq tools/pipelines/application:
------------------
""Reducing this rich data to only the detection of annotated genes [...] is not appropriate for analysis"". Modern RNA-seq pipelines also perform quantification and differential analysis at a minimum.
The description of the standard RNA-seq experiment is problematic:
Sequencing is of cDNA after reverse transcription, not RNA.
Ignores paired end reads (especially with longer insert sizes for fusion detection)
Shredder poor analogy given multiple distinct reads from same sequence, known biases in process
""[...] multiple sequence alignment is an NP-hard problem"". This is true but irrelevant.
Chromosomal translocations are indeed hard to detect by RNA-seq, but not impossible. There are strategies implemented in commonly used tools such as STAR, kallisto, and others to detect these and other structural variants. Individual reads do not have to themselves cover the sequence where translocation occurs, instead read pairs can imply that the insert contains a translocation -- in this case, sequence similarity is much higher. Regardless, cheaper orthogonal assays exist that can detect these events.
""The standard RNA-Seq analysis pipeline has a mean processing times of 28 core hours for mapping with software TopHat, followed by an additional 14 hours of quantification"". See ""Please stop using TopHat"" (https://twitter.com/lpachter/status/937055346987712512?lang=en) by one of the authors of TopHat and the senior author of the cited paper. Standard pipelines use newer aligners like STAR which are substantially faster.
""Reference based methods yield a relative abundance measurement of genes, which are by definition, hand crafted features"". Relating results back to genes is important to be able to connect sequencing results back to known biology. We see the fact that there is no obvious map from the proposed method back to genetic information as a weakness.

Proposed computational method computationally intractable and unlikely to scale to genome-wide context
------------------------
Plus:
Paper does acknowledge that scalability is a limitation.
Minus:
Lower bound of range of unique kmers per sample without pre-filtering is 10 billion; note that storing counts uncompressed as 32-bit integers corresponds to over 37GB per sample.
Experiments are for only four genes, two at a time with a 2-dimensional embedding. Unclear how patterns will hold when considering k-mers from more genes simultaneously or how embedding could scale. Model formulation suggests that k-mers from co-expressed genes will have similar embeddings, which could complicate visual inspection.
Abstract states that ""learned representation both useful for visualization as well as analysis"". Unclear what is done with model/embeddings besides visualization -- non-visual analyses are performed with k-mer counts before embedding. Identification of abnormalities are described only by visual inspection, which is unlikely to scale as more k-mers are added and/or if the dimensionality of the embedding increases.

Experiments without context to existing literature and are not annotation free
----------------------------------------

The paper describes that existing methods are limited by their dependence on annotations. The paper does not describe existing methods using annotations designed to address tasks/applications suggested for the new model. The paper does not compare developed model to these existing methods. To make the experiments performed in the paper computationally tractable, RNA-seq reads are aligned to the reference genome (annotations of sequence) and sequences in specific regions, defined with respect to genes of interest (annotations of genes). The experiments are therefore dependent on annotation although they lose their information/interpretability in this context. The paper notes that many kmers are not included in exonic sequences according to exact matches to annotations of coding sequence. Nonetheless, these reads are in this dataset, which means they are also identified by standard tools/annotations despite their differences. In Figure 3, embedding overlap between kmers in the annotated coding sequence of ZFX and ZFY are illustrated. It would be helpful to show the proportions of reads that were used for this analysis that mapped uniquely vs not to genomic intervals in these genes (reads rather than kmers). In this regard, the absence of any note or methods describing how reads were mapped (what method, with which parameters) to reference (which reference genome, which gene annotations) is a serious limitation.

Other notes
---------------------
The heatmap in Figure 5A is ambiguous and could be improved by better annotating what is on rows/columns, perhaps also including numeric information textually in addition to by color.
Identification of kmers spanning translocation region (illustrated in embedding space by 5B) was done entirely without kmer2vec (identifying exclusive kmers, assembly of kmers, BLAST alignment to two chromosomes). Thus, claim that, as a consequence, ""kmer2vec captures real genomic abnormalities and allows to extract directly from the kmer embedding space the abnormal sequence"" is unsubstantiated.

",,1,1,2,1,1,0,,Narrow and incomplete view of commonly used modern RNA-seq tools/pipelines and their application/use in biomedical research.,"Overall, we do not believe that the described model/experiments demonstrate utility for either the specific problems in cancer genomics that motivate the paper or the biomedical research field in general.",,,0.3,1,1
r1OoL_Yxz,ICLR.cc/2018/Conference/-/Paper17/Official_Review,"The authors suggest using a mixture of shared and individual rewards within a MARL environment to induce cooperation among independent agents. They show that on their specific application this can lead to a better overall global performance than purely sharing the global signal, or using just the independent rewards.

The paper is a little too focused on the packet routing example domain and fails to deliver much in terms of a general theory of reward design for cooperative behaviours beyond showing that mixed rewards can lead to improved results in their domain. They discuss what and how rewards, and this could be made more formal, as well as (at the very least) some guiding principles to follow when mixing rewards. It feels like there is a missing section between sections 2 and 3, where this methodological content could be described.

The rest of the paper has similar issues, with key intuition and concepts either missing entirely or under-represented. The technical content often assumes that the reader is familiar with certain terms, and it is difficult to see what meaningful conclusions can be drawn from the evaluation.


On a minor note, the use of the term cooperative in this paper could be better defined. In game theory, cooperative games are those in which agents share rewards. Non-cooperative (game theory) games are those where agents have general reward signals (not necessarily cooperature or adversarial). Conventionally (yes there is existing reward design/shaping literature for MARL) people have used the same terms in MARL. Perhaps the authors could define their approach as weakly cooperative, or emergent cooperation.

The related work could be better described. There are existing papers on MARL and the issues with cooperation among independent learners, and this could be referenced. This includes reward shaping and reward potential. I would also have expected to see brief mention of empowerment in this section too (the agent favouring states where it has the power to control outcomes in an information theoretic sense), as an underyling principle for intrinsic reward. However, more importantly, the authors really needed to do more to synthesize this into an overall picture of what principles are at play and what ideas/methods exist that have tried to exploit some of these principles.

Detailed comments:
  ""¢ [p2] the authors say ""We set the meta reward signals as 1 - max(U l )."", before they define what U_l is.
  ""¢ [p2] we have ""As many applications in the real world can be modeled using similar
methods, we expect that other fields can also benefit from this work."" This statement is too vague, and the authors could do more to identify which application areas might benefit.
  ""¢ [p3, first para] ""However, the reward design studies for MARL is so limited."" Drop the word 'so'. Also, I would argue that there have been quite a few (non-deep) discussions about reward design in MARL, cooperative, non-cooperative and competitive domains. 
  ""¢ [p3, sec 2.2] ""This makes the diligent agents confuse about..."" should be ""confused"", and I would advise against anthropomorphism at least when the meaning is obscured.
  ""¢ [p3, sec 3] ""After having considered several other options, we finally choose the Packet Routing Domain as our experimental environments."" Not sure what useful information is being conveyed here.
  ""¢ [sec 3] THe domain could be better described with intuition and formal descriptions, e.g. link utilization ratio, etc, before.
  ""¢ [p6] ""Importantly, the proposed blR seems to have similar capacity with dlR,"" The discussion here is all in terms of the reward acronyms with very little call on intuition or other such assistance to the reader.
  ""¢ [p7] ""We firstly try gR without any thinking"" The language could be better here.", before they define what U_l is.,2,1,1,1,1,0,,3-The paper is a little too focused on the packet routing example domain and fails to deliver much in terms of a general theory of reward design for cooperative behaviours beyond showing that mixed rewards can lead to improved results in their domain.,,"[[Even though there's not a lot of positivity, I just can't help but see this as a textbook example of specific and constructive feedback--it really fulfills those positivity requirements. Not a hint of mean-spiritedness]]","2,3-<It feels like there is a missing section between sections 2 and 3, where this methodological content could be described.
 >",0.6,0.8,1
rBxeRAL2CWc,ICLR.cc/2022/Workshop/OSC/Paper27/-/Official_Review,"Overall, it was difficult to understand the main approach presented in the paper. 
It was unclear as to the kind of environments that were being considered in the approach. The presentation could have benefitted significantly by considering a few standard environments / domains; then following that with the approach and discussing each component of the approach.","1: Technically incorrect, off-topic, or other significant issues - rejection",2,1,1,1,1,1,,,,"[[Though this did offer some constructive criticism, it didn't seem to offer any encouragement or highlight anything in detail. So I don't think it can count as positive]]",,1,1,1
rJe9Ftrkj4,icaps-conference.org/ICAPS/2019/Workshop/SPARK/-/Paper4/Official_Review,"This paper focuses on the use of P&S techniques for Home Health Care Management. In particular, it addresses the problem of scheduling the work of a number of caregivers in order to deal with the current services demand. In a nutshell, the idea is to take a multi-agent perspective: each caregiver is a different agent, described by her own characteristics, that can cooperate with others in order to satisfy the overall demand. The core of the method relies on the possibility to break tasks between 2 agents with different qualification levels. 
Some ideas to address the dynamic aspects of the problem at hand are also introduced and discussed. 

The paper is generally easy to follow: the addressed problem is formalised, and an ad-hoc algorithm is proposed to deal with it. On this matter, I'd have preferred to get more information about the P&S techniques, and possibly the use of more general (domain-independent) approaches. As it stands, it does not give too much to the SPARK community, as I struggle to see how the proposed approaches can be used in different domains. I'm wondering, for instance, how good would a SLS approach perform. 

All in all, the paper deals with an interesting problem, and can be of interest for the SPARK community. For future work in the area, I'd suggest the authors to either (a) focus on SLS or MIP approaches, or (b) investigate how more general existing planning techniques can deal with the specific problem. In particular, (b) would potentially provide insights and lessons for the wider ICAPS community.",3: Marginally above acceptance threshold,1,0,1,0,1,0,"The paper is generally easy to follow: the addressed problem is formalised, and an ad-hoc algorithm is proposed to deal with it",,,,,0.4,0.4,0
rJekQDxD2m,ICLR.cc/2019/Conference/-/Paper913/Official_Review,"This paper propose to add an OT-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences. Indeed, it allows the computation of a distance between embeddings of a set of words, and this distance is then used to define a penalized objective function.
The main issue with this computation is that it provides a distance between a set of words but not a sequence of words. The ordering is then not taken into account. Authors should discuss this point in the paper.
Experiments show an improvement of the method w.r.t. not penalized loss.

Minor comments:
- in Figure 1, the OT matching as described in the text is not the solution of eq (2) but rather the solution of eq. (3) or the entropic regularization (the set of ""edges"" is higher than the admissible highest number of edges).
- Introduction ""OT [...] providing a natural measure of distance for sequences comparisons"": it is not clear why this statement is true. OT allows comparing distributions, with no notion of ordering (see above).  
- Table 1: what is NMT?
- first paragraph, p7: how do you define a ""substantial"" improvement of the scores?
- how do you set parameter $\\gamma$ in the experiments? Why did you choose \\beta=0.5 for the ipot algorithm?

",5: Marginally below acceptance threshold,1,0,1,0,1,0,,,,,,0.4,0.4,0
rJgd4DB4TQ,ICLR.cc/2019/Conference/-/Paper204/Official_Review,"This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.

Moreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.). No demonstration or comparison with state of the art is provided. 

The authors claim ""This work is the first to apply modern object detection deep learning approaches to document data"" but there are previously published works. For example:

Tuggener, Lukas, et al. ""DeepScores--A Dataset for Segmentation, Detection and Classification of Tiny Objects."" ICPR 2018.
Pacha, Alexander, et al. ""Handwritten music object detection: Open issues and baseline results."" DAS 2018.

Actually, in my opinion Music Object Detection in musical scores would be a much better test-bed/application for the proposed pipeline than any of the datasets used in this paper.  The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. 

Finally, the presentation of the paper is marginal. Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. There seem to be also missing content in the last sections which makes them impossible to read/understand.",,1,1,2,2,2,2,,"This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.~Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text.","This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses. ~~~ Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. ~~~ There seem to be also missing content in the last sections which makes them impossible to read/understand. ~~~ The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. ~~~ Moreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.).",This paper lacks any novelty/contribution~~~I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario,This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection,0.6,1.8,2
rJgGojoDFN,ICLR.cc/2019/Workshop/LLD/-/Paper38/Official_Review,"Paper summary:

This paper proposes a novel unsupervised embedding for time-series. Its architecture mainly consists in a series of dilated causal convolutions, followed by a temporal averaging to obtain a representation which is independent on the length of the time-series. The authors propose a triplet loss with negative mining to train the embedding, which is novel for real-valued time-series. This method is experimentally validated on a classification and a regression task.

General opinion:

* Pros:
    * Good writing
    * Detailed appendix with experimental hyperparameters, so that the results are pretty reproducible.
    * For classification and regression, the proposed method reaches results close to the state-of-the-art.
* Cons:
    * The authors state that the embedding is unsupervised, but in Appendix C they acknowledge that it is trained with early-stopping based on the final classification accuracy, thereby relying on an implicit supervision.
    * This method does not improve over the state-of-the-art on time-series classification, even though it is its natural purpose
    * The experimental validation of the proposed method is weak (cf detailed method).
    * I have some concerns at the conceptual level (cf detailed questions).

Taking into consideration these aspects, I tend to vote for a weak rejection.

Detailed questions:
- On a conceptual level, the ideas underlying the use of a triplet loss explained in the 3rd paragraph of section 2 seems a bit incomplete to me. On the one hand, the authors state that the embedding of a sub-series should be close to the embedding of the series. On the other hand, they also state that this embedding should be far from the embedding of a randomly sampled sub-series, possibly in the same long time-series. This seems contradictory, because if they belong to the same global time-series, they are both sub-series of the global time-series and therefore should be close. Also, the fact that no scale is taken into account when defining sub-series seems quite irrealistic.
- Why is using a *causal* embedding important for classification purposes? 
- Experimentally, what are the results when the number of negative samples, K, varies? Experiments have been performed with this parameter varying as an ensemble is taken. It is a pity that the importance of this value is not reported, as it would have provided an intuition on its importance.
- The runtimes reported in Table 1 are a bit strange. Why does the runtime of the raw values vary so much (x30) when moving from daily to quarterly predictions, while the runtime of the representations diminishes (/3)?",2: Marginally below acceptance threshold,1,1,1,1,1,1,,,"Why does the runtime of the raw values vary so much (x30) when moving from daily to quarterly predictions, while the runtime of the representations diminishes (/3)? ~~~ It is a pity that the importance of this value is not reported, as it would have provided an intuition on its importance. ~~~ Also, the fact that no scale is taken into account when defining sub-series seems quite irrealistic.",The runtimes reported in Table 1 are a bit strange.,,1,1,1
rJhtcsdxf,ICLR.cc/2018/Conference/-/Paper401/Official_Review,"Here are my main critics of the papers:

1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)? If so your phrase ""is zero given a sequence of inputs X1, ...,T"" is misleading. 
2. Lack of motivation for IE or UIE. Where is your background material? I do not understand why we would like to assume (1), (2), (3). Why the same intuition of UIE can be applied to RNNs? 
3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization. Not much novelty.
4. The experimental results are not convincing. It's not compared against any previous published results. E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1]. Also it only has been tested on very simple datasets.


[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations. Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro.",2: Strong rejection,2,1,2,2,2,1,,"1,3-Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)? If so your phrase ""is zero given a sequence of inputs X1, ...,T"" is misleading.~~~5-The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization. Not much novelty.","3- The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization. Not much novelty.","3-1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)? If so your phrase ""is zero given a sequence of inputs X1, ...,T"" is misleading. 
 2. Lack of motivation for IE or UIE. Where is your background material? I do not understand why we would like to assume (1), (2), (3). Why the same intuition of UIE can be applied to RNNs? 
 3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization. Not much novelty.
 4. The experimental results are not convincing. It's not compared against any previous published results. E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1]. Also it only has been tested on very simple datasets.",,0.4,1.6,2
rJx1LGPe2V,icaps-conference.org/ICAPS/2019/Workshop/XAIP/-/Paper2/Official_Review,"I am not sure if the paper is relevant to the XAIP Workshop. I suppose goal recognition can be seen as a way of trying to ""explain"" what the goal of an agent is from observations. But the authors made no attempt to make that stance. Even if that is the case, there is plenty of existing work [1] where the agent tries to make the goal or plan recognition task easier for the observer.

If there is a case to be made for operator count heuristics specifically for this purpose, I would love to hear it.

[1] Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Robot Behavior. Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David Smith, Subbarao Kambhampati.

[2] Also this might be interesting to the authors (at the intersection of XAIP and landmarks): https://arxiv.org/abs/1903.08218",2: Marginally below acceptance threshold,2,2,1,2,2,1,"2-I am not sure if the paper is relevant to the XAIP Workshop. I suppose goal recognition can be seen as a way of trying to ""explain"" what the goal of an agent is from observations. But the authors made no attempt to make that stance. Even if that is the case, there is plenty of existing work [1] where the agent tries to make the goal or plan recognition task easier for the observer.",,"2,3","3,2,1-the authors made no attempt to make that stance.",,0.4,1.6,2
rklj4rCChQ,ICLR.cc/2019/Conference/-/Paper77/Official_Review,"SUMMARY:
This paper is about potential problems of the information bottleneck principle in cases where the output variable Y is a deterministic function of the inputs X. Such a deterministic relationship between outputs and inputs induces the problem that the the IB ""information curve"" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate (""interesting"") solutions. The authors argue that most real classification problems indeed show such a deterministic relation between the class labels and the inputs X, and they explore several issues that result from such pathologies.

EVALUATION:
In my opinion, the whole story could be summarized as follows: if  Y is
a deterministic function of p-dimensional inputs X, then the joint distribution P(X,Y) is 
degenerate in that its support lies in a space of dimension p (an not p+1 as it would be in the non-degenerate situation), and this is the source of all pathologies observed. As a consequence, only the cumulative distribution is defined, but there is no density with respect to the Lebesgue measure of R^{p+1}. Thus, one has to be careful when defining the mutual information I(X,Y), which explains the problems with the IB information curve (which should asymptotically converge to I(X;Y) as I(X;T) gets large. Another consequence of this degeneracy concerns the latent variable interpretation of the IB: if T is treated as a latent variable (as, for instance, in the ""deep"" IB models) then we have the conditional independence relation ""Y independent of X given T"", which simply makes no sense if Y is deterministic in X (there is, of course, a deeper underlying problem here: the IB problem is difficult in that it is difficult to define a geneative model with a faithful DAG...).
Analyzing situations in which Y = f(X) (with f being a deterministic function) is certainly interesting from a theoretic point of view, but I am not convinced that this analysis is truly relevant for practical problems. 
In particular, I strongly disagree with the statement that ""in most classification problems, the labels Y are a deterministic function of X"". I would rather argue that the opposite is the case, because I don't think that there are too many such problems with zero Bayes error rate.  In particular, I would argue that digit recognition problems like MNIST so not have deterministic labels, since there will always be images of handwritten characters that will give room for interpretation...",2: Strong rejection,2,1,1,1,1,1,,,,,,1,1,1
rklKvHPsh7,ICLR.cc/2019/Conference/-/Paper335/Official_Review,"The paper studies question generation, which is an important problem in many real applications. The authors propose to use better caching model and more evalution methods to deal with the problem. However, the paper is poorly written and hard to follow, and the proposed model lacks of novelty. The main reasons are as below:

1) In model section, the task definition is not clear. It is expected to see what's the question generation task studied in this paper. An example or a model overview will definitly help.

2) The encoder and decoder are not novel, it is expected to cite and compare with the existing similar encoder architecture, such as the encoder proposed in bidaf ""Seo, Minjoon, et al. ""Bidirectional attention flow for machine comprehension."" arXiv preprint arXiv:1611.01603 (2016).""  The math symbols are aligned, for example, h_a or h^a is used to represent the encoding. Besides, adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.

3) The proposed copy mecahnism is not clear. A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t. A more serious question, what is the fuse operation used to define p_t? concat, elementwise_plus or others?

4) In the training, how to deal the ground truth that are not in the vocab? The authors stated ""using a modified heuristic described below"", but no follow-ups in the paper.

5) The paper is not well written and organized. Small typos: in introduction, 'and and answer span', 'and output and output sequences'. In model, 'Glorot initialization', 'Bahdanau attention', it is not the common way to cite others' work. In encoder, the defintion of the state for decoder could be reorganized to the decoder.

I have read the authors' detailed rebuttal. Thanks.",3: Clear rejection,2,1,1,1,1,1,,,,,"3-<adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.>~~~2,3< A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t>",1,1,1
RWiaKM5rz2E,graphicsinterface.org/Graphics_Interface/2020/Conference/Paper61/-/Official_Review,"The problem statement is unclear and informal throughout most of the paper. Towards the end the reader gets an idea in retrospect, but the involved concepts remain vague. The definition of ""hollow body"" is essentially via an algorithm (that involves quite a number of rather arbitrary engineering choices) rather than via a mathematical concept (that would then be realized or approximated by an algorithm / implementation).

The motivation remains unclear. No plausible use case for the orientation dependent definition of ""hollow body"" is given. Then paper mentions (quite extensively!) 3D display technology as well as CAD systems, but ultimately no connection to these is made.

There are numerous language and grammar errors. At times this makes it hard to even make sense of the text, or to be certain things are understood as intended.

The paper spells out algorithmic parts in detail that are standard knowledge (like determination of connected components in a labeled voxel grid) and therefore could be reduced to a short sentence (and perhaps a reference).

Robustness (with respect to non-uniforn sampling, noise, etc.) is not addressed or discussed. Presented results apparently use synthetic point clouds rather than real-world data.

The algorithm makes use of multiple thresholds. Their choice is not discussed. This, together with the various formal clarity issues, hinders reproducibility of the method and the results. In the result section successes and fails are reported; it remained unclear what even constitutes a ""fail"".

Overall, the paper's presentation is of a quality and clarity that is not acceptable for publication. The paper's contribution is quite limited and its potential significance remained open.
",3: Clear rejection,1,1,2,2,2,2,,"The problem statement is unclear and informal throughout most of the paper.~At times this makes it hard to even make sense of the text, or to be certain things are understood as intended.","The problem statement is unclear and informal throughout most of the paper. ~~~ There are numerous language and grammar errors. ~~~ At times this makes it hard to even make sense of the text, or to be certain things are understood as intended. ~~~ The paper spells out algorithmic parts in detail that are standard knowledge (like determination of connected components in a labeled voxel grid) and therefore could be reduced to a short sentence (and perhaps a reference). ~~~ Presented results apparently use synthetic point clouds rather than real-world data. ~~~ In the result section successes and fails are reported; it remained unclear what even constitutes a ""fail"". ~~~ Overall, the paper's presentation is of a quality and clarity that is not acceptable for publication. ~~~ The paper's contribution is quite limited and its potential significance remained open.","Then paper mentions (quite extensively!) 3D display technology as well as CAD systems, but ultimately no connection to these is made.~~~At times this makes it hard to even make sense of the text, or to be certain things are understood as intended.~~~it remained unclear what even constitutes a ""fail"".","Then paper mentions (quite extensively!) 3D display technology as well as CAD systems, but ultimately no connection to these is made.~~~At times this makes it hard to even make sense of the text, or to be certain things are understood as intended.~~~it remained unclear what even constitutes a ""fail"".",0.6,1.8,2
ryeMrlnP3N,icaps-conference.org/ICAPS/2019/Workshop/XAIP/-/Paper19/Official_Review,"
The paper presents the Explainable Planning (XAIP) framework  - introduced in the past IJCAI-XAI-17 workshop - to be provided ""as a service"". The key element of XAIP is the ability for the user to formulate contrastive questions (i.e., why did you do action A rather than B?). This would allow the user understanding the rationale behind the plan provided, transforming an opaque plan into a trustworthy one. To answer such a kind of question, the user has to (i) use its own planners; (ii) be able to add constraints on which the contrastive questions will be evaluated (aka hypothetical model, HModel); (iii) have a validation of the Hplan synthesised.

The approach is definitively interesting for the workshop, and for XAI broadly. The paper is well written and clear. The motivating example and the workflow are useful and gently  introduce to the reader the rationale behind XAIP. 

COMMENTS:

-- In my view, the approach shown in Fig. 5 should be an iterative process, as the user might continue generating HModels, for example by asking to explain the behaviour of HModel^2 generated from HModel_1... Roughly, I consider XAIP an approach to better understand how a planner works in a given domain, so the trustworthy grows as the understanding grows. The authors also agree with this view, as they describe XAIPSaas as an iterative process in Sec 3.3. This might be also reflected into Fig.5

-- ""The user question is encoded as a set of constraints, which represent the formal question, and this can be done through a dialogue where the user is guided to select the constraints that match their question."" As far as I can understand from the paper, it seems there is a set of questions pre-defined (bullet points of Sec 3) - that the user can select from a GUI - where actions A and B act like a placeholder that the user can replace with domain variables. Then, each question is formalised using the approach of Smith, 2012. 
If so - as it seems from Sec 5 -  I think using the term ""dialogue"" now is misleading as it seems the framework can encode naturale language into PDDL. To realise this, the authors might plan to employ word-embedding algorithms, that are able to represent word meaning into a N-dimensional vector space. Words with similar meaning are mapped to a similar position in the vector space. For example, ""powerful"" and ""strong"" are close to each other, whereas ""powerful"" and ""Paris"" are farther away. The word vector differences also carry meaning. For example, the word vectors can be used to answer analogy questions using simple vector algebra: ""King"" - ""man"" + ""woman"" â‰ˆ ""Queen"" (Mikolov, Yih, and Zweig, 2013). This knowledge also grows over time, making the system able to ""understand"" different meaning of sentences on the base of the lexicon used.

-- An interesting feature that might be added to XAIP as a service would be the ability to perform analytics on the domain/problem. Roughly, it is a step-by-step approach that generate a HModel for each contrastive question. It might be usel to the user to select a ""set"" of contrastive questions that the system automatically encodes in a cascading HModels, then providing ""analytics"" about the behaviour of the system that summarises how the planner works in that domain as a whole in a ""simulation"" setting."," whereas ""powerful"" and ""Paris"" are farther away. The word vector differences also carry meaning. For example",2,0,1,1,0,0,"1-The approach is definitively interesting for the workshop, and for XAI broadly. The paper is well written and clear. The motivating example and the workflow are useful and gently introduce to the reader the rationale behind XAIP.",,,"1-The paper is well written and clear.~~~3-In my view, the approach shown in Fig. 5 should be an iterative process, as the user might continue generating HModels, for example by asking to explain the behaviour of HModel^2 generated from HModel_1... ~~~3-The authors also agree with this view, as they describe XAIPSaas as an iterative process in Sec 3.3. This might be also reflected into Fig.5~~~3,2-As far as I can understand from the paper, it seems there is a set of questions pre-defined (bullet points of Sec 3) - that the user can select from a GUI - where actions A and B act like a placeholder that the user can replace with domain variables. Then, each question is formalised using the approach of Smith, 2012. 
 If so - as it seems from Sec 5 - I think using the term ""dialogue"" now is misleading as it seems the framework can encode naturale language into PDDL. To realise this, the authors might plan to employ word-embedding algorithms, that are able to represent word meaning into a N-dimensional vector space.~~~3-An interesting feature that might be added to XAIP as a service would be the ability to perform analytics on the domain/problem.",1-<The paper is well written and clear. The motivating example and the workflow are useful and gently introduce to the reader the rationale behind XAIP.>,0.4,0.4,0
ryxcHAeZtE,icaps-conference.org/ICAPS/2019/Workshop/HSDIP/-/Paper11/Official_Review,"The paper purports to present a novel domain-independent planning heuristic tailored to web service composition. The paper is very hard to read, full of sentences that are not at all easy to parse (let alone make sense of) and concepts coming out of nowhere. There are way too many inaccuracies and plainly wrong statements. Let me enumerate just a few, mostly from the Introduction and State of the Art sections:

* The characterization of heuristics is wrong: search heuristics are implicitly understood to take into account, in one form or another, the goal of the problem, so it makes no sense to talk, as the authors do, of using goal information in the computation of the heuristic as a novel contribution.

* The authors do not make clear in which sense translating a web service composition problem to PDDL loses ""much of the semantics on the way"".

* ""Uniform action costs"" is not a heuristic; the way in which it is presented as an admissible heuristic makes no sense at all.

* In p. 2 it is stated that the heuristic presented in (Haslum and Geffner 2000) ""only works for the relaxed problem when the service effect does not have a delete-list, and only the add-list is added to the current state to create a new state."" The sentence is not too clear, but it certainly gives the incorrect impression that the heuristic does not apply to problems with delete effects, which is not the case.

* Standard concepts from the planning literature such as ""post-uniqueness"", ""binariness"", ""open world assumption"" are brought into the discussion out of nowhere, with no clear relation to what is being discussed.

* The State of the Art section states that 
""the state of the art in generating heuristics is mainly based on the relaxation of an original problem by abstraction. Some of them use the domain description to structure the search space, others analyze services to identify landmarks which help to break down the search problem. But no heuristic found so far has used the semantics of the planning problem. Thus, those heuristics are often not applicable to general planning in real-world problems"" 

It is not clear nor explained elsewhere in which sense the heuristics do not ""use the semantics of the planning problem"", let alone why they are not applicable to ""general planning in real-world problems"".

* No formalization at all of the problem that is being tackled is given.

* The Discussion section states that 
""The problems used in academia are mostly formalized in PDDL with few semantics, e.g. input and output parameter type class hierarchies."" Additionally, in the ""toy domains"" used in academia, ""the services available are domain specific and are mostly necessary to solve the problem. Thus, the planning task is not a task of using the right services but rather to bring them in the right order. In contrast, in the general planning problem, the right services have to be selected to establish a domain. This domain includes the relevant services that help the agent to reach its goal.""""

Unfortunately I couldn't make much sense of the previous, e.g. it is not clear to me at all in what is the meaning of ""PDDL with few semantics"", of "" input and output parameter type class hierarchies"", and so on.

* ...


The above is but a quick sample. Overall, the paper does not meet the standards of clarity, scholarship and relevance of the workshop, and hence I propose rejection.
",,2,1,2,2,2,2,,"3-There are way too many inaccuracies and plainly wrong statements.~~~3,4-The characterization of heuristics is wrong: search heuristics are implicitly understood to take into account, in one form or another, the goal of the problem, so it makes no sense to talk, as the authors do, of using goal information in the computation of the heuristic as a novel contribution.",3,"3-The paper is very hard to read, full of sentences that are not at all easy to parse (let alone make sense of) and concepts coming out of nowhere. There are way too many inaccuracies and plainly wrong statements. Let me enumerate just a few~~~3-It is not clear nor explained elsewhere in which sense the heuristics do not Ã¢â‚¬Å“use the semantics of the planning problemÃ¢â‚¬Â, let alone why they are not applicable to Ã¢â‚¬Å“general planning in real-world problemsÃ¢â‚¬Â.[[Same vibes as 'even' review from the last set. I really don't feel that we successfully incorporated this concept into our toxicity guidelines.]]~~~3,5-No formalization at all of the problem that is being tackled is given. [[This is I think the 3rd review I've seen that gives me the sense that we need another toxicity guideline along the lines of 'sassy'â€¦or excessivelyâ€¦harsh? Excessive negativity maybe?]]","3-<The paper is very hard to read, full of sentences that are not at all easy to parse (let alone make sense of) and concepts coming out of nowhere. >",0.6,1.8,2
S1epUX832m,ICLR.cc/2019/Conference/-/Paper1147/Official_Review,"This paper proposes a method for learning how to explore environments. The paper mentions that the ""exploration task"" that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed.

<<Pros>>

-The paper is well-written (except for a few typos).
-The overall approach is simple and does not have much complications. 
-The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow.  

<<Cons>>

**The technical novelty is not significant**

-This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. 

**The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. **

-Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters?  Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? 

-The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup.

-What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map.

-The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the ""learning for Navigation"" section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both ""exploration strategy"" and ""imitation learning"" : ""Pathak, Deepak, et al. ""Zero-shot visual imitation."" International Conference on Learning Representations. 2018.
"" is missed in navigation comparison. The aforementioned paper is also missed in the references.  

-Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning.

-The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. 


** Technical details are missing or not explained clearly**

- Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? 

-The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? 

-Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isn't it only 2.5D (information obtained by depth sensor) used in the proposed method?

**Presentation can be improved**

-The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. 

- Interpretation of ""green vs white vs black"" in the reconstructed maps is left to the reader in Fig. 1. 

- Last line in page 5: there is no need for reiteration. It is already clear.

**Missing references**

-Since the paper is about learning to explore, discussion about ""exploration techniques in RL"" is recommended to be added in at least the related work section. 

-A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. 


", a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed.,2,1,1,1,1,0,,,,"[[This one is odd; it somehow contains sentences that meet all the positivity criteria and not a lot of the negativitiy criteriaâ€¦Somehow our positivity definition doesn't seem to account for this sort of review. Maybe we need an ""Overall good vibes"" requirement for something to be considered positive.]]","1-<The overall approach is simple and does not have much complications. The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow.>~~~ 2,3<What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters? Why are the graphs cut at 1000 steps?>",0.6,0.8,1
S1gpjKyptV,ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Official_Review,"The authors propose a loss function to generate natural images that include two separate objects via a GAN.  The loss uses a sort of self-supervision by noting that the decomposition of a natural image with two objects into individual object images should match closely to the original object images.  The decomposition and composition network are then learned jointly.  Additionally, at test time, the authors provide a loss that tunes pixels to preserve color and texture.

I thought this short paper was quite clear (given space constraints) --- the objective was presented and described well.

The authors claim originality that the composition self-consistency loss is a new insight --- I am not familiar with work that conflicts with that claim, though I cannot be certain.

Questions/comments
- What are the qualitative and quantitative differences between the $\\hat{c}^{after}$ and $\\hat{c}^{after}_s$ images?  This should be made a bit more clear in the text.
- In the CelebA + Glasses experiment, what were the composite images used to train?","5: Top 15% of accepted papers, strong accept",1,0,1,0,1,0,I thought this short paper was quite clear (given space constraints) --- the objective was presented and described well.,,,,,0.4,0.4,0
S1gsJSsy2m,ICLR.cc/2019/Conference/-/Paper857/Official_Review,"In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to ""denoise"" a recurrent neural network.  The idea is that for every normal step of an RNN, one induces an additional ""dimension"" of recurrency in order to create attractor dynamics around that particular hidden state. The authors introduce their idea and run some basic experiments. This paper is well written and the idea is novel (to me) and worthy of exploration.  Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.

MAJOR

The point is taken that RNNs are susceptible to noise due to iterated application of the function. In my experience, countering noise (in the sense of gaussian noise added) isn't a huge problem in practice because there are many regularization methodologies to handle it. This leads me to the point that I think the experiments need to compare across a number of regularization techniques.  The paper is motivated by discussion of noise, ""noise robustness is a highly desirable property in neural networks"", and the experiments show improved performance on smaller datasets, all of which speak to regularization. So I believe comparisons with regularization techniques are pretty important here. 

MODERATE

There is some motivation at the beginning of this piece, in particular about language, and does not contain citations, but should.

""Training is in complete batches to avoid the noise of mini-batch training.""  Please explain, I guess this is not a type of noise that the method handles? 

What about problems that require graded responses, which is likely anything requiring integration? For example,  what happens in the majority task if the inputs were switched to a non-discrete version, where one must hold analog numbers?


MINOR

Any discussion about the (presumably dramatic) increase in training time due to the attractor dynamics unrolling + additional batching due to noise vectors (if I understood correctly)?

What are your confidence intervals over?  Presumably, we'd like to get confidence over multiple network instantiations.

Pg 1. Articulated neural network? 


QUESTIONS

Does using a the 'c' variable as a bias instead of an initial condition really matter? 

How does supervised training via eqn (4) relate to the classic training of Hopfield nets? I assume not at all, but it would be useful to clarify?

What RNN architecture did you use in the Figure 5 simulations (tanh vanilla RNN or GRU?)
"," one induces an additional """"dimension"""" of recurrency in order to create attractor dynamics around that particular hidden state. The authors introduce their idea and run some basic experiments. This paper is well written and the idea is novel (to me) and worthy of exploration.  Unfortunately",2,1,2,1,1,1,"1,3-This paper is well written and the idea is novel (to me) and worthy of exploration. Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.","2,5-Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.",,,"2,5-Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.",0.6,1.2,1
S4hlKFqSn-q,ICLR.cc/2022/Workshop/OSC/Paper24/-/Official_Review,"Summary: The paper proposes an object-centric architecture for reward functions that can be quickly learned using a single expert demonstration followed by a small number of queries asked to an oracle to find out whether the task was successful or not. The reward function consists of a reward function, a graph, and equivalence function assignments for each graph edge. The reward function is compositional and object-centric in the sense that the full reward is an average of reward computed per object pair. The graph determines which object pairs should be considered during reward calculation based on which edges are present in the graph. Finally, to each edge, a set of equivalence functions are assigned from a predefined function library. When the contribution of each object pair to the reward is computed, the object pair can be fed to its equivalence functions to nullify the effects of transformations (such as rotation or scaling) that do not violate the task goal.

The learning takes an iterative active-learning-based approach. An initial reward function is first learned using M-AIRL which returns a reward function and a fully connected graph. In each iteration, a slightly different graph and equivalence assignment proposals are sampled with respect to the previous ones and then a reward function is trained. A final state is generated based on the newly trained reward function and the final state is queried against the oracle to find out if this state is acceptable. Based on this feedback the proposal (along with the reward function) is either accepted or discarded and the iterations continue until a stopping criterion. 

Pros: Demonstrates an interesting use-case for object-centric representations that shows sample efficient learning of reward functions and generalization to unseen environments.

Cons: Adding a brief description of M-AIRL could be helpful.

Conclusion: The paper is interesting and relevant for the workshop.
","2: Good workshop paper, accept",2,1,0,1,0,0,,,,"1,2-Demonstrates an interesting use-case for object-centric representations that shows sample efficient learning of reward functions and generalization to unseen environments.~~~3-Adding a brief description of M-AIRL could be helpful.~~~1-The paper is interesting and relevant",1-<Demonstrates an interesting use-case for object-centric representations that shows sample efficient learning of reward functions and generalization to unseen environments.>,0.4,0.4,0
SAexlUirnW5,ICLR.cc/2022/Workshop/OSC/Paper15/-/Official_Review,"Summary: The authors introduce a new problem setup where the agent observes a trajectory and is given a text description of a target trajectory. The agent must predict new initial conditions (i.e. an intervention) for a given pivot object such that when the dynamics are rolled out, the resulting trajectory conforms to the input text description. The authors propose to do this prediction by constructing an event-based tree and then performing a symbolic search on this tree.

Pros: 
1. The idea of the event tree is interesting and novel. 
2. The experiment results show the clear benefit of using the event tree in comparison to feed-forward prediction using DeepSets. It is also more computationally efficient than the brute force method.
3. Analysis shows the benefit of using a GNN for learning the value model.

Cons: 
1. Description of the forward model is limited and it is unclear if it is learned from data.
2. What does the forward model output in more formal terms? 

Conclusion: The paper is interesting and relevant for the workshop.
","2: Good workshop paper, accept",1,1,1,0,0,0,,,,,,0.4,0.4,0
SJgexWfknE,icaps-conference.org/ICAPS/2019/Workshop/KEPS/-/Paper11/Official_Review,"This paper introduces an approach for refining PDDL+ process' models description. The underlying idea is to generate plans using an existing —even if inaccurate— model, observe how the predicted execution differs from an actual (or simulated) execution, and refine the PDDL+ accordingly. For refining purposes decision trees are used, and are analysed for identifying conditions to be used for the different processes values. In fact, the refined domain models is likely to include a large number of processes, where each of them has different numerical effects, according to the surrounding conditions.

The paper is well written and easy to follow. The approach is well-motivated, and has the potential to foster the use of PDDL+ in real-world applications, as it reduces the pressure on knowledge engineers, and support maintainability. 

As future areas of work, it would be great to assess whether different approaches for learning have the potential to overcome some of the issues of decision trees, or if more sophisticated features selection methods can substantially improve the accuracy of refined models. ", observe how the predicted execution differs from an actual (or simulated) execution,2,0,0,0,0,0,"1-The paper is well written and easy to follow. The approach is well-motivated, and has the potential to foster the use of PDDL+ in real-world applications, as it reduces the pressure on knowledge engineers, and support maintainability. ~~~2,3-As future areas of work, it would be great to assess whether different approaches for learning have the potential to overcome some of the issues of decision trees, or if more sophisticated features selection methods can substantially improve the accuracy of refined models.",,,"1-The paper is well written and easy to follow.~~~1,2-The approach is well-motivated, and has the potential to foster the use of PDDL+ in real-world applications, as it reduces the pressure on knowledge engineers, and support maintainability.~~~2,3-As future areas of work, it would be great to assess whether different approaches for learning have the potential to overcome some of the issues of decision trees, or if more sophisticated features selection methods can substantially improve the accuracy of refined models.","1-<The paper is well written and easy to follow. The approach is well-motivated, and has the potential to foster the use of PDDL+ in real-world applications>",1,0,0
Skg_bLZFtE,ICLR.cc/2019/Workshop/LLD/-/Paper58/Official_Review,"This paper presents a simple method to incorporate gaze signals into standard CNNs for image classification, adding an extra term in the loss function. The term is based in the difference between the Class Activation Map obtained from the model, and the human map constructed using the eye tracking information. The authors apply their method to the POET dataset and report interesting results when using different sizes for the training set. They show that the gazed network achieved equivalent performance to that of a standard CNN using less training data for intermediate data regimes.

The paper well written. It presents a simple idea which has a lot of potential, specially in the context of medical data (as suggested by the authors in their planned future works). Some comments I would like to see in the camera ready version of this work:

- It is not clear how the human attention map is constructed. The authors just say that this is obtained by ""integrating the eye tracking signal in time"". Since this is a crucial element in their framework, I would like to see a detailed description of how this is obtained. If space constraint is a problem, you could just add an appendix section with this info.
- In the orange line in Figure 2 (the line associated to the standard CNN) I do not see the std. This value is reported in table 2 (Appendix B), so I guess this can be a problem related to image transparency. Please, fix this problem so that we can see the confidence interval for the standard CNN as we can do with the gazed CNN.
- Do you think this idea could be also useful to improve image segmentation based on CNNs with limited data?

Minor corrections:
- In Section 4: ""To test the this hypothesis"" should be ""To test this hypothesis"".
", I would like to see a detailed description of how this is obtained. If space constraint is a problem,2,0,1,0,0,0,,,,"1-The paper well written. It presents a simple idea which has a lot of potential, specially in the context of medical data (as suggested by the authors in their planned future works).~~~2,3-Some comments I would like to see in the camera ready version of this work:
 
 - It is not clear how the human attention map is constructed. The authors just say that this is obtained by Ã¢â‚¬Å“integrating the eye tracking signal in timeÃ¢â‚¬Â. Since this is a crucial element in their framework, I would like to see a detailed description of how this is obtained. If space constraint is a problem, you could just add an appendix section with this info.
 - In the orange line in Figure 2 (the line associated to the standard CNN) I do not see the std. This value is reported in table 2 (Appendix B), so I guess this can be a problem related to image transparency. Please, fix this problem so that we can see the confidence interval for the standard CNN as we can do with the gazed CNN.
 - Do you think this idea could be also useful to improve image segmentation based on CNNs with limited data?~~~3,2-Minor corrections:
 - In Section 4: Ã¢â‚¬Å“To test the this hypothesisÃ¢â‚¬Â should be Ã¢â‚¬Å“To test this hypothesisÃ¢â‚¬Â.","1-<The paper well written. It presents a simple idea which has a lot of potential, specially in the context of medical data>~~~3-< Please, fix this problem so that we can see the confidence interval for the standard CNN as we can do with the gazed CNN.>",0.6,0.2,0
Skozh1abf,ICLR.cc/2018/Conference/-/Paper1065/Official_Review,"The authors explore how sequence models that look at proprioceptive signals from a simulated or real-world robotic hand can be used to decode properties of objects (which are not directly observed), or produce entropy maximizing or minimizing motions.

The overall idea presented in the paper is quite nice: proprioception-based models that inject actions and encoder/pressure observations can be used to measure physical properties of objects that are not directly observed, and can also be used to create information gathering (or avoiding) behaviors. There is some related work that the authors do not cite that is highly relevant here. A few in particular come to mind:

Yu, Tan, Liu, Turk. Preparing for the Unknown: uses a sequence model to estimate physical properties of a robot (rather than unobserved objects)

Fu, Levine, Abbeel. One-Shot Learning of Manipulation Skills: trains a similar proprioception-only model and uses it for object manipulation, similar idea that object properties can be induced from proprioception

But in general the citations to relevant robotic manipulation work are pretty sparse.

The biggest issue with the paper though is with the results. There are no comparisons or reasonable baselines of any kind, and the reported results are a bit hard to judge. As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline.

Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper!

One point about style: I found the somewhat lofty claims in the introduction a bit off-putting. It's great to discuss the greater ""vision"" behind the work, but this paper suffers from a bit too much high-level vision and not enough effort put into explaining what the method actually does.",4: Ok but not good enough - rejection,2,1,2,1,1,0,"1-Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper!~~~2,3-The biggest issue with the paper though is with the results. There are no comparisons or reasonable baselines of any kind, and the reported results are a bit hard to judge. As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline.","3,2-As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline.",,"[[This is one of the hardest for me to judge. It meets each of the positivity guidelines at some point, but still feels like it should be reworded to avoid toxicity.]]","1-<I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper!>",0.3,1,1
SkxLzVYy5E,ICLR.cc/2019/Workshop/LLD/-/Paper8/Official_Review,"The authors present a novel normalization procedure based on weighted maximum likelihood. They extend the idea to make it works for feedforward neural networks.

Globally, the paper is clear and well written, and the contribution interesting.

The algorithm is well-motivated, and the related work is explained clearly.  I like the flexibility of the algorithm, highlighted in section 3.3. However, the step ""increment"" in Algorithm 1 could be explained more clearly. I think it is worth to include a quick discussion on the complexity of one iteration, and compare it to other regularisation procedure.

The numerical experiments seem promising, and the combination of the regularisation procedure with layernorm gives impressive results when classes are imbalanced.

In conclusion, I recommend acceptance of this paper.","4: Top 50% of accepted papers, clear accept",2,0,0,0,0,0,"1-Globally, the paper is clear and well written, and the contribution interesting.~~~1-The algorithm is well-motivated, and the related work is explained clearly.~~~1,3-I like the flexibility of the algorithm, highlighted in section 3.3. However, the step ""increment"" in Algorithm 1 could be explained more clearly. I think it is worth to include a quick discussion on the complexity of one iteration, and compare it to other regularisation procedure.~~~1-The numerical experiments seem promising, and the combination of the regularisation procedure with layernorm gives impressive results when classes are imbalanced.",,,"1-the paper is clear and well written, and the contribution interesting.~~~1,2-The algorithm is well-motivated, and the related work is explained clearly.~~~2-I like the flexibility of the algorithm, highlighted in section 3.3.~~~2,3-However, the step ""increment"" in Algorithm 1 could be explained more clearly. I think it is worth to include a quick discussion on the complexity of one iteration, and compare it to other regularisation procedure.~~~1,2-The numerical experiments seem promising, and the combination of the regularisation procedure with layernorm gives impressive results when classes are imbalanced.","1,2-<The algorithm is well-motivated, and the related work is explained clearly. I like the flexibility of the algorithm, highlighted in section 3.3. However, the step ""increment"" in Algorithm 1 could be explained more clearly. I think it is worth to include a quick discussion on the complexity of one iteration, and compare it to other regularisation procedure.>",1,0,0
SyKJh-qlM,ICLR.cc/2018/Conference/-/Paper791/Official_Review,"This paper introduces a machine learning adaptation of the active inference framework proposed by Friston (2010), and applies it to the task of image classification on MNIST through a foveated inspection of images. It describes a cognitive architecture for the same, and provide analyses in terms of processing compression and ""confirmation biases"" in the model.
- Active perception, and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency. The problem and its potential applications are well motivated.
- The perception-driven control formulation is well-detailed and simple to follow.
- The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling

Questions and comments:
- While an 85% compression rate is significant, 88% accuracy on MNIST seems poor. A plot demonstrating the tradeoff of 
accuracy for compression (by varying Href or other parameters) would provide a more complete picture of performance. Knowing baseline performance (without active inference) would help put numbers in perspective by providing a performance bound due to modeling choices.
- What does the distribution of number of saccades required per recognition (for a given threshold) look like over the entire dataset, i.e. how many are dead-easy vs difficult?
- Steady state assumption: How can this be relaxed to further generalize to non-static scenes?
- Figure 3 is low resolution and difficult to read.

Post-rebuttal comments:

I have revised my score after considering comments from other reviewers and the revised paper. While the revised version contains more experimental details, the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context. The paper also contains grammatical errors and is somewhat difficult to understand. Finally, while it proposes an interesting formulation of a well-studied problem, more comparisons and analysis are required to validate the approach.
", and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency. The problem and its potential applications are well motivated.,2,1,1,1,0,0,,,,,"1-<The problem and its potential applications are well motivated.>~~~1,2-<The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling>",0.4,0.6,1
SylU6f9xiE,icaps-conference.org/ICAPS/2019/Workshop/KEPS/-/Paper2/Official_Review,"This paper defines a decidable fragment of epistemic planning that can be compiled to fully-observable nondeterministic (FOND) planning, and keeps the same computational complexity. A small case-study is provided to demonstrate the fruitfulness of the compilation.

Generally, the paper provides the necessary context to the user, and it is well contextualised. The theoretical side is sound, and the compilation approach is described at an appropriate level of detail. The example helps to understand part of the details, and gives some useful descriptions of the resulting operators. 

As a suggestion for further improvements, and given the topic of the workshop, it would have been nice to provide some additional insights into the characteristics of the generated models. How can they be made easier to handle by planning engines? what are the most challenging aspects? Are there aspects of the language that are forcing the use of some caveats?

* Minor issues
- don't -> do not
- ""As explained in Section, "" -> the AAAI template does not enumerate sections.",3: Marginally above acceptance threshold,2,1,0,0,0,0,,,,"1,2-Generally, the paper provides the necessary context to the user, and it is well contextualised. The theoretical side is sound, and the compilation approach is described at an appropriate level of detail. The example helps to understand part of the details, and gives some useful descriptions of the resulting operators.~~~2,3-As a suggestion for further improvements, and given the topic of the workshop, it would have been nice to provide some additional insights into the characteristics of the generated models. How can they be made easier to handle by planning engines? what are the most challenging aspects? Are there aspects of the language that are forcing the use of some caveats?","1-<Generally, the paper provides the necessary context to the user, and it is well contextualised. The theoretical side is sound, and the compilation approach is described at an appropriate level of detail.>",0.6,0.2,0
vLCITCEc32,MIDL.io/2020/Conference/Paper322/-/Official_Review,"This work is based on a private dataset of 332 CT slices (not volumes!) from 140 patients with three different types of annotated liver lesions (cysts, hemangiomas, mets). They compare two multi-task approaches for segmentation and classification and two baseline (ablation, single-task) approaches on this dataset. Additionally, the encoders are either randomly initialised or pretrained on ImageNet (out of domain) or the LiTS dataset (same domain).

The number of 2D slices being used is relatively small, which limits the contribution to some degree, but the setup is solid and the results definitely interesting.

I missed some details on the architectures (numbers of filters, for instance) and possible image preprocessing.  I also wondered if the number of resolution levels is really only 3, which would limit the receptive field (without knowing any details about the employed blocks, it is hard to guess, but it could be around 44Â² pixels theoretical maximum, the ERF being even smaller).

It would also have been interesting to do an ablation study on the SE (squeeze & excitation) blocks, but at least they were used in all four compared approaches, so the comparison is fair.

Overall, I would rate it between 3 and 4, but I do think it is a nice contribution to MIDL, so I voted 4 (""strong"" accept).",4: Strong accept,2,1,0,0,0,0,,,,"3,1-The number of 2D slices being used is relatively small, which limits the contribution to some degree, but the setup is solid and the results definitely interesting.~~~3-I missed some details on the architectures (numbers of filters, for instance) and possible image preprocessing. I also wondered if the number of resolution levels is really only 3, which would limit the receptive field (without knowing any details about the employed blocks, it is hard to guess, but it could be around 44Ã‚Â² pixels theoretical maximum, the ERF being even smaller). It would also have been interesting to do an ablation study on the SE (squeeze & excitation) blocks, but at least they were used in all four compared approaches, so the comparison is fair.~~~1-I do think it is a nice contribution to MIDL","1-< but the setup is solid and the results definitely interesting.> ~~~ 3-< would also have been interesting to do an ablation study on the SE (squeeze & excitation) blocks, but at least they were used in all four compared approaches, so the comparison is fair.>",0.6,0.2,0
wkTlp0FQB4N,ICLR.cc/2021/Conference/Paper1930/-/Official_Review,"The paper discuss how to detect erroneous steps in gradient descent on a non-trusted GPU using a separate slower trusted execution environment,
by randomly deciding in each step whether to check the values returned by the GPU, as well as using small learning rates and clipping the gradients to ensure all updates are small.

The reason for checking this is based on the assumption that since GPU calculations are out sourced there may be trust issues and attackers with control of the
values returned by the GPU can alter the final network in subtle ways. 

The paper includes experiments and shows that this approach is faster than just running everything on the trusted execution environment.
The experiments test an attack approach where the attacker tries to inject some bad samples to get some success of making the network being trained output some particular class on a particular image kind.


I have questions about the model. It is explained that the attacker is full control of CPU and GPU etc. but it is also assumed the attacker runs the code on GPU as expected. I would prefer a complete black box model, where the system can input model, batches, parameters etc. to the GPU and get whatever it wants back, i.e. gradients, activations, whatever you desire and given an input the attacker can decide to return whatever you want.

It seems the attacker is seemingly completely oblivious to the actual network, and parameters being used, and only measures his current success rate, and seems very naive. 

Finally, the time is measure compared to a pure based TEE solution which is stated in the paper as completely unsatisfactory, and should instead be compared with training time without using any form of verification as this is the target.

In my opinion this paper is an implementation of a straight forward idea and the theorems for setting the probability parameters  are basic probably computations.
There is no theoretical contribution in the paper and all the arguments are heuristically based on some intuition about training dynamics.

In short, in my opinion the paper is simply not relevant or strong enough to warrant acceptance at ICLR.

",3: Clear rejection,1,1,2,1,1,0,,"It seems the attacker is seemingly completely oblivious to the actual network, and parameters being used, and only measures his current success rate, and seems very naive.","There is no theoretical contribution in the paper and all the arguments are heuristically based on some intuition about training dynamics. ~~~ In short, in my opinion the paper is simply not relevant or strong enough to warrant acceptance at ICLR.",seems very naive.~~~There is no theoretical contribution in the paper and all the arguments are heuristically based on some intuition about training dynamics.,,0.3,1,1
Wl1eR9aJh1,NeurIPS.cc/2022/Workshop/HITY/Paper23/-/Official_Review,The paper proposes a novel method that modifies implicit constrained optimization (ICO) in a way that the constraint influences the top layer of the network only. The reasoning the authors provide is that the top layer is associated to be responsible for the problem at hand while the bottom layers of a network provide general features. This is definitely an interesting take and simplification which merits further attention. ,1: accept,1,1,0,0,0,0,,,,,,0.6,0.2,0
wwBG26i8lE,NeurIPS.cc/2022/Workshop/HITY/Paper15/-/Official_Review,"In this paper, it is shown that algorithms based on active learning perform better than recent data subset selection algorithms.  Furthermore, an attempt has been made to explain why this is the case. The results look promising, and the text is clear.

Figure 1 and Figure 2: Please provide information over how many seeds you averaged and state the uncertainty intervals. If only one measurement was done, this result may not be significant.

Section 3.3: Please explain why it is a fair and valid comparison if the one algorithm uses 18000 samples, whereas the other only uses 8000. To me, a fair comparison would have been if only a subset of the random initial set is used to keep the amount of training data comparable (i.e. 4000 random and 4000 from the corresponding bin).
Please repeat this experiment on CIFAR-100 and Imagenet-30 to strengthen your statements.
",1: accept,2,1,0,0,0,0,,,,"1-The results look promising, and the text is clear.~~~2,3-Figure 1 and Figure 2: Please provide information over how many seeds you averaged and state the uncertainty intervals. If only one measurement was done, this result may not be significant.~~~2,3-
 Section 3.3: Please explain why it is a fair and valid comparison if the one algorithm uses 18000 samples, whereas the other only uses 8000. To me, a fair comparison would have been if only a subset of the random initial set is used to keep the amount of training data comparable (i.e. 4000 random and 4000 from the corresponding bin).
 Please repeat this experiment on CIFAR-100 and Imagenet-30 to strengthen your statements.","1-<The results look promising, and the text is clear.>~~~ 2,3-<lease repeat this experiment on CIFAR-100 and Imagenet-30 to strengthen your statements.>",0.6,0.2,0
-x1PPh8dev,ICLR.cc/2021/Conference/Paper2045/-/Official_Review,"This paper proposed a new distributed training method for GNNs. Specifically, unlike traditional distributed training methods for CNNs where data points are independent, nodes in a graph are dependent on each other. Thus, this dependence incurs communication between different workers in the distributed training of GNNs. This paper aims to reduce the communication cost in this procedure. Here, this paper proposed to sample more neighbor nodes within the same worker while reducing the sampling probability for the neighbor nodes on other workers. It also provides some theoretical analysis and conducts the experiments to verify the proposed method. 

1. The idea is simple. It is just a trad-off between intra-worker sampling and inter-worker sampling. In fact, it does not address the real challenge in distributed training of GNNs. Even though sampling more intra-worker neighbor nodes can reduce the communication cost, it will impair the prediction performance. A good solution should reduce communication costs and try to make the prediction performance as good as possible. However, this method only focuses on the former one. 

2. In the proof of Theorem 1, this paper assumes there exists a constant $D_1$, and further claims that $D$ is small. However, no evidence is provided to verify $D$ is small. Thus, the claim in Theorem 1 does not hold. Moreover, without any knowledge regarding $D$, the bound for $s$ is useless. 

3. Regarding experiments, an important baseline is missed. Specifically, the method only using intra-worker neighbor nodes should be used. Otherwise, the current experimental results cannot support the efficacy of the proposed method.",4: Ok but not good enough - rejection,2,1,1,1,1,1,,,,"[[Bare minimum respect, but not positive]]",,1,1,1
XMHZSU0fP7X,ICLR.cc/2021/Conference/Paper2487/-/Official_Review,"The paper studies dynamic graph embedding and proposes the method that first transforms the signal of certain edge's existence to its frequency domain by DFT and then considers it as edge features followed by the simplified vanilla GCN.  
Pros:
  1. The idea of using DFT to capture the dynamics is simple yet powerful based on the results of experiments.
  2. The experiments are able to show the improvements in various tasks.
Cons:
  1. The definition of the edge existence signal f is not clear (e.g., Eq. 2). It's not defined how to compute the probability in Eq. 2, so I bet the authors directly use existence as Pr=1 and non-existence as Pr=0. But it's not clearly mentioned in Section 4.1. Similarly, how to compute the conditional probability in Eq. 5 is also not clear. 
  2. Usually in graph learning (especially GCNs), receptive field denotes the nodes that are used for aggregations. The paper claims the proposed FTSE has a larger receptive field, but didn't give the reason. 
  3. The proposed method actually remove the dependencies between two timestamps, which are usually very important information to capture the network dynamics. This is one limitation of the paper.
  4. The notations are inconsistent. In Eq. 6, what is i? In Appendix C, the symbol N is first denoted as the embedding vector dimension, but then used as the number of nodes.
  5. My major concern is the complexity analysis. Based on Eq. 4, the features in frequency domain are computed for every node pairs (i.e., N^2). That's why when conducting the convolutions in Eq. 6, the complexity is O(N^2), which is a bit high compared to the vanilla GCNs where the adjacency matrix is very sparse (i.e., |E| number of edges). Based on this, I think the complexity analysis in Appendix C is not fair. In the analysis, the authors directly assume dense matrix multiplication (e.g., the multiplication between A and X has O(N^3) time complexity), which is true for the proposed method. However, for the baseline which uses vanilla GCN + RNN, GCN only has O(|E|) time complexity.",5: Marginally below acceptance threshold,1,1,1,1,1,1,,,,,,1,1,1
yh3YXD5moG,MIDL.io/2020/Conference/Paper198/-/Official_Review,"The authors propose a method for brain MRI segmentation quality control (QC). Their method makes use of pix2pix to generate a synthetic MRI from the segmentation result and then compares the synthetic and original MRIs to create an error map. This error map and the original MRI are then input to a CNN that classifies the result as either good or bad.

Strengths:
-	Using pix2pix to generate synthetic MRI in order to create error maps is interesting and can be used to localize areas of segmentation error, and initial results appear promising.
-	Training (n=1,600 subjects) and testing (n=~800) set sizes are large, demonstrating robust evaluation.

Weaknesses:
-	Evaluation of the segmentation error maps is limited to qualitative visual inspection. Some form of qualitative evaluation would be useful.
-	It is unclear how the training/testing images were scored and under what criteria, which would be useful for understanding the good/bad rating system.
-	While segmentation QC is a valuable tool, summarizing the entire segmentation quality into a single binary good/bad may not be useful for practical use. It is very subjective to say that something is good/bad, for example Fig 2a and Fig 2c show dramatically different severity of segmentation error. This ""amount"" of error may be more valuable than binary good/bad, and let the users decide their tolerance for error.

This is an interesting topic, and I generally like the pix2pix approach to compare against the original imaging as a way to get to a segmentation error map; however, I am less enthusiastic about second half of the proposal, the classification methodology, as I think binary evaluation does not necessarily have straightforward clinical utility. Instead, it seems that quantification of the error map as a measure of quality would be more useful.
", and let the users decide their tolerance for error.,2,1,0,0,0,0,,,,"1,2-Using pix2pix to generate synthetic MRI in order to create error maps is interesting and can be used to localize areas of segmentation error, and initial results appear promising.~~~1,2-Training (n=1,600 subjects) and testing (n=~800) set sizes are large, demonstrating robust evaluation.~~~2,3-Evaluation of the segmentation error maps is limited to qualitative visual inspection. Some form of qualitative evaluation would be useful.~~~2,3-It is unclear how the training/testing images were scored and under what criteria, which would be useful for understanding the good/bad rating system.~~~2,3-While segmentation QC is a valuable tool, summarizing the entire segmentation quality into a single binary good/bad may not be useful for practical use. It is very subjective to say that something is good/bad, for example Fig 2a and Fig 2c show dramatically different severity of segmentation error. This Ã¢â‚¬Å“amountÃ¢â‚¬Â of error may be more valuable than binary good/bad, and let the users decide their tolerance for error.","1-<This is an interesting topic, and I generally like the pix2pix approach to compare against the original imaging as a way to get to a segmentation error map>",0.6,0.2,0
YsYOhfnQ7D,KDD.org/2023/Workshop/epiDAMIK/Paper9/-/Official_Review,"This paper describes a novel RL approach to optimizing infection disease control policy. The proposed method combines supervised learning with RL and shows strong performance compared to baseline policies. 

Positives: 
+ The branching-process formulation is well-described and intuitive
+ The need for an estimate of the probability of infection is well-motivated 
+ The experiments are extensive and clearly show the strengths (and limitations) of the proposed RLSL framework 
+ Overall, the paper is clear and easy to follow


Places for Improvements 
- The intuition of the representation and state space for both the SL and RL settings could be improved. Currently, it seems as though many representations were tested and this one was eventually chosen. Were they tested on validation data? More information about how these representations are necessary
- Why was PPO chosen for the RL policy? Were other RL techniques considered? This choice could benefit from more justification
- How hyperparameters were chosen should be discussed more. Currently, it almost seems as though the architecture and hyperparameters for the 2D CNN were chosen based on the same set in which policies were evaluated, which would be problematic 
- As mentioned in the paper, calibration of the SL estimates is critical for the threshold-based approach. The authors should consider calibrating these probabilities and evaluating the calibration error in some way, to see if it can improve all methods, especially the threshold-based baseline ","4: Good paper, accept",2,0,1,1,0,0,"1,2,3-+ The branching-process formulation is well-described and intuitive
 + The need for an estimate of the probability of infection is well-motivated 
 + The experiments are extensive and clearly show the strengths (and limitations) of the proposed RLSL framework 
 + Overall, the paper is clear and easy to follow",,,"1,2-The branching-process formulation is well-described and intuitive~~~1,2-The need for an estimate of the probability of infection is well-motivated~~~1,2-The experiments are extensive and clearly show the strengths (and limitations) of the proposed RLSL framework~~~1,2-Overall, the paper is clear and easy to follow~~~
 2,3-The intuition of the representation and state space for both the SL and RL settings could be improved. Currently, it seems as though many representations were tested and this one was eventually chosen. Were they tested on validation data? More information about how these representations are necessary~~~2,3-Why was PPO chosen for the RL policy? Were other RL techniques considered? This choice could benefit from more justification~~~2,3-How hyperparameters were chosen should be discussed more. Currently, it almost seems as though the architecture and hyperparameters for the 2D CNN were chosen based on the same set in which policies were evaluated, which would be problematic 
 - As mentioned in the paper, calibration of the SL estimates is critical for the threshold-based approach. The authors should consider calibrating these probabilities and evaluating the calibration error in some way, to see if it can improve all methods, especially the threshold-based baseline",1-<The branching-process formulation is well-described and intuitive>,0.4,0.4,0
yWhveVnkiE,NeurIPS.cc/2022/Workshop/Offline_RL/Paper46/-/Official_Review,"**Summary**. This paper presents a finding that the residual gradient (RG), which takes gradient of both $(Q_{\\theta} - \\bar{y})^2$ and $(\\bar{Q}_{\\theta} - {y})^2$, performs very well for offline RL in D4RL benchmark and compatible to SAC-N though RG uses 50x less compute than SAC-N. 

**Significance and novelty**. 
This finding is interesting and has a potential for practical significance as it shows that a simple design can have strong empirical performance and computational benefit in offline RL setting. Simple designs with strong performance and computational benefits are desirable characteristics to make offline RL more widely adopted. 

The result is however quite preliminary as the paper merely showed the performance without properly explaining why such a design works, even with empirical study. Thus, it is not clear where the performance gain actually from. An interesting case is the other extreme of RG where $\\eta = \\infty$ (i.e. we remove $(Q_{\\theta} - \\bar{y})^2$ completely from the objective and only compute gradient of $(\\bar{Q}_{\\theta} - {y})^2$ ). The contribution is purely empirical which does not make new algorithmic or modeling contributions. They basically plainly applied Zhang et al. (2019)'s bidirectional target networks with minimum of the critic ensemble. Though it should be fine with that almost plug-in design, to me it's more important to understand why such a design works well in offline RL (e.g. what's going in the method that makes it works that well with much less compute), than just showing the good performance. This work is promising but needs more thorough investigation as to understand the method before being accepted for representation. ",4: Ok but not good enough - rejection,1,1,1,2,1,0,,,"The result is however quite preliminary as the paper merely showed the performance without properly explaining why such a design works, even with empirical study. ~~~ The contribution is purely empirical which does not make new algorithmic or modeling contributions. ~~~ They basically plainly applied Zhang et al. (2019)â€™s bidirectional target networks with minimum of the critic ensemble.",,,0.3,1,1
zNGa0EiGc3,MIDL.io/2020/Conference/Paper213/-/Official_Review,"This paper evaluates how a deep learning segmentation algorithm performs in CT imaging with synthetically reduced counts (low dose images) across 7 anatomical regions of interest.

The paper evaluates the segmentation performance of an algorithm over increasingly reduced CT imaging counts, but does not provide any methodological innovation.

The paper does not provide any details as to the deep learning segmentation method used. It is also unclear how the algorithm was trained for this task.

Overall, the results are to be expected in that segmentation performance declines as image quality degrades. Presuming this algorithm was trained on clean CT imaging (full count), it is unsurprising that segmentation results generalize poorly on noisy CT test data.

Enthusiasm for the results is also limited due to testing on n=5 images.

For future work, one possibility might be to train on simulated low-dose imaging and see how well the segmentation performs.
",1: Strong reject,1,1,2,1,1,0,,The paper does not provide any details as to the deep learning segmentation method used.~It is also unclear how the algorithm was trained for this task.,"The paper evaluates the segmentation performance of an algorithm over increasingly reduced CT imaging counts, but does not provide any methodological innovation.",,,0.3,1,1
